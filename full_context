CV 
% AUTHOR: Sina Atalay
% LICENSE: https://github.com/sinaatalay/rendercv/blob/main/LICENSE

\documentclass[10pt, letterpaper]{article}

% Packages:
\usepackage[
        ignoreheadfoot, % set margins without considering header and footer
        top=2 cm, % seperation between body and page edge from the top
        bottom=2 cm, % seperation between body and page edge from the bottom
        left=2 cm, % seperation between body and page edge from the left
        right=2 cm, % seperation between body and page edge from the right
        footskip=1.0 cm, % seperation between body and footer
        % showframe % for debugging 
    ]{geometry} % for adjusting page geometry
\usepackage[explicit]{titlesec} % for customizing section titles
\usepackage{tabularx} % for making tables with fixed width columns
\usepackage{array} % tabularx requires this
\usepackage[dvipsnames]{xcolor} % for coloring text
\definecolor{primaryColor}{RGB}{0, 79, 144} % define primary color
\usepackage{enumitem} % for customizing lists
\usepackage{fontawesome5} % for using icons
\usepackage{amsmath} % for math
\usepackage[
    pdftitle={Mario Rico's CV},
    pdfauthor={Mario Rico},
    colorlinks=true,
    urlcolor=primaryColor
]{hyperref} % for links, metadata and bookmarks
\usepackage[pscoord]{eso-pic} % for floating text on the page
\usepackage{calc} % for calculating lengths
\usepackage{bookmark} % for bookmarks
\usepackage{lastpage} % for getting the total number of pages
\usepackage[default, type1]{sourcesanspro} % for using source sans 3 font
\usepackage{ifthen}

% Some settings:
\pagestyle{empty} % no header or footer
\setcounter{secnumdepth}{0} % no section numbering
\setlength{\parindent}{0pt} % no indentation
\setlength{\topskip}{0pt} % no top skip
\makeatletter
\let\ps@customFooterStyle\ps@plain % Copy the plain style to customFooterStyle
\patchcmd{\ps@customFooterStyle}{\thepage}{
\color{gray}\textit{\small Mario Rico - Page \thepage{} of \pageref*{LastPage}}}{}{} % replace number by desired string
\makeatother
\pagestyle{customFooterStyle}

\titleformat{\section}{
        % make the font size of the section title large and color it with the primary color
        \Large\color{primaryColor}
    }{
    }{
    }{
        % print bold title, give 0.15 cm space and draw a line of 0.8 pt thickness
        % from the end of the title to the end of the body
        \textbf{#1}\hspace{0.15cm}\titlerule[0.8pt]\hspace{-0.1cm}
    }[] % section title formatting

\titlespacing{\section}{
        % left space:
        0pt
    }{
        % top space:
        0.3 cm
    }{
        % bottom space:
        0.2 cm
    } % section title spacing

\newcolumntype{L}[1]{
    >{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}
} % left-aligned fixed width column type
\newcolumntype{R}[1]{
    >{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}
} % right-aligned fixed width column type
\newcolumntype{K}[1]{
    >{\let\newline\\\arraybackslash\hspace{0pt}}X
} % justified flexible width column type
\setlength\tabcolsep{-1.5pt} % no space between columns
\newenvironment{highlights}{
        \begin{itemize}[
                topsep=0pt,
                parsep=0.10 cm,
                partopsep=0pt,
                itemsep=0pt,
                after=\vspace{-1\baselineskip},
                leftmargin=0.4 cm + 3pt
            ]
    }{
        \end{itemize}
    } % new environment for highlights

\newenvironment{header}{
        \setlength{\topsep}{0pt}\par\kern\topsep\centering\color{primaryColor}\linespread{1.5}
    }{
        \par\kern\topsep
    } % new environment for the header

\newcommand{\placelastupdatedtext}{% \placetextbox{<horizontal pos>}{<vertical pos>}{<stuff>}
  \AddToShipoutPictureFG*{% Add <stuff> to current page foreground
    \put(
        \LenToUnit{\paperwidth-2 cm-0.2 cm+0.05cm},
        \LenToUnit{\paperheight-1.0 cm}
    ){\vtop{{\null}\makebox[0pt][c]{
        \small\color{gray}\textit{}\hspace{\widthof{Last updated in February 2024}}
    }}}%
  }%
}%

% save the original href command in a new command:
\let\hrefWithoutArrow\href
 % new command for external links:
\renewcommand{\href}[2]{\hrefWithoutArrow{#1}{\mbox{\ifthenelse{\equal{#2}{}}{ }{#2 }\raisebox{.15ex}{\footnotesize \faExternalLink*}}}}

\let\originalTabularx\tabularx
\let\originalEndTabularx\endtabularx

\renewenvironment{tabularx}{\bgroup\centering\originalTabularx}{\originalEndTabularx\par\egroup}

% For TextEntrys (see https://tex.stackexchange.com/a/600/287984):
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1\topsep=0pt\itemsep=0pt\parsep=0pt\parskip=0pt\labelwidth=0pt\itemindent=0pt\labelsep=0pt}\item[]}
\let\endchangemargin=\endlist 

% Ensure that generate pdf is machine readable/ATS parsable
\pdfgentounicode=1

\begin{document}
    \placelastupdatedtext
    \begin{header}
        \fontsize{30 pt}{30 pt}
        \textbf{Mario Rico Ibáñez}

        \vspace{0.3 cm}

        \normalsize
        \mbox{\hrefWithoutArrow{tel:+410766880725}{{\footnotesize\faPhone*}\hspace*{0.13cm}+41 076 688 07 25}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{mailto:mario.ricoibanez@epfl.ch}{{\small\faEnvelope[regular]}\hspace*{0.13cm}mario.ricoibanez@epfl.ch}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{https://www.linkedin.com/in/mario-rico-ibáñez-6b5888225/}{{\small\faLinkedinIn}\hspace*{0.13cm}}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{https://github.com/MarioRicoIbanez}{{\small\faGithub}\hspace*{0.13cm}}}
        \hspace*{0.5 cm}
        \mbox{\faMapMarker \hspace*{0.13cm} Lausanne, Switzerland}
        \hspace*{0.5 cm}
    \end{header}

    \vspace{0.3 cm}

\section{Education}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{MSc}
    &
    \textbf{Computer Science}, École Polytechnique Fédérale de Lausanne
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item \textbf{Coursework}: Machine Learning, Modern Natural Language Processing, Reinforcement Learning, Mobile Networks, Intelligent agents, Algorithms, Distributed Algorithms
        \item \textbf{Projects:} Adversarial Attacks on Image-to-image Generative Models
        \item \textbf{GPA:5.36/6} .
    \end{highlights}
    &
    2024 to Current
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{BSc}
    &
    \textbf{Telecommunications Engineering}, Universitat Politècnica de València
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item \textbf{Specialization}: \textbf{Telecommunication Systems}
        \item \textbf{GPA}: \textbf{4}. 22/40 subjects with honours. \textbf{Top 3} out of 200+ students
        \item Participant in the \textbf{High Performance Group} with \textbf{English teaching}.

        \item \textbf{Coursework:} Algebra, Calculus, Probability and Random Signals, Digital Signal Processing, Digital and Analog Electronics, Information Theory, Spatial Communications, Mobile and Wireless Communications, Digital Communications, Telematics, Antennas, Electromagnetic Waves, Microwaves
        \item \textbf{Bachelor's thesis}: Deep Reinforcement Learning for UAV Base Station Dynamic Positioning
    \end{highlights}
    &
    2020 to 2024
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{Dip}
    &
    \textbf{Artificial Intelligence by SAMSUNG INNOVATION}, Universitat Politècnica de València
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Diploma equivalent to 35 ECTS
        \item \textbf{Coursework:} Maths for Data Science, Probability and Statistics, Python for Data Science, Machine Learning, Deep Learning, Natural Language Processing
        \item \textbf{Final Project}: Emotion Detection through audio and text \href{https://github.com/laurarimi/IA-project/tree/main}{Repo}
        \end{highlights}
    &
    Sept. 2022 to Dec. 2022
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{BSc}
    &
    \textbf{Mathematics}, Universidad Nacional de Educación a Distancia
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Currently enrolled in a long-term program
        \item \textbf{Coursework}: Discrete Maths, Formal Languages and Basic Geometry
    \end{highlights}
    &
    2023 to 2030
\end{tabularx}

    \section{Experience}
\begin{tabularx}{\textwidth-0.4cm-0.13cm}{K{0.2cm} R{4.1cm}}
    \textbf{Computer Vision \& Behaviour Analysis Lab}, Machine Learning Developer
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Developed SOTA text models for \textbf{Multilabel Emotion Recognition} within a psychology context
        \item Conducted research on Deep Learning for Emotion Recognition, comparing \textbf{Encoder-only} Transformers (like BERT and RoBERTa) with \textbf{Decoder-only} Transformers (like Llama-2, Mistral, Qwen)
        \item Explored the \textbf{explainability} of \textbf{Large Language Models} in the context of Emotion Recognition


        
    \end{highlights}
    
    &  March 2023 to July 2024

\end{tabularx}
\vspace{0.1cm}
\begin{tabularx}{\textwidth-0.4cm-0.13cm}{K{0.2cm} R{4.1cm}}
     \textbf{Instituto de Telecomunicaciones y Aplicaciones Multimedia}, Machine Learning Developer
    
    

    
   \begin{highlights}
\item Developing 5G/6G multi-carrier, massive MIMO, \textbf{GPU-based} system-level \textbf{wireless communication simulator} (\textbf{PyTorch})
\item Utilizing \textbf{Deep Reinforcement Learning} strategies to optimize the placement of Base Stations
\item Investigating the \textbf{applicability} of Deep Reinforcement Learning techniques in telecommunications and endeavoring to implement algorithms across varied scenarios, including singular 4G layer, multi-layer environments, and several distributions of users.
\end{highlights}

& January 2024 to Current

\\
Supervised by Prof. Valery Naranjo Ornedo and Dr. David López Pérez

\end{tabularx}

\section{Publications}

Rico Ibáñez, Mario; del Amor, Rocío; Naranjo, Valery. Mejorando el Análisis de Emociones en texto mediante Llama 2. \textit{In: XLI Congreso Anual de la Sociedad Española de Ingeniería Biomédica}. Valencia: Universitat Politècnica de València, 2023. Pp. 670-673. ISBN: 978-84-17853-76-1 \href{https://repositorio.upct.es/handle/10317/13756}{Link}

\vspace{0.1cm}

Rico Ibáñez, Mario; Akhtarshenas, Azim; López-Pérez, David; Geraci, Giovanni. Optimizing Aerial Base Station Positioning Using Deep Reinforcement Learning in UAV Networks. \textit{International Conference on Communications}.


\section{Projects}

\begin{tabularx}{\textwidth-0.4 cm-0.13cm}{K{0.2 cm} R{4.1 cm}}

\textbf{Adversarial Attacks on Image-to-Image Generative Models}

\vspace{0.10 cm}

\begin{highlights}
    \item Investigated the robustness of \textbf{Image-to-Image generative models} against \textbf{adversarial attacks}, such as \textbf{Automatic Projected Gradient Descent}, analyzing vulnerabilities and defenses, such as \textbf{RobustCLIP}
    \item Developed adversarial strategies targeting models based on \textbf{Diffusion Models }, specifically assessing the stability of embeddings
    \item Conducted extensive experiments evaluating the performance impacts of adversarial perturbations across datasets
    \item Supervised by Prof. Volkan Cevher and Elias Abad Rocamora 
\end{highlights}

\end{tabularx}

\vspace{0.2 cm}

\begin{tabularx}{\textwidth-0.4 cm-0.13cm}{K{0.2 cm} R{4.1 cm}}

\textbf{Impact of Whole-Slide Image Resolution on Pathology Models}

\vspace{0.10 cm}

\begin{highlights}
    \item Systematically analyzed how changes in \textbf{Whole-Slide Image (WSI)} resolution affect the performance of histopathological foundation models
    \item Implemented a preprocessing pipeline to generate and validate tiles across varying magnification levels (5x, 10x, 20x, 40x), ensuring data integrity and consistency
    \item Utilized \textbf{Multiple-Instance Learning (MIL)} techniques, including mean pooling and gated attention mechanisms, to aggregate embeddings from the UNI model
    \item Conducted ablation studies on three breast histology datasets (BACH, BRACS, BreakHis) demonstrating the importance of magnification levels and model architecture choices
\end{highlights}

\end{tabularx}

\section{Additional Experience and Awards}

    \begingroup\leftskip=0.2 cm
    \advance\csname @rightskip\endcsname 0.2 cm
    \advance\rightskip 0.2 cm

    \textbf{IAESTE Faculty Delegate:} Served a full academic year as a delegate for the IAESTE organization, focusing on securing international job placements for students. Actively engaged with both students and employers, facilitating successful job matches and supporting students throughout their application processes \par\endgroup

    \vspace{0.2 cm}
    \begingroup\leftskip=0.2 cm
    \advance\csname @rightskip\endcsname 0.2 cm
    \advance\rightskip 0.2 cm

    \textbf{Honors at Upper Secondary School:} Achieved Honors in the last year\par\endgroup

    
    \section{Skills and Interests}

        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm

        \textbf{Programming:} Python, Pytorch, Tensorflow, Keras, Bash, Assembly, SystemVerilog, Matlab, Java(Beginner), Git, LaTeX, Docker \par\endgroup

        \vspace{0.2 cm}
        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm

        \textbf{Languages:} Spanish (Native), English (Fluent), French (B1)  \par\endgroup
         \vspace{0.2 cm}
        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm
        \textbf{Interests:} Sports (Gym, Running, Tennis and Boxing in particular), Technological Advancements, Nature, Rubik Cube, Teaching \par\endgroup



    

\end{document}



RL PROJECT 06.2025 FOR EPFL COURSE EE-REINFORCEMENT-LEARNING
\documentclass{article}

\usepackage[final]{neurips_2024}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}  % for [H]
\usepackage{titling}  % if using \maketitle or custom title spacing
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}

% \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}  % LAST
\usepackage{xcolor}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
% \usepackage[colorlinks=true, linkcolor=mydarkblue, citecolor=mydarkblue, urlcolor=mydarkblue]{hyperref}

% \usepackage[round]{natbib}
\newcommand{\takeaway}[1]{\vspace{1mm}{\color[HTML]{1d5c38}\textbf{{$\triangleright$\hspace{5pt}#1}}}}
% \newcommand{\takeaway}[1]{\vspace{1mm}{\color{mydarkblue}\textbf{{$\triangleright$\hspace{5pt}#1}}}}

\title{Evolution Strategies for Deep RL
pretraining}

\author{
  Adrian Martínez López, Ananya Gupta, Hanka Goralija, Mario Rico Ibáñez, \\
  Saúl Fenollosa Arguedas, Tamar Alphaidze \\
  École Polytechnique Fédérale de Lausanne (EPFL)
}

\begin{document}

\maketitle
 \begin{abstract}
%In this project, we explore Evolution Strategies as both a standalone reinforcement learning method and a pretraining step for gradient-based algorithms like DQN and PPO. We tested these approaches on Flappy Bird, Breakout, and MuJoCo Walker environment, covering both discrete and continuous action spaces. Flappy Bird showed that ES-pretrained agents could learn good policies quickly and reach high rewards faster than DQN alone. In Breakout, ES struggled due to the high-dimensional image input, failing to make progress even with large populations. In MuJoCo, ES pretraining did not improve PPO performance, likely because PPO relies on separate actor and critic networks, while ES only optimizes a single policy. Overall, ES is useful for early exploration in simple environments, but its limitations become clear in more complex tasks.

Although Deep Reinforcement Learning has proven highly effective for complex decision-making problems, it demands significant computational resources and careful parameter adjustment in order to develop successful strategies. Evolution strategies offer a more straightforward, derivative-free approach that is less computationally costly and simpler to deploy. However, ES generally do not match the performance levels achieved by DRL, which calls into question their suitability for more demanding scenarios.
This study examines the performance of ES and DRL across tasks of varying difficulty, including Flappy Bird, Breakout and Mujoco environments, as well as whether ES could be used for initial training to enhance DRL algorithms. The results indicate that ES do not consistently train faster than DRL. When used as a preliminary training step, they only provide benefits in less complex environments (Flappy Bird) and show minimal or no improvement in training efficiency or stability across different parameter settings when applied to more sophisticated tasks (Breakout and MuJoCo Walker).
The code is on \href{https://github.com/talphaidze/Evolution-Strategies-for-Deep-RL-Pretraining}{GitHub}.
 \end{abstract}

%\begin{abstract}
%Deep Reinforcement Learning (DRL) has shown remarkable success in complex sequential decision-making tasks. However, this power comes with a cost, as it often requires extensive training and careful tuning to converge to effective policies. In contrast, Evolution Strategies (ES) have surfaced as a simpler, gradient-free alternative that is computationally cheaper and easier to implement. However, ES methods typically fall short of the performance achieved by DRL, raising questions about their applicability to more challenging environments. In this work, we compare the performance of ES and DRL across environments of varying complexity (Flappy Bird, Breakout and Mujoco) and analyze the trade-offs between these approaches. Additionally, we also investigate the use of ES as a pretraining strategy for DRL algorithms. Our findings show that ES is not consistently faster than DRL and that its effectiveness as a pretraining method is limited to simpler environments, offering little to no impact in training speed and robustness to hyperparameter selection in more complex tasks.
%\end{abstract}
\vspace{-10pt}
\section{Introduction}
\vspace{-5pt}
Deep Reinforcement Learning (DRL) has shown remarkable success in tackling complex sequential decision-making problems \citep{sutton2018reinforcement}. However, this power comes with a cost, as it often requires extensive training and careful tuning to converge to effective policies~\citep{dulacarnold2019challengesrealworldreinforcementlearning, ghosh2021generalizationrldifficultepistemic}. In contrast, Evolution Strategies (ES) have surfaced as a simpler, gradient-free alternative that is computationally cheaper and easier to implement~\citep{salimans2017evolution}. These algorithms estimate gradients by performing a search over small parameter perturbations, thus avoiding the need to perform expensive gradient computations. However, ES methods typically fall short of the performance achieved by DRL, raising questions about their applicability to more challenging environments.

In this work, we evaluate and compare the performance of these two distinct paradigms: \textbf{DRL}, with algorithms such as Deep Q-Networks (DQN)~\citep{mnih2013playingatarideepreinforcement} and Proximal Policy Optimization (PPO)~\citep{schulman2017proximalpolicyoptimizationalgorithms}, and \textbf{ES}. We perform this analysis across environments that differ in complexity and reward structure. Our experiments span two main domains: arcade-style games with discrete action spaces, such as Flappy Bird and Breakout, and continuous control tasks in high-dimensional state and action spaces, modeled using the Mujoco physics simulator \citep{todorov2012mujoco}. Additionally, we investigate the applicability of ES as a pretraining strategy for DRL algorithms, in order to increase training speed and robustness to hyperparameter selection. 

Our findings show that ES is not consistently faster than DRL approaches, denying the first claim about training speed enhancement. Additionally, pretraining strategies appear to be effective for simpler environments, while they show little to no benefit in more complex tasks. In simple environments such as Flappy Bird, pretraining with ES appears to accelerate the learning curve for DRL algorithms, reaching higher rewards faster than DRL-only approaches. Unfortunately, this behaviour is not consistent on more complex environments, such as Breakout or Mujoco, where ES pretraining does not accelerate training and does not either make DRL algorithms more robust to hyperparameter selection.

% \section{Introduction}

% Reinforcement Learning (RL) has shown remarkable success in tackling complex sequential decision-making problems \citep{sutton2018reinforcement}. In this project, we evaluate and compare the performance of three distinct RL approaches, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Evolution Strategies (ES), across environments that differ in complexity and reward structure. Our experiments span two main domains: arcade-style games such as Flappy Bird and Breakout, implemented using OpenAI Gym \citep{brockman2016openai} with discrete actions. On the other hand, we have continuous control tasks in high-dimensional state and action spaces, modeled using the MuJoCo physics simulator \citep{todorov2012mujoco}, in specific, we used the BRAX version of these environments, \citep{freeman2021braxdifferentiablephysics}. We analyze how traditional gradient-based methods like DQN and PPO perform relative to the gradient-free ES approach, particularly in scenarios with sparse rewards or deceptive gradients \citep{salimans2017evolution}. Additionally, we explore a hybrid training strategy, where ES is used for pre-training to enhance exploration before fine-tuning with gradient-based methods to improve sample efficiency. Our findings highlight the strengths, weaknesses, and synergies of these algorithms in terms of convergence speed, final performance, and sensitivity to hyperparameters.

\vspace{-5pt}
\section{Related Work}
\vspace{-5pt}
\subsection{Evolution Strategies}
\label{sec: ES}
Traditional reinforcement learning approaches typically optimize policy parameters using gradient-based methods, such as policy gradients or actor-critic algorithms \citep{sutton2018reinforcement}. These methods assume that the environment is smooth and differentiable with respect to actions, which allows gradients to propagate through trajectories using the chain rule. However, in many practical scenarios, such as physical simulations or real-world robotic systems, the environment may be non-differentiable, highly stochastic, or possess sparse and delayed rewards. In such cases, standard RL algorithms struggle to find effective policies \citep{dulac2019challenges}.

An alternative to gradient-based RL is to frame the learning problem as a black-box optimization task. In this setting, the policy is viewed as a mapping from states to actions, parameterized by $\theta \in \mathbb{R}^d$, and the objective is to maximize the expected cumulative reward obtained by executing the policy in the environment. Evolution Strategies (ES) offer a solution by searching for optimal parameters through random perturbations and selection based on observed rewards, without requiring access to environmental gradients or backpropagation \citep{salimans2017evolution}.


\subsubsection{Mathematical Formulation}

Let $F(\theta)$ denote the total reward achieved by running a policy with parameters $\theta$ in the environment. The goal is to find parameters $\theta$ that maximize $F(\theta)$. However, since $F$ is assumed to be a black-box function, meaning we do not have access to how the reward depends on individual actions or parameters, direct gradient computation is infeasible. To overcome this, Evolution Strategies optimize a smoothed version of the objective:

\begin{equation}
\tilde{J}(\theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}[F(\theta + \sigma \epsilon)],
\end{equation}

where $\sigma > 0$ is a small noise parameter and $\epsilon \sim \mathcal{N}(0, I)$ is a standard multivariate Gaussian random vector. This formulation introduces smoothing by averaging over nearby parameter vectors, thereby making the objective amenable to gradient estimation.

The gradient of $\tilde{J}$ with respect to $\theta$ can be derived as follows. By the linearity of expectation and the chain rule:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \mathbb{E}_{\epsilon}\left[ \nabla_\theta F(\theta + \sigma \epsilon) \right].
\end{equation}

Since $F$ is a black-box, we cannot directly compute $\nabla_\theta F(\theta + \sigma \epsilon)$. Instead, using the log-likelihood trick, we rewrite the gradient in terms of the score function:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \mathbb{E}_{\epsilon}\left[ F(\theta + \sigma \epsilon) \nabla_\theta \log p_\theta(\theta + \sigma \epsilon) \right],
\end{equation}

where $p_\theta(\cdot)$ is the probability density function of $\mathcal{N}(\theta, \sigma^2 I)$.

Since the Gaussian log-probability satisfies:

\begin{equation}
\log p_\theta(x) = -\frac{1}{2\sigma^2} \|x - \theta\|^2 + \text{const} \implies \nabla_\theta \log p_\theta(x) = \frac{x - \theta}{\sigma^2}.
\end{equation}

Substituting $x = \theta + \sigma \epsilon$ yields:

\begin{equation}
\nabla_\theta \log p_\theta(\theta + \sigma \epsilon) = \frac{\sigma \epsilon}{\sigma^2} = \frac{\epsilon}{\sigma}.
\end{equation}

Thus, the gradient becomes:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \frac{1}{\sigma} \mathbb{E}_{\epsilon} \left[ F(\theta + \sigma \epsilon) \cdot \epsilon \right].
\end{equation}

In practice, this expectation is approximated with Monte Carlo sampling. Given $n$ independent samples $\epsilon_1, \dots, \epsilon_n$, the gradient estimate is:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) \approx \frac{1}{n \sigma} \sum_{i=1}^n F(\theta + \sigma \epsilon_i) \cdot \epsilon_i.
\end{equation}

\subsubsection{Algorithm}

The practical implementation of Evolution Strategies consists of sampling perturbations, evaluating the corresponding perturbed policies in the environment, estimating the gradient based on the collected rewards, and updating the policy parameters accordingly. The complete algorithm can be described as follows:

\begin{algorithm}
\caption{Evolution Strategies Algorithm}
\label{alg:es}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Initial parameters $\theta_0$, learning rate $\alpha$, noise standard deviation $\sigma$, population size $n$
\FOR{iteration $t = 0, 1, 2, \dots$}
    \STATE Broadcast $\theta_t$ to all workers
    \FOR{each worker $i = 1$ to $n$ in parallel}
        \STATE Sample perturbation $\epsilon_i \sim \mathcal{N}(0, I)$
        \STATE Compute perturbed parameters $\theta_i = \theta_t + \sigma \epsilon_i$
        \STATE Execute policy with $\theta_i$ and obtain reward $F_i$
    \ENDFOR
    \STATE Aggregate all $(F_i, \epsilon_i)$ pairs
    \STATE Estimate gradient: $g_t = \frac{1}{n\sigma} \sum_i F_i \epsilon_i$
    \STATE Update parameters: $\theta_{t+1} = \theta_t + \alpha g_t$
\ENDFOR
\end{algorithmic}
\end{algorithm}

Because the evaluations of perturbed policies are independent, Evolution Strategies are naturally suited for parallel computation. In practice, to reduce variance, rewards $F_i$ are often replaced by rank-transformed and mean-centered values, a technique known as fitness shaping.
The current parameters $\theta_t$ are broadcast once to all workers at the start of each iteration. Each worker independently samples noise vectors and computes rewards locally.

To minimize communication overhead, workers do not transmit full parameter vectors. Instead, they communicate only the random seeds used to generate $\epsilon_i$ and the corresponding scalar rewards $F_i$. Given a common random seed initialization, all workers can deterministically reconstruct the sampled perturbations. This design ensures that only $O(1)$ data per rollout is communicated, making the method scalable to very high-dimensional policy spaces.



Unlike policy gradient methods, which inject noise into the action space at every timestep, Evolution Strategies perturb the policy parameters once at the beginning of each episode. This leads to a gradient estimator whose variance is independent of the episode length, making ES particularly robust in long-horizon or sparse-reward environments. Additionally, since ES avoids backpropagation through time, its gradient computation is significantly lighter than that of policy gradient methods.
A drawback, however, is that ES requires complete episode rollouts to compute returns, so the overall update can be delayed if even a single episode within the population takes a long time to finish.


\vspace{-5pt}
\section{Methodology}
\vspace{-5pt}

We aim to evaluate and compare the performance of gradient-based deep reinforcement learning (DRL) algorithms with gradient-free evolution strategies (ES). Specifically, we investigate two hypotheses: (1) whether ES can achieve intermediate performance benchmarks (e.g., reaching 25\% of the optimal reward) faster than DRL algorithms, and (2) whether ES can serve as an effective pretraining method to improve DRL training speed and robustness.

We conduct experiments across three benchmark environments of varying complexity. Concretely, we focus on two main environment domains: two arcade-style games with discrete action spaces, \textbf{Flappy Bird} and \textbf{Breakout}, and one continuous control task modeled using the \textbf{Mujoco} physics simulator \citep{todorov2012mujoco}.

For discrete action environments, we compare the performance of Deep Q-Networks (DQN) and the basic ES implementation, described in Section \ref{sec: ES}. In the continuous Mujoco domain, we use Proximal Policy Optimization (PPO) as the representative DRL method, since DQN is not applicable in continuous settings.

\subsection*{Performance Comparison}

% We train agents using DRL and ES from scratch under controlled and identical experimental conditions. Each agent's performance is evaluated based on final reward and total training time required to reach a maximum reward. The goal is to compare training speed and post-training model performance, while assessing how robust a training algorithm is to hyperparameter, environment and seed variation

% To evaluate ES as a pretraining method, we first train agents with ES for a fixed number of episodes and use the resulting parameters to initialize the DRL networks. This allows us to test whether such initialization leads to faster convergence or improved stability in DRL training. Full implementation details are provided in Section~\ref{sec:experimental_setup}.

We train DRL and ES agents from scratch under identical conditions, comparing them based on final reward, training time, and robustness to hyperparameters, environment, and seed variation. To evaluate ES as a pretraining method, we initialize DRL networks with parameters from ES-trained agents and assess whether this improves convergence or stability. Full implementation details are in Section 4.

% \section{Methodology}

% Our goal is to compare the performance of gradient-based DRL algorithms against gradient-free ES methods. Additionally, we also evaluate the applicability of ES as a pretraining step for DRL algorithms. Our hypothesis is that, being a simpler and cheaper training method, this pretraining strategy could help speed up training or make algorithms more robust to hyperparameter selection.

%  We perform our analysis across three environments of varying complexity. Concretely, we focus on two main environment domains: arcade-style games with discrete action spaces, \textbf{Flappy Bird} and \textbf{Breakout}, and continuous control tasks in high-dimensional state and action spaces, modeled using the \textbf{Mujoco} physics simulator \cite{todorov2012mujoco}.

% For the discrete action space environments, we perform a comparison between DQN and ES, whereas for the Mujoco environment we use PPO as the DRL algorithm, given that DQN cannot work with continuous action spaces.

% \subsection*{Performance Comparison}
% We compare the performance of DRL and ES in Flappy Bird, Breakout and Mujoco under identical conditions. For each environment and strategy, we train an agent from scratch and evaluate their performance based on final reward and total training time required to reach a maximum reward. The goal is to compare training speed and post-training model performance, while assessing how robust is a training algorithm to hyperparameter, environment and seed variation. Regarding the pretraining strategy, we first train an agent for a short period using ES and then initialize the DRL network with the set of obtained params. Implementation details are described in Section \ref{sec:experimental_setup}.


% \subsection{Pretraining}
% Mention the Pretraining Setting

% Mention the two hypotheses:
% 1. Does pretraining with ES speed up DRL algorithms?
% 2. Does it make them more robust to hyperparam selection?


% Our goal was to compare the performance of gradient-based and gradient-free reinforcement learning methods across environments of varying complexity. We implemented and evaluated performance of Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Evolution Strategies (ES) across three environments: Flappy Bird, Breakout, and MuJoCo (HalfCheetah, Hopper and Walker). The first two environment, Flappy Bird and Breakout, have discrete action spaces, while MuJoCo involves continuous control. DQN was applied to the discrete environments, PPO to the continuous one, and ES was evaluated across all three settings.

% Each algorithm was trained independently under comparable conditions, with hyperparameters selected based on standard implementations or empirical tuning. For each environment, we trained agents from scratch and measured learning progress over time. In addition, we explored a hybrid approach in which ES was used for pretraining the network parameters before fine-tuning with a gradient-based algorithm. This approach was motivated by ES's strong exploration capabilities, particularly in sparse-reward environments. Specifically, the best-performing policy obtained through ES was used to initialize the hidden layers of a DQN (or PPO) agent, while the output layers were retrained during fine-tuning. By doing this we leverage the low-computation, short-time convergence properties of Evolution Strategies (ES), we can quickly obtain a reasonable policy initialization. However, ES methods often reach a plateau where further learning stagnates. To overcome this limitation, we subsequently fine-tune the model using Reinforcement Learning (RL), which allows for more granular reward optimization and continued policy improvement.
\vspace{-0.3cm}
\section{Experimental Setup}
In this section, we present the configuration details for the environments used in our experiments. Specifically, \cref{sec:experimental_setup_flappy_bird} outlines the setup for the Flappy Bird environment, \cref{sec:experimental_setup_breakout} covers the Breakout environment, and \cref{sec;experimental_setup_mujoco} details the configuration for the MuJoCo environment. 

\label{sec:experimental_setup}
\vspace{-0.15cm}
\subsection{Flappy Bird}
In \cref{sec:flappy_bird_environment_details}, we describe the configuration of the Flappy Bird environment used in our experiments, including the state representation and reward structure. In \cref{sec:flappy_bird_model_arch}, we present the model architectures explored and explain how they were integrated into the hybrid approach. The parameters used for this environment can be seen in \cref{sec:flappy_bird_training_params}.
\label{sec:experimental_setup_flappy_bird}
\vspace{-0.15cm}
\subsubsection{Environment Details}
\label{sec:flappy_bird_environment_details}
% The experiments were conducted in the FlappyBirdEnv, a custom environment implemented using the Python Learning Game Engine. The environment simulates the dynamics of the Flappy Bird game and provides visual observations. Frame skipping was set to 4, meaning that each selected action was repeated for four consecutive frames, with the corresponding rewards accumulated. The reward structure included a small positive reward of +0.1 for each timestep the bird remained alive and an additional +1.0 reward for each pipe successfully passed. To penalize failure, each time a terminal state was reached, typically due to collision with an obstacle or the ground, the agent received a negative reward of -1.0. This setup encouraged agents to maximize survival while progressing through the game.
Experiments were conducted in FlappyBirdEnv, a custom environment built with the Python Learning Game Engine that simulates the dynamics of the Flappy Bird game. Frame skipping was set to 4, repeating each action for four frames with cumulative rewards. The agent received +0.1 reward per timestep survived and +1.0 for each pipe passed, while collisions or failures resulted in a -1.0 penalty. This reward structure encouraged maximizing survival and progression.
\vspace{-0.15cm}
\subsubsection{Model Architecture}
\label{sec:flappy_bird_model_arch}
Both DQN and ES agents were based on a fully connected multilayer perceptron policy with two hidden layers of 64 units each and Tanh activation functions. The output layer produced a vector of scores corresponding to each discrete action. For DQN, this represented Q-values used for greedy or $\epsilon$-greedy action selection. In ES, the same output was used to deterministically select actions via argmax, with the reward used to guide population updates. This shared architecture facilitated weight transfer when initializing DQN from a pretrained ES policy.
% \subsubsection{Logging and Checkpointing}
% During training, we logged the mean episode reward over time to track how well the agents were learning. For DQN, we saved checkpoints regularly so we could go back to earlier points in training if needed, or continue training from where we left off. For ES, we didn’t use checkpoints in the same way, since it works differently - each generation is evaluated and replaced based on performance. However, at the end of training, the best-performing policy in the ES population was exported for initialization in DQN. All results were plotted with respect to cumulative training time, allowing fair comparison of learning dynamics across DQN, ES, and hybrid ES→DQN configurations.
\vspace{-0.15cm}
\subsection{Breakout}
\label{sec:experimental_setup_breakout}
In \cref{sec:breakout_environment_details}, we detail the configuration of the Breakout environment used in our experiments, including both image-based and RAM-based setups. \Cref{sec:breakout_model_arch} outlines the model architectures selected for each input type and their compatibility with the hybrid training framework. The parameters used for the breakout environment can be seen in \cref{sec:breakout_training_parmas}.
\vspace{-0.15cm}
\subsubsection{Environment Details}
\label{sec:breakout_environment_details}
The experiments were conducted using the Atari Breakout environment, a classic arcade game where the agent controls a paddle to bounce a ball and break bricks. The goal is to clear all bricks without letting the ball fall. Breakout is a standard reinforcement learning benchmark due to its visual complexity, sparse rewards, and need for precise control.

The action space includes four discrete actions: NOOP, FIRE, MOVE LEFT, and MOVE RIGHT. The agent earns +1 for each brick broken. An episode ends when all lives are lost or the level is cleared. To evaluate different input modalities, we used two environment variants:

For \textbf{Image-based setup (ALE/Breakout-v5)}, The agent observes stacked grayscale frames (84×84×4), capturing both spatial and temporal dynamics. This high-dimensional input emphasizes the role of vision and sequence learning.

For \textbf{RAM-based setup (Breakout-ram-v4)}, The agent receives a 128-dimensional vector representing the game’s internal memory state, offering a compact but semantically rich input. This setup tests whether ES performs better in low-dimensional, non-visual spaces.

% \begin{itemize}
%     \item{\textbf{Image-based setup (ALE/Breakout-v5)}: In this version, the agent observes raw pixel data consisting of game screen frames. To capture motion and temporal dependencies, each observation was formed by stacking the four most recent grayscale frames. This results in a high-dimensional 3D observation space capturing spatial and temporal information.}
%     \item{\textbf{RAM-based setup (Breakout-ram-v4)}: This configuration uses a compact, 128-dimensional vector representing the game's internal memory state, bypassing visual input entirely. It provides a lower-dimensional but semantically rich representation of the environment. The action and reward structure remains the same as in the image-based version. This setup was specifically used to test whether evolutionary strategies could perform better in a low-dimensional, non-visual input space.}
% \end{itemize}
\vspace{-0.25cm}
\subsubsection{Model Architecture}
\label{sec:breakout_model_arch}
% The model architecture was selected dynamically (RAM-based (vetorized) input for ES to simplify the environment and standard image-based for DQN) based on the input type. For pixel-based environments (ALE/Breakout-v5), a convolutional neural network (CNN) architecture (CnnPolicy) was used, matching the original DQN architecture suitable for high-dimensional visual input. For RAM-based environments (Breakout-ram-v4), a fully connected multilayer perceptron (MlpPolicy) was used with a custom architecture defined by two hidden layers of 256 units each (net\_arch: [256, 256]). The same model implementation was compatible with both the gradient-based DQN training and the gradient-free Evolution Strategy training, as it adhered to a shared BaseModel interface supporting parameter access, perturbation, and evaluation.
For \textbf{Image-based setup (ALE/Breakout-v5)}, A convolutional neural network (CNN) was used to process stacked grayscale frames (84×84×4), following the standard DQN design. This architecture, consisting of three convolutional layers and two fully connected layers, includes approximately 850,000 parameters. It is well-suited for extracting spatial and temporal features but is computationally intensive.

For \textbf{RAM-based setup (Breakout-ram-v4)}, A multilayer perceptron (MLP) with two hidden layers of 256 units each was used for the 128-dimensional RAM input. This simpler architecture has around 90,000 parameters, making it more efficient but less expressive for complex spatial reasoning.

Both models were implemented under a shared interface supporting parameter access, perturbation, and evaluation, enabling use with both DQN and Evolution Strategies.

\vspace{-0.15cm}
\subsection{Mujoco Environments}
\label{sec;experimental_setup_mujoco}
In \cref{sec:environment_details_mujoco} and \cref{sec:model_arch_mujoco}, we describe how the MuJoCo environments from the Brax library were configured, trained, and architecturally implemented for our experiments. These environments were selected due to their diversity in dynamics, reward structures, and control complexity, making them ideal testbeds for evaluating and comparing gradient-based (PPO) and gradient-free (ES) reinforcement learning methods under consistent and reproducible conditions. The parameters are detailed in \cref{sec:Training_parameters_mujoco}.
\vspace{-0.25cm}
\subsubsection{Environment Details}
\label{sec:environment_details_mujoco}
We evaluated agent performance on three MuJoCo benchmark environments from the Brax library: HalfCheetah, Hopper, and Walker2d. Each environment involves planar locomotion in 2D with continuous control over joint torques. HalfCheetah tasks a 6-actuator robot with maximizing forward velocity and does not terminate episodes early. Hopper is a single-legged agent with a 3-dimensional action space, terminating episodes upon falling. Walker2d, a bipedal version of Hopper with six joints, also ends episodes if the agent becomes unstable. All environments allow configuration of key parameters like control cost, forward reward weight, and initial state noise. These tasks vary in dynamics and reward structure, making them well-suited for comparing gradient-free and gradient-based reinforcement learning methods. Furthermore, BRAX environments are differentiable, making them a fair comparision of ES and RL. 
\vspace{-0.25cm}
\subsubsection{Model Architecture}
\label{sec:model_arch_mujoco}

For all MuJoCo environments, we used a fully connected neural network as the policy architecture. The network consisted of 4 hidden layers, each with 32 units and non-linear activation functions. This architecture was kept lightweight to facilitate efficient optimization and compatibility across algorithms.

When using ES as a pretraining step for PPO, this network was employed as the actor component. The weights obtained from ES training were transferred to initialize the PPO actor. For the critic network, however, we initialized a separate randomly seeded network, allowing PPO to learn the value function independently. This setup enabled seamless integration between ES and PPO while preserving the actor-critic structure essential for PPO’s learning dynamics.
\section{Results}
\subsection{Flappy Bird}
\begin{figure}[htbp]
\centering
  \includegraphics[width=0.825\textwidth]{images/flappy.pdf}
  \caption{Smoothed learning curves for ES, DQN, and ES-pretrained DQN versus cumulative training time in Flappy Bird environment}
  \label{fig:flappy}
\end{figure}
In the Flappy Bird environment, as we can see on Figure \cref{fig:flappy}, ES demonstrated strong learning capabilities. Within a relatively short training time, ES was able to find a stable policy that achieved consistent survival and reward accumulation. While DQN eventually reached higher final rewards, it required significantly more training steps and showed greater sensitivity to hyperparameters and random seeds. When comparing the two methods, it’s important to note that DQN was trained in parallel across multiple environments, whereas ES was trained sequentially. Additionally, DQN experienced sudden drops in performance during training, from which it often did not recover. This kind of instability might be caused by large updates in the wrong direction, possibly due to overestimated Q-values or noisy gradients, which pushed the policy into worse regions and caused it to reinforce poor decisions through the replay buffer. When DQN was initialized with policy parameters obtained from ES, the agent reached competitive performance much faster compared to training from scratch. These results suggest that ES is particularly effective in simple, sparse-reward environments like Flappy Bird, where its stable and robust exploration helps provide a strong starting point for gradient-based fine-tuning.
\subsection{Breakout}

% Figure~\ref{fig:breakout} showcases a performance comparison between DQN and ES on different versions of the Breakout environment, as described in Section~\ref{sec:experimental_setup_breakout}. After analyzing the results, we highlight two main takeaways:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.825\linewidth]{images/breakout_es_vs_dqn.pdf}
    \caption{Smoothed learning curves for ES and DQN versus cumulative training time in Breakout environment}
    \label{fig:breakout}
\end{figure}

% In our experiments, the Deep Q-Network (DQN) using a convolutional neural network (CNN) policy consistently achieved higher mean rewards compared to Evolution Strategies (ES), as we can see on figure \ref{fig:breakout}. However, the DQN's learning curve exhibited greater variance and was more sensitive to hyperparameter settings. When ES was applied to the DQN CNN policy in the original Breakout environment with image-based input, it performed poorly, plateauing at a low mean reward of around 1.5 even when using a population size of 50. To address the high input dimensionality, we tested ES with a DQN multilayer perceptron (MLP) policy in the “Breakout-ram-v4” environment, where the agent receives raw RAM states instead of pixel observations. This version of ES showed faster initial learning and achieved a mean reward of 4 within a few generations, but similarly plateaued early and failed to improve further. These results indicate that ES struggles to scale in environments with high-dimensional or temporally extended input spaces, whereas DQN maintains a performance advantage due to its capacity for fine-grained gradient-based updates and its use of temporal difference learning.

In our experiments, as shown in Figure \ref{fig:breakout}, the Deep Q-Network (DQN) with a CNN policy consistently achieved higher mean rewards of around 30, whereas Evolution Strategies (ES) plateaued at much lower rewards. From these observations, we highlight two main takeaways:

\takeaway{DQN consistently outperforms ES in Breakout, achieving mean rewards around 30 on the image-based environment.}
DQN’s CNN policy effectively handles high-dimensional pixel inputs, but its learning curve exhibits greater variance and is sensitive to hyperparameter tuning.

\takeaway{ES struggles to scale in complex settings, plateauing early in both pixel-based and RAM-based environments.}
ES applied to the CNN policy in the image-based environment plateaued at a mean reward near 1.5 despite a large population size. When using a simpler MLP policy on RAM-based inputs, ES showed faster initial progress with rewards reaching around 4 but failed to improve further. These results underscore ES’s difficulty extracting useful representations in high-dimensional and temporally extended settings, unlike the gradient-based updates and temporal difference learning used by DQN.

% \takeaway{DQN consistently outperforms ES in Breakout, despite greater sensitivity to hyperparameters.}  
% In the image-based Breakout environment, DQN with a CNN policy consistently achieves substantially higher rewards than ES. However, the DQN's learning curve exhibits greater variance and is more sensitive to hyperparameter settings

% \takeaway{ES fails to scale in high-dimensional or temporally complex settings, plateauing even in simplified environments.}  
% ES struggles in both pixel-based and RAM-based versions of Breakout, showing limited progress and early performance plateaus. These results highlight ES's difficulty in extracting useful representations from complex input spaces, in contrast to the more adaptive nature of gradient-based methods like DQN.

\vspace{-1.5mm}

\subsection{MuJoCo}

% SAY THAT WE FIND THAT DEPENDING ON THE ENVIRONMENT THAT WE CHOOSE, THE PPO ALGORITHM MIGHT PERFORM EXTREMELY WELL AND SOLVE THE ENVIRONMENT REALLY FAST, OR NOT SOLVE THE ENVIRONMENT AT ALL. WE SHOW THAT, ON THE OTHER HAND, EVOLUTION STRATEGIES ARE NOT AS FAST, BUT THEY SEEM TO BE MORE ROBUST TO ENVIRONMENT SELECTION.

% THEN WE SAY THAT WE HAVE TRIED PRETRAINING WITH ES AND THEN TRAINING WITH PPO TO SEE IF PPO WOULD BE MORE ROBUST TO HYPERPARAMETER SELECTION, BUT THIS IS NOT THE CASE GIVEN THAT THE MODEL PERFORMS IN THE SAME MANNER.

Figures \ref{fig:mujoco_combined} showcases a performance comparison between PPO and ES on different Mujoco environments, described in Section \ref{sec;experimental_setup_mujoco}. After analyzing the obtained results, we come up with two main takeaways: 

\takeaway{PPO performs inconsistently across seeds and environments, while ES is slower but yields significantly more stable and repeatable outcomes.}

PPO demonstrates strong performance in some environments but can be unstable and highly sensitive to hyperparameter choices. In HalfCheetah (Figure \ref{fig:half_cheetah_es_ppo}), PPO converges 20x faster than ES. However, in other environments such as Walker2d (Figure \ref{fig:walker_es_ppo}) or Hopper (Figure \ref{fig:hopper_es_ppo}), PPO does not manage to converge and oscillates between low reward values. On the other hand, ES reliably solves most of the evaluated environments, failing to fully solve Walker2d. However, its convergence is much slower compared to PPO (20x slower in HalfCheetah).


\takeaway{Pretraining with ES does not improve PPO's training speed nor enhance robustness to hyperparameter selection.}  

While ES eventually solves most environments, it is significantly slower, up to 20× in HalfCheetah, and consistently fails to reach intermediate reward thresholds faster than PPO. This lag in early performance undermines its viability as a pretraining strategy for accelerating PPO's training. On the other hand, despite initializing PPO with parameters obtained from ES, the resulting training behavior remains largely unchanged (Figure \ref{fig:hopper_es_ppo}). PPO continues to exhibit similar sensitivity to hyperparameters, indicating that ES pretraining does not improve training stability. This results could stem from the fact that PPO uses an actor-critic, structure, while ES only accounts for the actor network optimization.

\noindent

% \vspace{-1cm}
\begin{figure}[htbp]
    \centering

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/hopper_combined.pdf}
        \caption{Smoothed learning curves for ES, PPO, and ES-pretrained PPO versus cumulative training time in Hopper environment.}
        \label{fig:hopper_es_ppo}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/half_cheetah_es_vs_ppo.pdf}
        \caption{Smoothed learning curves for ES and PPO versus cumulative training time in Half Cheetah environment.}
        \label{fig:half_cheetah_es_ppo}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/walker_es_vs_ppo.pdf}
                \caption{Smoothed learning curves for ES and PPO versus cumulative training time in Walker environment.}

        \label{fig:walker_es_ppo}
    \end{subfigure}

    \caption{Performance comparison of ES, PPO, and ES pretraining across various Mujoco environments.}
    \label{fig:mujoco_combined}
\end{figure}





% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{HopperReward.png}
% \end{figure}

% \vspace{1em}
% \noindent

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{HalfCheetahReward.png}
%         \caption{HalfCheetah}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{WalkerReward.png}
%         \caption{Walker}
%     \end{subfigure}
 
% \end{figure}

% \vspace{1em}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.4\linewidth]{mujocolegend.png}
%     \caption*{\small{Legend: ES, PPO, and ES-pretrained PPO.}}
% \end{figure}

\vspace{1em}


\vspace{-0.5em}




% \section{Discussion}

% Our experiments show that Evolution Strategies can be a useful tool in reinforcement learning, particularly during the early stages of training. In the Flappy Bird environment, ES quickly found stable policies and provided a strong initialization for DQN, helping it reach high performance faster and with more stability. This highlights the value of ES in sparse-reward, low-dimensional tasks where exploration is more important than fine-tuned value estimates.

% However, the performance of DQN was inconsistent across runs. We observed cases where the agent's reward dropped sharply and failed to recover. This instability is likely due to overestimated Q-values or large updates based on noisy targets, which can lead the network into worse regions of the parameter space and reinforce poor behavior through the replay buffer.

% In contrast, Breakout proved much more challenging for ES. The high-dimensional image input significantly increased the complexity of the policy space, and even with larger populations, ES failed to make meaningful progress. Switching to a RAM-based version helped slightly, but performance still plateaued early. This suggests that ES is less effective in environments that require processing large observation spaces or learning long temporal dependencies.

% In the continuous MuJoCo environments, PPO outperformed ES in terms of final performance. While ES was able to reach moderate rewards, its convergence was slower, and using ES for pretraining PPO did not lead to improvements. This is likely due to architectural differences, as PPO relies on separate actor and critic networks, while ES only optimizes a single policy network. Transferring knowledge between these architectures remains a challenge.
\vspace{-0.3cm}
\section{Limitations and next steps}
%Even though ES have demonstrated strong performance in solving DRL tasks, and in some cases even outperformed traditional gradient-based methods, they do not appear to be effective as a pretraining mechanism for RL algorithms, contrary to our initial hypothesis. Due to architectural and learning paradigm mismatch:
%This is perhaps the most significant obstacle. In environments like Flappy Bird and MuJoCo, the knowledge acquired during ES training often failed to transfer meaningfully to subsequent reinforcement learning. We attribute this to two main factors: (i) structural differences between the algorithms-particularly in how actor-critic methods like PPO use separate value and policy networks versus the single-policy optimization in ES-and (ii) fundamental differences in the optimization process itself. While ES relies on population-based black-box search, RL depends on local gradient information. As a result, the representations learned by ES may not align with the assumptions or learning dynamics of gradient-based methods, limiting their effectiveness for fine-tuning.

%Although Evolution Strategies (ES) have shown strong performance in solving deep reinforcement learning (DRL) tasks - and in some cases even outperform traditional gradient-based methods - they did not prove effective as a pretraining mechanism for reinforcement learning algorithms, contrary to our initial hypothesis. We identify two key limitations:

%First, there is an architectural and learning paradigm mismatch. In environments such as Flappy Bird and MuJoCo, knowledge acquired during ES training often failed to transfer meaningfully to subsequent reinforcement learning. This stems from:
%(i) structural differences between the algorithms - actor-critic methods like PPO rely on separate value and policy networks, whereas ES uses a single-policy optimization approach; and
%(ii) fundamental differences in the optimization process - ES is a population-based black-box search method, while RL leverages local gradient information. As a result, representations learned through ES may not align with the assumptions or learning dynamics of gradient-based algorithms, limiting their effectiveness for fine-tuning. 

%Second, ES struggled to perform well in more complex environments such as Breakout. Even when using a vectorized (feature-based) version of the environment instead of the raw-pixel CNN-based version, performance remained poor. This suggests that ES has difficulty scaling to environments with high-dimensional or visually complex state spaces, further limiting its utility in such settings.

%Future research should focus on developing adaptive or architecture-aware hybrid approaches to improve transfer between ES and deep RL algorithms. This may include methods that align ES-learned representations with RL objectives, use shared modules between phases, or adapt architectures to reduce incompatibility. Such strategies could enhance the effectiveness of ES as a pretraining stage, especially in complex environments where pure ES struggles.
Although Evolution Strategies (ES) have shown strong performance in some DRL tasks - occasionally outperforming gradient-based methods - they were ineffective as a pretraining mechanism for RL, contrary to our hypothesis. We identify two main limitations:

First, ES and RL differ significantly in architecture and learning dynamics. In environments like Flappy Bird and MuJoCo, ES-trained policies did not transfer well due to differences in structure (e.g., PPO's separate value and policy networks vs. ES's single-policy optimization) and optimization methods (black-box search vs. gradient-based learning), leading to incompatible representations.
Second, ES performed poorly in complex environments such as Breakout. Even with vectorized (feature-based) inputs, results were weak, and performance degraded further with raw-pixel (CNN-based) inputs - highlighting ES’s difficulty in scaling to high-dimensional tasks.

Future work should explore adaptive, architecture-aware hybrid approaches that improve transfer between ES and RL, such as aligning learned representations, using shared modules, or modifying architectures to bridge the gap.


\vspace{-0.3cm}
\section{Conclusion}

Our results show that ES can effectively accelerate early learning in simple environments like Flappy Bird, especially when used to initialize gradient-based methods like DQN. However, ES struggled to scale to high-dimensional tasks like Breakout and was not compatible with PPO due to architectural differences. While DQN and PPO achieved better final performance overall, they were more sensitive to hyperparameters and showed variability across runs. These findings suggest that ES is a useful tool for exploration in low-complexity settings, but combining it with gradient-based methods in more complex tasks remains a challenge. Future work could explore adaptive or architecture-aware hybrid approaches that improve transfer between ES and deep RL algorithms.

\newpage
\bibliographystyle{chicago}
\bibliography{ref}


\newpage
\appendix
\section{Deep Q-Networks (DQN)}

The Deep Q-Network (DQN) algorithm combines Q-learning with deep neural networks to handle high-dimensional state spaces in reinforcement learning. It approximates the action-value function $Q(s, a; \theta)$ with a neural network parameterized by weights $\theta$, where $s$ is the current state and $a$ is the action taken.

\subsection{Mathematical Formulation}

The goal is to learn an optimal policy that maximizes the expected return by estimating the action-value function:

\begin{equation}
Q^*(s, a) = \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \,|\, s_0 = s, a_0 = a, \pi \right],
\end{equation}

where $\gamma \in [0, 1]$ is the discount factor and $r_t$ is the reward at timestep $t$. In Q-learning, the optimal $Q$-function satisfies the Bellman equation:

\begin{equation}
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \,|\, s, a \right].
\end{equation}

DQN minimizes the temporal-difference (TD) error between the current estimate $Q(s, a; \theta)$ and the target value $y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})$, where $\theta^{-}$ are the parameters of a separate target network. The loss function is:

\begin{equation}
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( y - Q(s, a; \theta) \right)^2 \right],
\end{equation}

where $D$ is the experience replay buffer. The target $y$ is treated as a fixed value during the optimization step:

\begin{equation}
y = r + \gamma \max_{a'} Q(s', a'; \theta^{-}).
\end{equation}

To stabilize training, DQN introduces two key mechanisms:

\begin{itemize}
    \item \textbf{Experience Replay:} A buffer $D$ stores past transitions $(s, a, r, s')$, and mini-batches are sampled uniformly from it to break correlation between sequential data.
    \item \textbf{Target Network:} A separate network with parameters $\theta^{-}$ is used to compute the target values. Its parameters are periodically updated to match $\theta$.
\end{itemize}

\subsection{Algorithm}

The DQN algorithm can be summarized as follows:

\begin{algorithm}
\caption{Deep Q-Network (DQN)}
\label{alg:dqn}
\begin{algorithmic}[1]
\STATE Initialize replay buffer $D$ with capacity $N$
\STATE Initialize Q-network with weights $\theta$
\STATE Initialize target Q-network with weights $\theta^{-} = \theta$
\FOR{each episode}
    \STATE Initialize environment and receive initial state $s$
    \FOR{each timestep}
        \STATE With probability $\epsilon$ select a random action $a$, otherwise $a = \arg\max_{a} Q(s, a; \theta)$
        \STATE Execute action $a$, observe reward $r$ and next state $s'$
        \STATE Store transition $(s, a, r, s')$ in $D$
        \STATE Sample random minibatch from $D$
        \STATE Compute target $y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})$
        \STATE Perform gradient descent on $\left( y - Q(s, a; \theta) \right)^2$
        \STATE Update $s \leftarrow s'$
        \STATE Every $C$ steps: $\theta^{-} \leftarrow \theta$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

By combining off-policy Q-learning with deep neural networks, DQN has been successfully applied to challenging domains such as Atari games from raw pixels, achieving human-level performance in many cases.


\section{Parameters}
\label{sec:Parameters}
In \cref{sec:flappy_bird_training_params}, we detailed the hyperparameters used to train agents in the Flappy Bird environment. \Cref{sec:breakout_training_parmas} presents the training parameters for the Breakout experiments, covering both DQN and ES configurations. Finally, \cref{sec:Training_parameters_mujoco} outlines the training setup adopted for the MuJoCo environments, including environment-specific hyperparameters and standardized PPO configurations from the Brax benchmark suite. Together, these sections provide a comprehensive overview of the experimental conditions under which each algorithm was evaluated.

\subsection{Flappy Bird Training Parameters}
\label{sec:flappy_bird_training_params}
The Deep Q-Network (DQN) agent was trained for 1,000,000 timesteps, corresponding to approximately 5000 episodes. Training was parallelized across 8 synchronous environments to speed up sample collection and stabilize learning. The agent followed an $\epsilon$-greedy exploration strategy, with $\epsilon$ annealed linearly from 0.2 to 0.0001 over the first 10\% of training. Additional hyperparameters included a learning rate of 5e-5, a replay buffer size of 10,000 transitions, a mini-batch size of 32, and a discount factor $\gamma$ of 0.90.

The Evolution Strategies (ES) algorithm used a population size of 16 and a Gaussian noise standard deviation of $\sigma=0.05$. It was trained sequentially for 1000 generations with a learning rate of 0.005. ES operated directly on the policy parameters, using returns as a fitness measure to guide the search. After training, the trained ES policy was used to initialize the hidden layers of the DQN network. This provided a warm start for DQN, allowing it to begin learning from a reasonably good policy rather than from scratch.
\subsection{Breakout Training Parameters}
\label{sec:breakout_training_parmas}
For DQN, we trained the agent using the following hyperparameters: a learning rate of 2e-4, buffer size of 100,000 transitions, batch size of 32, training frequency of every 4 steps, and one gradient update per training step. Target network updates were performed every 1,000 steps. Training followed an $\varepsilon$-greedy policy, where $\varepsilon$ decayed from 0.1 to 0.01 over time. DQN training was managed through Stable-Baselines3 and included support for both CnnPolicy and MlpPolicy depending on the input type.

For Evolution Strategy, training was conducted over 500 generations with a population size of 50. Each individual was evaluated over 5 episodes, and symmetric Gaussian noise was applied to the model parameters using a standard deviation (sigma) of 0.2. The learning rate for ES updates was set to 0.01. Parameters were updated using the reward-weighted average of perturbations, normalized by the population standard deviation. Checkpoints were saved every 100 generations, and metrics were logged using Weights \& Biases (wandb). For both ES and DQN we tried different hyperparameters and arrived on the parameters specified in this report.

\subsection{Mujoco Training Parameters}
\label{sec:Training_parameters_mujoco}

For all MuJoCo tasks, we followed the default PPO training configurations provided in the Brax benchmark suite \citep{freeman2021braxdifferentiablephysics}. These setups were optimized for efficient and stable learning in continuous control environments using large-scale parallel simulation (num envs = 8192).
\begin{itemize}
\item \textbf{Walker2d}: Trained for $7{,}864{,}320$ timesteps with $20$ evaluation points. The reward was scaled by $5$, and the discount factor was $\gamma = 0.997$. We used a learning rate of $6 \times 10^{-4}$, batch size $128$, and $32$ gradient updates per environment step. Training began after $8192$ transitions were collected, with a replay buffer of size $2^{20}$.

\item \textbf{HalfCheetah} and \textbf{Hopper}: Both trained for $6{,}553{,}600$ timesteps using similar settings, but with a higher reward scaling factor of $30$, batch size $512$, and $64$ gradient updates per step to support faster locomotion dynamics.
\end{itemize}



All environments used observation normalization to stabilize training, with each state input standardized to reduce variance across features. Actions were applied at every simulation step without repetition (i.e., action repeat was set to 1), ensuring fine-grained control. A fixed random seed (set to 1) was used to ensure reproducibility of results. Training was constrained to one computational device per host, matching the single-device setup used in the original Brax benchmarks. These standardized settings ensured fair comparison between the PPO and ES approaches across all environments.



\end{document}

MNLP PROJECT 06.2025 FOR EPFL COURSE MNLP
\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage[table]{xcolor}  % Optional, only needed if you want to color cells
\usepackage{graphicx}
\usepackage{makecell} % Include in preamble if not already present
\usepackage{siunitx}
\usepackage[most]{tcolorbox}
\usepackage{fvextra} % for breakable verbatim
\tcbuselibrary{listings}
\sisetup{round-mode=places, round-precision=3}
\usepackage{listings}

\definecolor{codebg}{rgb}{0.97,0.97,0.97}
\definecolor{keywordcolor}{rgb}{0.2,0.2,0.6}
\definecolor{stringcolor}{rgb}{0.6,0.1,0.1}

\lstdefinestyle{config}{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{keywordcolor}\bfseries,
  stringstyle=\color{stringcolor},
  showstringspaces=false,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  captionpos=b,
}


%%%%%%%%%%%%%%%%%%% TITLE AND AUTHORS %%%%%%%%%%%%%%%%%%%

\title{Chatsplaining: EPFL best classmate}

\author{\normalfont 
Tamar Alphaidze | 393635 | \texttt{tamar.alphaidze@epfl.ch} \\
Mario Rico Ibáñez | 395172 | \texttt{mario.ricoibanez@epfl.ch} \\
Adrián Martínez López | 396379 | \texttt{adrian.martinez@epfl.ch} \\
Jon Lecumberri Arteta | 386801 | \texttt{jon.lecumberriarteta@epfl.ch} \\
Chatsplaining \\
}

%%%%%%%%%%%%%%%%%%% PROPOSAL %%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%  MACROS  %%%%%%%%%%%%%%%%%%%%
\newcommand{\qwenb}{Qwen-0.6B }
\newcommand{\qwenbbase}{Qwen-0.6B-Base }
\newcommand{\qwentulu}{QWEN + TuLU }
\newcommand{\qwentulumcqa}{TuLU + MCQA  }
\newcommand{\qwentulumcqamath}{TuLU + MCQA + MATH }
\newcommand{\lora}{LoRA }
\newcommand{\omath}{OpenMath }
\newcommand{\tuluds}{tulu-v2-sft-mixture }
\newcommand{\unimcqa}{unified-4choice-mcqa}

%%%%%% datasets of unified %%%%%%
\newcommand{\medmcqa}{MedMCQA }
\newcommand{\mmlu}{MMLU }
\newcommand{\swag}{SWAG }
\newcommand{\race}{RACE }
\newcommand{\hellaswag}{HellaSwag }
\newcommand{\cosmosqa}{CosmosQA }
\newcommand{\quail}{QuAIL }
\newcommand{\openbookqa}{OpenBookQA }
\newcommand{\sciq}{SciQ }
\newcommand{\aquarat}{AquaRat }

\newcommand{\allenaimath}{ARC }
\newcommand{\allenaiarc}{MathQA }

\newcommand{\todomario}[1]{\textcolor{red}{TODO: #1}}


\newcommand{\graniteembed}{granite-embedding-107m-multilingual}








\begin{document}
\maketitle
\begin{abstract}
% This project investigates and compares multiple training strategies for natural language understanding using the \texttt{\qwenbbase} model, with a focus on multiple-choice question answering (MCQA) in STEM domains. We implement and evaluate four model variants: (1) supervised fine-tuning (FT), (2) quantization for efficiency, (3) retrieval-augmented generation (RAG) for enhanced factual grounding, and (4) direct preference optimization (DPO) for alignment with human preferences. The MCQA models are evaluated on datasets such as SciQ, MedQA, and AquaRat, while open-answer tasks and preference datasets inform the generative and alignment components. Our results show that ...
The convergence of advanced natural language processing with educational pedagogy presents a unique opportunity to democratize high-quality, personalized tutoring experiences on a large scale. In this paper, we investigate and compare multiple training strategies for natural language understanding using the \texttt{Qwen/Qwen0.6b-Base} language model, with a focus on multiple-choice question answering (MCQA) in STEM domains. We implement and evaluate four model variants: (1) supervised fine-tuning (SFT), (2) quantization for efficiency, (3) retrieval-augmented generation (RAG) for enhanced factual grounding, and (4) direct preference optimization (DPO) for alignment with human preferences. We demonstrate that a two-stage curriculum learning approach, composed of instruction-based SFT followed by succesive SFTs on specific labeled MCQA data, is critical for reliable reasoning, and that DPO can effectively align model outputs to human judgments.
\end{abstract}


\section{Introduction}

Although human one-on-one tutoring is excellent at providing personalised instruction and emotional support, its high cost, limited availability and scalability issues mean that many learners do not have access to individualised educational support \cite{bloom1984two}. The recent advances in Large Language Models offer a solution to this problem \cite{KASNECI2023102274, brown2020languagemodelsfewshotlearners}. The natural language capabilities of LLMs can be leveraged to engage students in meaningful dialogue, provide personalized explanations, and adapt pedagogical strategies regardless of the time, place, and the student's social background. 
Unlike conventional computer-assisted instruction, which relies on pre-programmed responses, LLMs can generate contextually appropriate feedback and identify and address specific misconceptions to guide students towards a deeper understanding, rather than simply providing answers \cite{KASNECI2023102274}. Furthermore, their ability to communicate in natural language eliminates interaction barriers, making educational support more accessible and intuitive for learners of all ages and technical proficiency. The convergence of advanced natural language processing with educational pedagogy presents a unique opportunity to democratize high-quality, personalized tutoring experiences on a large scale.
In this paper, we attempt to build such intelligent tutor by fine-tuning the \texttt{Qwen3-0.6B-Base} language model from Alibaba \cite{qwen3technicalreport}.

In order to achieve this, we explore the development and training of four different models of natural language understanding, all based on the \texttt{\qwenbbase} language model. The task spans multiple paradigms of model training and optimization, including supervised fine-tuning, quantization, retrieval-augmented generation (RAG), and direct preference optimization (DPO). The final objective is to evaluate the performance of each approach across multiple-choice question answering (MCQA) tasks, especially in STEM-related domains.

Our baseline MCQA model employs traditional supervised fine-tuning on labeled multiple-choice data, while a LoRA variant \cite{hu2021loralowrankadaptationlarge} enables parameter-efficient training through low-rank adaptation. To address computational constraints, we develop Quantized MCQA models using 4-bit and 8-bit quantization (W4A8 and W8A8) via the \texttt{llm-compressor} library  \cite{llmcompressor2024}, trading some accuracy for significant reductions in memory usage and inference latency. Our RAG model \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} addresses knowledge limitations by dynamically retrieving external information during inference, enabling factual grounding beyond the base model's encoded knowledge. Finally, we implement Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} to align model outputs with human preferences through direct fine-tuning on preference pairs, bypassing traditional reward modeling approaches. These methodologies collectively examine the spectrum of efficiency, accuracy, and alignment considerations in modern language model deployment.

In the sections that follow, we describe the training methodologies, evaluation benchmarks, and findings associated with each approach.


\section{Approach}
\label{sec:approach}

We followed an approached based on different stages. This is done mainly because the \qwenbbase is a pre-trained non usable model, it is just trained on a bunch of data but does not answer in a proper way. Then, we first trained it to follow instructions, by this way the model is more responsible and usable on different domains. We called this a \textbf{foundational training}. After that we have other stages for \textbf{MCQA} models and \textbf{DPO}. For the first ones, we trained on general knowledge MCQA and then improve it on math topics, to be more correlated with the final objective, \textbf{EPFL related questions}. For the latter we perform a DPO finetuning to align the model with human preferences and enhance its preference selection capabilities.

\subsection{Multiple-Choice Question Answering}

Across this project there are three models than rely on an MCQA one as their backbone: MCQA, Quantized MCQA and RAG. In this section the common approach for these is presented. The goal is to be able to predict the correct answer of STEM-based EPFL course material multiple choice questions. From the foundational training presented above, this task required the implementation of a significative prompt as well as further fine-tuning towards more MCQA-related data. In all models cross-entropy loss was used between the predicted token distribution and the correct answer label.

\subsubsection*{MCQA Base model}

In all cases the MCQA task was framed as a multiple-choice classification problem over natural language input. Each input instance consisted of a context passage (optional), a question $q$, and a set of four answer choices $\{a_1, a_2, a_3, a_4\}$. The model subsequently learned to select the correct answer $a^* \in \{a_1, \dots, a_4\}$. 
%We finetune the \texttt{\qwenbbase} model using supervised learning to solve this task.
Each example was formatted into a prompt with the following structure, while full prompt can be seen in Appendix \ref{apen: prompt}:
{\scriptsize
\begin{verbatim}
Context: <context> \n Question: <question> \n 
A. <answer_1> \n B. <answer_2> \n
C. <answer_3> \n D. <answer_4> \n
Answer:
\end{verbatim}
}

As mentioned above, the model was trained to predict the correct letter (A-D) as the next token. 

Our approach combines the Instruction Following Phase with MCQA and domain-specific fine-tuning. Following the foundational training based on Instruction Following, we fine-tuned the model on a mixture of MCQA datasets to develop general proficiency in multiple-choice question answering across diverse domains. This stage allows the model to learn the specific format patterns required for MCQA tasks while maintaining broad knowledge coverage. Finally, recognizing our ultimate objective of creating an intelligent model for STEM and EPFL-like content, we conducted a third training stage focused specifically on mathematics and logical reasoning MCQA data. This targeted approach ensures that the model develops the specialized capabilities necessary for handling the complex quantitative and analytical problems characteristic of EPFL coursework, while building upon the general instruction-following and MCQA foundations established in the previous stages. Figure \ref{fig:mcqa_model} shows the full pipeline implemented for the MCQA model.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{MCQA-fine_tuning.png}
  \caption{Architecture of the MCQA model using Combined Approach}
  \label{fig:mcqa_model}
\end{figure}

\subsubsection*{Quantized model}

To reduce inference latency and VRAM usage, we applied post-training quantization to the supervised MCQA model using the \texttt{llm-compressor} library  \cite{llmcompressor2024}. Specifically, we experimented with both W8A8 (8-bit weights, 8-bit activations) and W4A8 (4-bit weights, 8-bit activations) quantization schemes.

We applied SmoothQuant as a pre-processing step to reduce activation outliers and improve quantization robustness. SmoothQuant applies a layer-wise scaling transformation:
\[
\tilde{x} = x / \alpha, \quad \tilde{W} = \alpha W
\]
where $\alpha$ is a learned smoothing factor for each layer, $x$ is the activation input, and $W$ is the weight matrix. This reduces dynamic range in activations and concentrates representational power in quantized weights.

Following this, we applied GPTQ (Group-wise Post-training Quantization) to all linear layers except the final \texttt{lm\_head}. GPTQ optimizes for minimal quantization error by analytically solving for the best low-bit approximation to the original weights, using second-order statistics from calibration data.

We evaluated both quantized models using log-likelihood accuracy on held-out MCQA samples. %While W8A8 retained similar performance to the original model with reduced VRAM usage, W4A8 showed degradation in accuracy-highlighting the trade-off between compression and task performance.
\subsubsection*{Retrieval-Augmented Generation}
To enhance the model's factual grounding with specific and evolving information from EPFL coursework, we implement a \textbf{Retrieval-Augmented Generation (RAG)} system consisting of a \textbf{retriever} and a \textbf{generator}. The retriever's role is to identify relevant context using the \textbf{\graniteembed{}} model \cite{awasthy2025graniteembeddingmodels}, an efficient 107-million-parameter dense retriever. For this, an external corpus of EPFL materials and textbooks is chunked and encoded into 384-dimensional vectors, which are indexed in a \textbf{FAISS} (Facebook AI Similarity Search) vector store for efficient retrieval. At inference time, a user's query is embedded with the same model, and a \textbf{Maximum Inner Product Search (MIPS)} equivalent to cosine similarity for normalized vectors retrieves the top-2 most similar passages. This context is then passed to the generator.

\subsection{Direct Preference Optimization}

We frame the DPO task as a binary classification problem over natural language input. Each input instance consists of a prompt $p$, which provided context and a question, and a pair of answer choices $\{chosen, rejected\}$. The model subsequently learns to select the human-preferred, chosen answer, thus aligning model behaviour to a more human style. 
Each example is formatted into a triplet with the following structure:
\begin{verbatim}
Prompt: <prompt>
Chosen: <chosen>
Rejected: <rejected>
\end{verbatim}
As mentioned above, the model is trained to maximize the difference between the rewards assigned to the chosen and rejected responses.

In order to train the model, we perform a DPO finetuning on top of the instruction-following SFT, described in subsection \ref{sec:approach}. Performing a prior SFT is helpful since the model follows instructions more reliably and the \textit{chosen} and \textit{rejected} answer pairs are not out-of-distribution. Figure \ref{fig:dpo_model} shows the full pipeline for the DPO model.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{DPO_finetuning.png}
  \caption{Architecture of the DPO model using Combined Approach}
  \label{fig:dpo_model}
\end{figure}

\section{Experiments}

This section contains all relevant information regarding the different experiments that were done in order to produce the final models. 

\subsection{Data}

\subsubsection{Instruction Following Dataset}

As commented in section \ref{sec:approach}, we perform an initial instruction-following finetuning on our 2 model architectures. The goal of this first training stage is to obtain a model that behaves like an assistant, capable of following instructions reliably. To accomplish that, we used \texttt{\tuluds}, a high-quality instruction-tuning mixture that integrates data from sources such as FLAN, ShareGPT, OpenAssistant, Alpaca, and others \cite{ivison2023camelschangingclimateenhancing}. Each open-answer entry is formatted with: \texttt{question} (input prompt), \texttt{answer\_text} (free-form answer), \texttt{source} (dataset identifier), and \texttt{explanation} (optional reasoning or context).

\subsubsection{DPO Dataset}
\label{sec:dpo_data}
We introduce the following dataset composed of different preference question pairs from different domains and sources. This dataset is a blend of the following sets:
\vspace{-0.25em}
\paragraph{M1 Preference Data:}M1 preference questions with answer pairs regenerated by Google Gemini 2.5 Pro in order to have better quality answers. The dataset was crafted in a meticulous manner, avoiding length biases and ensuring that the rejected response is slightly worse than the chosen one. This dataset replaces the original M1 preference set annotated by MNLP's students.
\vspace{-0.25em}
\paragraph{Nectar:}a 7-wise comparison dataset with 7 responses per prompt, ranked by GPT-4 based on helpfulness and harmlessness. It covers diverse domains such as math, science, coding, and writing. Responses come from models like GPT-4, LLaMA-2, and Mistral. We select the best response as chosen and randomly select the rejected answer from the rest~\cite{starling2023}.
\vspace{-0.25em}
\paragraph{StepDPO:}A math-reasoning preference pair dataset consisting of 10k prompts. This dataset looks to reinforce the model's mathematical capabilities~\cite{lai2024stepdpostepwisepreferenceoptimization}.
\vspace{-0.25em}
\paragraph{UltraFeedback:}A collection of 20k prompts with GPT-4-ranked responses for helpfulness and honesty \cite{cui2023ultrafeedback}.

All datasets are converted into a unified for-
mat containing the following fields: \texttt{prompt}, \texttt{chosen}, \texttt{rejected}.

\subsubsection{MCQA Dataset}
% We use several multiple-choice QA datasets, each targeting different domains and reasoning types, highlighting:
% \begin{itemize}[noitemsep, topsep=0pt, leftmargin=0.2em]
%     \item \textbf{\sciq}: General science questions intended for middle and high school students \cite{SciQ}.
%     \item \textbf{\medmcqa}: Medical reasoning questions requiring professional-level domain knowledge \cite{wu2025medreasonelicitingfactualmedical}.
%     \item \textbf{\aquarat}: Multi-step arithmetic and logical reasoning problems that test mathematical and deductive capabilities \cite{ling2017program}.
%     \item \textbf{\allenaimath}:  large-scale dataset of math word problems \cite{amini-etal-2019-mathqa}. 
%     \item \textbf{\hellaswag}:  formatted as a collection of multiple-choice questions with the goal of finishing  \cite{zellers2019hellaswag}.
%     \item \textbf{\mmlu}: massive multitask test consisting of multiple-choice questions from various branches of knowledge \cite{hendryckstest2021}. Only non benchmark splits were used.
%     \item \textbf{\swag}: consists of multiple choice questions about grounded situations \cite{zellers2018swagaf}.
%     \item \textbf{\race}:  large-scale reading comprehension dataset \cite{lai-etal-2017-race}
%     \item \textbf{\cosmosqa}: problems that require commonsense-based reading comprehension, formulated as multiple-choice questions \cite{huang-etal-2019-cosmos}.
%     \item \textbf{\quail}: reading comprehension dataset \cite{DBLP:conf/aaai/RogersKDR20}. 
%     \item \textbf{\openbookqa}: questions necessitating multiple steps of reasoning, integration of extra common and commonsense knowledge, as well as deep text comprehension skills \cite{OpenBookQA2018}.
% \end{itemize}

We employ several multiple‐choice QA datasets grouped by domain:  
\textbf{Science}: \sciq{} \cite{SciQ} for middle‐ and high‐school science;  
\textbf{Medical}: \medmcqa{} \cite{wu2025medreasonelicitingfactualmedical} for professional‐level medical reasoning;  
\textbf{Math \& Logic}: \aquarat{} \cite{ling2017program} for multi‐step arithmetic and logical problems, \allenaimath{} \cite{amini-etal-2019-mathqa} for large‐scale math word problems, and MathQA \cite{amini2019mathqainterpretablemathword} for mathematical reasoning;
\textbf{Commonsense \& Situational}: \hellaswag{} \cite{zellers2019hellaswag} and \swag{} \cite{zellers2018swagaf} for grounded and situational commonsense completion;  
\textbf{Reading Comprehension}: \race{} \cite{lai-etal-2017-race}, \cosmosqa{} \cite{huang-etal-2019-cosmos}, and \quail{} \cite{DBLP:conf/aaai/RogersKDR20};  
\textbf{Multi‐disciplinary Knowledge}: \mmlu{} (non‐benchmark splits) \cite{hendryckstest2021} for broad subject coverage;  
\textbf{Open‐Book Reasoning}: \openbookqa{} \cite{OpenBookQA2018} for multi‐step reasoning integrating external knowledge. Finally, the MCQA from the M1 was also added.






% %%%%%% datasets of unified %%%%%%
% \newcommand{\medmcqa}{MedMCQA }
% \newcommand{\mmlu}{MMLU }
% \newcommand{\swag}{SWAG }
% \newcommand{\race}{RACE }
% \newcommand{\hellaswag}{HellaSwag }
% \newcommand{\cosmosqa}{CosmosQA }
% \newcommand{\quail}{QuAIL }
% \newcommand{\openbookqa}{OpenBookQA }
% \newcommand{\sciq}{SciQ }
% \newcommand{\aquarat}{AquaRat }

% \newcommand{\allenaimath}{ARC }
% \newcommand{\allenaiarc}{MathQA }


Importantly, all datasets but \texttt{ARC} and \texttt{MathQA} are combined to make single merged dataset for MCQA fine-tuning (\texttt{\unimcqa}), and \texttt{ARC} and \texttt{MathQA} are combined separately to make a dataset for subsequent domain-specific fine-tuning (\texttt{arc\_math\_qa\_merged}).

All datasets are converted into a unified format containing the following fields: \texttt{question}, \texttt{choices} (answer options), \texttt{answer\_index} (correct option index), \texttt{answer\_text} (correct answer content), \texttt{source} (original dataset), and \texttt{explanation}. 


\subsubsection{Quantization Dataset}
Calibration set is used to estimate the distribution of activations and optimize quantization thresholds.
We employ a held-out calibration set for quantization, sampled from the MCQA fine-tuning datasets \texttt{\unimcqa} and \texttt{arc\_math\_qa\_merged} -- which are used for MCQA and domain-specific fine-tunings respectively, as explained above. 
We employed a mixed-dataset approach, combining 256 samples each from \texttt{\unimcqa} and \texttt{arc\_math\_qa\_merged} datasets (512 total samples), which were shuffled with a fixed seed (42) for reproducibility. The calibration samples were formatted into a structured prompt template including \texttt{context}, \texttt{question}, \texttt{multiple-choice}, \texttt{options}, and \texttt{answer} fields, then tokenized with a maximum sequence length of 512 tokens without padding or special tokens.
While using the validation split would be methodologically preferable, we utilized the training split due to the unavailability of validation data. 

\subsubsection{RAG Dataset}
\noindent The efficacy of our RAG system depends on its external knowledge base, for which we curated a composite corpus from two distinct sources for broad and specialized coverage. The first source provides a general scientific foundation through a collection of freely available STEM textbooks. The second, more critical source consists of specialized EPFL course materials, including lecture notes and past exams, sourced from a student repository. This EPFL dataset underwent a rigorous cleaning pipeline to remove duplicate and incomplete files, ensuring index integrity. All documents were subsequently segmented into coherent passages of up to 512 tokens, resulting in a final knowledge base of approximately 39,000 chunks for indexing.

\subsection{Evaluation Method}

\subsubsection{MCQA-based models}

We evaluated our model's performance on a broad suite of established benchmarks. For mathematical and logical reasoning, we used \omath, \aquarat, the \allenaimath dataset, and \allenaiarc. For other skills, we employed \hellaswag for commonsense inference, \medmcqa for medical knowledge, \mmlu for broad academic knowledge, and \sciq for science question answering. All reported scores are on the official validation and test sets of these benchmarks.

\subsubsection{DPO model}
\label{subsec:dpo_evals}
We evaluate our DPO model across a range of preference-based benchmarks. For code generation, we use a code preferences dataset (CP) specially tailored for DPO~\cite{vezora_code_preference_pairs}, and for mathematical reasoning, we include RewardMath (RM) and Metamath-DPO (MM)~\cite{kim2024evaluatingrobustnessrewardmodels, pal2024smaugfixingfailuremodes}. We also evaluate on the MNLP DPO benchmark via the LightEval framework. To assess broader alignment, we use the held-out test set from UltraFeedback (UF). Finally, we evaluate over RewardBench~\cite{RewardBench}, a benchmark for evaluating reward models across chat, safety, and reasoning. It uses human-verified prompt–chosen–rejected trios and supports DPO-style evaluation to assess alignment quality.



\subsection{Baselines}
 For the general model, references \qwenbbase and \qwenb are employed to compare fine-tuning improvements and quality against an official post-trained version. For the Quantized model, the baseline is the final MCQA model with instruction following, MCQA, and domain-specific knowledge to track accuracy changes during quantization. For the DPO model, \qwenb serves as the sole baseline, since \qwenbbase is used as the reference model. We also compare against other DPO-aligned models such as Qwen1.5-Chat~\cite{qwen}, 
Zephyr-7B-beta~\cite{tunstall2023zephyr} or Llama-3-tulu-2-dpo-8b~\cite{ivison2023camels}. 


\subsection{Experimental details}

We provide an extensive explanation of the hyperparameters, base models and code details used for each model training phase in Appendix \ref{apen: experimental details}

% \subsubsection{Foundational training Experimental Details}

% We fine-tuned \texttt{Qwen/Qwen3-0.6B-Base} on the \texttt{\tuluds} for one epoch using per-device batch size 2, gradient accumulation 8, learning rate $2\times10^{-5}$, and max gradient norm 1.0. Training employed BF16 precision, gradient checkpointing, and a linear LR scheduler with warmup ratio 0.03. Logging every 20 steps, saving every 100 steps, reporting to WandB and with Seed 42.

% \subsubsection{SFTs for MCQA Experimental Details}

% We fine-tuned the model outputted from the foundational training stage using a two-phase curriculum learning approach. 
% First, for general MCQA adaptation, the model was trained on a broad mixture of datasets (\texttt{\unimcqa}) with a learning rate of $6 \times 10^{-6}$.
% Subsequently, to enhance mathematical and logical reasoning, the resulting model was further fine-tuned on the \texttt{\allenaiarc} dataset split using a more conservative learning rate of $2 \times 10^{-6}$. For both stages, we trained for one epoch using the AdamW optimizer, BF16 precision, gradient checkpointing, and a cosine learning rate scheduler with 50 warmup steps. The best model was selected based on evaluation loss.

% \subsubsection{Quantized Experimental Details}
% We conducted model quantization experiments using the \texttt{llm-compressor} library to optimize the MCQA final model for efficient inference. The base model was loaded with automatic device mapping and \texttt{float16} precision to manage memory usage effectively. For calibration data, We applied a two-stage quantization recipe combining SmoothQuant (smoothing strength 0.8) for activation conditioning followed by GPTQ quantization targeting all Linear layers while excluding the language modeling head (lm\_head). Two quantization schemes were evaluated: W8A8 (8-bit weights and 8-bit activations) and W4A8 (4-bit weights and 8-bit activations), both implemented through the oneshot API. The quantization process utilized the full calibration dataset for compression statistics, and the resulting models were saved using the save\_compressed flag. Post-quantization validation was performed through sample text generation to verify model functionality.

\subsection{Results}

The results of MCQA-based models across the selected benchmarks can be seen in Table \ref{tab:evaluations}. Appendix \ref{apen: comp} contains more detailed results as another experiment skipping the foundational training and just focusing on SFT and SFT + LoRA was implemented to further evaluate performances. Tables \ref{tab:DPO-pairs-result} and \ref{tab:DPO rewardbench} contain results for the DPO models. Appendix \ref{appendix:rag_performance} shows a detailed explanation of RAG results. 
Regarding the memory usage of the quantized model,  W4A8 quantization exhibited slightly higher peak VRAM usage (1237.18 MB) compared to W8A8 (1221.32 MB) during inference, despite producing a smaller final model and despite them having identical benchmark results.


\begin{table}[h!]
\scriptsize
\centering
\begin{tabularx}{\linewidth}{lX X X X X}
\toprule
\textbf{Task} & \textbf{Qwen-0.6B-Base} & \textbf{Qwen-0.6B} & \textbf{\qwentulumcqamath} & \textbf{Quantized (W8A8)/ (W4A4)} & \textbf{RAG} \\
\midrule
Hellaswag    & 0.2507   & 0.2504   & \textbf{0.4633} & 0.4600 & 0.4563 \\
MathQA       & 0.2419   & 0.2419   & \textbf{0.3142} & 0.3114 & 0.3114 \\
Aqua         & 0.2200 & 0.2200 & 0.3415 & \textbf{0.3527} & 0.3415 \\
MedQA      & 0.1000 & 0.1000 & 0.4231 & \textbf{0.4248} & 0.4196 \\
MMLU         & 0.1600 & 0.1600 & \textbf{0.5016} & 0.4993 & 0.4956 \\
SciQ         & 0.2700 & 0.2700 & \textbf{0.8426} & 0.8396 & 0.8416 \\
EPFL         & 0.1896 & 0.1879 & \textbf{0.3238} & 0.3104 & 0.3171 \\
\bottomrule
\end{tabularx}
\caption{Accuracy comparison across QA benchmarks.}
\label{tab:evaluations}
\end{table}

\begin{table}[h!]
\scriptsize
\centering
\begin{tabularx}{\linewidth}{lX X X X X}
\toprule
\textbf{Model} & \textbf{CP} & \textbf{MNLP} & \textbf{RM} & \textbf{UF} & \textbf{MM} \\
\midrule
Qwen1.5-0.5B-Chat & 0.665 & 0.472 & 0.641 & 0.416 & 0.819 \\
Qwen1.5-4B-Chat & 0.855 & 0.557 & 0.628 & 0.579 & 0.944 \\
Qwen1.5-72B-Chat & \textbf{0.975} & 0.790 & \textbf{0.708} & \textbf{0.650} & 0.917 \\
Llama-3-tulu-2-dpo-8b & 0.849 & 0.583 & 0.364 & 0.595 & 0.950 \\
Zephyr-7b-beta & 0.813 & 0.536 & 0.350 & 0.598 & 0.826 \\
Qwen3-0.6B & 0.619 & 0.599 & 0.588 & 0.455 & 0.845 \\
Base + SFT & 0.643 & 0.572 & 0.513 & 0.509 & 0.769 \\
Base + DPO & 0.918 & 0.814 & 0.609 & 0.627 & 0.949 \\
\textbf{Base + SFT + DPO} & 0.928 & \textbf{0.841} & 0.624 & 0.645 & \textbf{0.947} \\
\bottomrule
\end{tabularx}
\caption{DPO Performance of selected models across different preference pair selection benchmarks. Evaluation benchmarks are detailed in subsection \ref{subsec:dpo_evals}.}
\label{tab:DPO-pairs-result}
\end{table}



% \begin{table*}[h!]
% \small
% \centering
% \begin{tabularx}{\textwidth}{lXXXXXX}
% \toprule
% \textbf{Task} & \textbf{MCQA final} & \textbf{Quantized (W8A8)} & \textbf{Quantized (W4A8)} & \textbf{Qwen-0.6B-Base}\\
% \midrule
% hellaswag       & 0.4633 & 0.4600 & 0.4600 & n/a \\
% mathQA            & 0.3142 & 0.3114 & 0.3114  & n/a \\
% aqua      & 0.3415 & 0.3527 & 0.3527  & 0.2200 \\
% medmcqa        & 0.4231 & 0.4248 & 0.4248  & 0.1000 \\
% mmlu            & 0.5016 & 0.4993 & 0.4993  & 0.1600 \\
% sciq            & 0.8426 & 0.8396 & 0.8396  & 0.2700 \\

% \bottomrule
% \end{tabularx}
% \caption{Performance accuracy comparison across QA benchmarks}
% \label{tab:evaluations}
% \end{table*}% % % % % % % % % % % % % % % % % % 

% \begin{table*}[h!]
% \small
% \centering
% \begin{tabular*}{\textwidth}{l S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3]}
% \toprule
% \textbf{Model} & \textbf{Code Preferences} & \textbf{MNLP DPO} & \textbf{RewardMath} & \textbf{UltraFeedback} & \textbf{Metamath} \\
% \midrule
% Qwen3-0.6B & 0.619 & 0.599 & 0.588 & 0.455 & 0.845 \\
% Qwen3-0.6B-Base + SFT & 0.643 & 0.572 & 0.513 & 0.509 & 0.769 \\
% Qwen3-0.6B-Base + DPO & 0.918 & 0.814 & 0.609 & 0.627 & 0.949\\
% \textbf{Qwen3-0.6B-Base + SFT + DPO} & \textbf{0.928} & \textbf{0.841} & \textbf{0.624} & \textbf{0.645} &\textbf{0.947} \\
% \bottomrule
% \end{tabular*}
% \caption{DPO Performance of selected models across different preference pair selection benchmarks.}
% \label{tab:DPO pairs result}
% \end{table*}


\begin{table}[h!]
\scriptsize
\centering
\begin{tabular*}{0.95\linewidth}{@{\extracolsep{\fill}} l S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] }
\toprule
\textbf{Model} & \textbf{Chat} & \textbf{Chat Hard} & \textbf{Safety} & \textbf{Reasoning}\\
\midrule
Qwen1.5-0.5B-Chat & 0.355 & 0.629 & 0.570 & 0.598 \\
Qwen1.5-4B-Chat & 0.388 & 0.627 & 0.557 & 0.669 \\
Qwen1.5-72B-Chat & 0.623 & 0.660 & 0.676 & 0.855\\
Zephyr-7b-beta & 0.953 & 0.627 & 0.657 & 0.779 \\
Llama-3-tulu-2-dpo-8b & 0.953 & 0.535 & 0.665 & \textbf{0.866}  \\
Qwen3-0.6B & 0.2458	& \textbf{0.662} & 0.6919 & 0.6586 \\
Base + SFT & 0.673	& 0.5088	& 0.4743	& 0.5103	 \\
Base + DPO & 0.950	& 0.410	& 0.7432	& 0.7441	\\
\textbf{Base + SFT + DPO} & \textbf{0.958} &	0.360 &	\textbf{0.757} &	0.828 \\
\bottomrule
\end{tabular*}
\caption{RewardBench reward modelling metrics for the baseline \qwenb model and different finetuning combinations against the \qwenbbase reference model.}
\label{tab:DPO rewardbench}
\end{table}



\section{Analysis} 

\paragraph{MCQA model}  
\label{sec:analysis_mcqa_model}
% We conducted a qualitative error analysis on held-out examples. The combined \qwentulumcqamath model delivers richer implicit reasoning and fewer misinterpretations of negation or multi-step logic, for instance, it correctly solves chained AquaRat arithmetic where the SFT-only model often guesses. It still struggles with out-of-distribution external facts (e.g., recent figures) due to lack of retrieval. Removing the TuLU stage led to a 4-point drop on HellaSwag and SciQ, confirming the value of diverse instruction pre-training.

We analyzed held‐out examples and found that the \qwentulumcqamath model, trained via instruction, tuning to emit only the correct answer letter delivers the most consistent accuracy and clean outputs. Which then in the evaluation is assesed as correct. Other variants sometimes match or exceed its benchmark scores but frequently produce spurious tokens, repetitions, or full‐text answers. In contrast, our two‐stage model correctly handles chained AquaRat arithmetic where SFT‐only and LoRA models fail. Removing the TuLU stage causes a 4‐point drop on HellaSwag and SciQ, highlighting the critical role of instruction‐based tuning.  

\paragraph{Quantized model}
The quantization experiments demonstrated successful compression with minimal performance degradation. Both W8A8 and W4A8 schemes preserved nearly identical accuracy to the full-precision model across all benchmarks, with differences under 0.01 in most cases. This indicates that aggressive quantization can be applied to MCQA tasks without substantial quality loss.
Notably, W4A8 quantization required higher peak VRAM (1237.18 MB vs 1221.32 MB for W8A8) during inference, likely due to more complex dequantization operations required for 4-bit weights, despite producing a smaller final model. The two-stage SmoothQuant + GPTQ approach effectively maintained model quality while achieving significant memory savings, making quantized deployment viable for resource-constrained environments without compromising educational utility. The quantized models occasionally outperformed the base model because quantization acts as regularization, reducing overfitting, and the calibration process may have aligned representations better with certain tasks, though these small improvements likely fall within normal evaluation variance.

\paragraph{RAG model}
The effectiveness of RAG proved heavily contingent on the generator's foundational training. As detailed in Appendix \ref{appendix:rag_performance}, models extensively fine-tuned for MCQA, such as \qwentulumcqa{} and \qwentulumcqamath, gained no benefit from RAG and occasionally showed slight degradation. We hypothesize that these models overfit to the rigid MCQA prompt structure, treating retrieved context as distracting noise rather than an informative signal.
Conversely, the \qwentulu{} model, built on a broader instruction-following SFT, effectively leveraged retrieved context, including from custom embeddings. This is evidenced by the performance lift seen when comparing its baseline in Table \ref{tab:mcqa_results} to its RAG results in Table \ref{tab:rag_vidyc_base_model_tulu_sft_results}. This outcome reinforces our analysis in Section \ref{sec:analysis_mcqa_model}, confirming that general instruction tuning fosters the adaptability required to integrate external knowledge.
\paragraph{DPO model}

We observe that DPO significantly enhances the model's ability to assign rewards in alignment with human preferences across all evaluated benchmarks. Additionally, SFT prior to DPO leads to further improvements, though the gains are relatively modest. Notably, a compact model like Qwen3-0.6B, when fine-tuned with DPO in the right domains, demonstrates performance on par with or even exceeding that of much larger models such as Qwen1.5-72B-Chat, also fine-tuned with DPO. However, despite these improvements in reward modeling, we have also observed that DPO tends to encourage repetitive generations. We attribute this to a potential side effect of optimizing for preference pair selection, rather than directly improving the model's generative capabilities. In this setting, it is important to recognize that we are effectively training a reward model, not a generator. 

\section{Ethical considerations}
We fine-tune \qwenbbase{} to aid EPFL students with MC and open‐ended questions, raising several ethical points:
\begin{enumerate}[noitemsep, topsep=0pt, leftmargin=*]
  \item \textbf{Language Adaptation}: Extending to high‐resource languages is straightforward; low‐resource languages demand targeted data and fairness evaluation.
  \item \textbf{Benefits vs.\ Risks}: Improves learning support but may enable dishonesty, over‐reliance, or bias propagation.
  \item \textbf{Mitigation}: Provide clear guidelines, restrict to approved materials, conduct bias audits, and emphasize the tool as a supplement.
  \item \textbf{Equity}: Underrepresented students may be disadvantaged; diverse data and user feedback are essential.
\end{enumerate}

\section{Conclusion}
Our exploration of fine-tuning strategies for \qwenbbase reveals that a multi-stage curriculum is superior to direct task-specific training. We found that a foundational instruction-following SFT is essential for model adaptability, as further specialization on MCQA data introduces a critical trade-off: improved task accuracy at the cost of conversational ability due to format overfitting. Our key findings demonstrate that: 1) post-training quantization is highly effective, preserving performance with negligible loss; 2) a model's ability to leverage RAG is contingent on its conversational foundation, not its MCQA specialization; 3) Direct Preference Optimization (DPO) effectively aligns the model with human judgments, boosting the model's preference selection capabilties significantly. Primary limitations include the specialization-vs-adaptability trade-off . Future work should focus on mitigating this trade-off, alongside exploring larger models and adaptive retrieval to advance personalized STEM tutoring.

\clearpage

% Entries for the entire Anthology, followed by custom entries
\newpage
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\onecolumn
\section{Appendix}


\subsection{MCQA - Full prompt}
\label{apen: prompt}
% later, in your appendix
\begin{tcolorbox}[
  breakable,
  sharp corners,
  boxrule=0.5pt,
  colback=white,
  colframe=black,
  listing only,
  listing options={
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakanywhere=true,
    prebreak=\mbox{\tiny\ensuremath\hookleftarrow},
    postbreak=\mbox{\tiny\ensuremath\rightarrow}
  },
  width=\linewidth,
  left=0pt,
  right=0pt,
  top=1mm,
  bottom=1mm,
]
You are a helpful assistant, that answer STEM questions. Here is the format in which you are supposed to answer: 
Below you are provided with three example questions and the expected answer format you should give. Just answer with A, B, C, or D.

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.

Performance enhancing synthetic steroids are based on the structure of the hormone:
A. testosterone.
B. cortisol.
C. progesterone.
D. aldosterone.
Answer:A

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.
Asp235Phe in a molecular report indicates that:
A. asparagine has been replaced by phenylalanine.
B. phenylalanine has been replaced by asparagine.
C. aspartic acid has been replaced by phenylalanine.
D. phenylalanine has been replaced by aspartic acid.
Answer:C

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.
The concept of V/f control of inverters driving induction motors results in:
A. constant torque operation  
B. speed reversal  
C. reduced magnetic loss  
D. harmonic elimination  
Answer:A

Answer the following question in the same way:
\end{tcolorbox}

\subsection{Comparison of final and intermediate MCQA models across QA benchmarks}

Complementary results of the MCQA model are visible in this section. Table \ref{tab: MCQA2} shows the results across the different experiments of MCQA model. Table \ref{tab:mcqa_results} shows specific baselines and tuned models across benchmarks. highlighting top performers. 

\label{apen: comp}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXX}
\toprule
\textbf{Task} & \textbf{\qwentulumcqamath} & \textbf{MCQA SFT} & \textbf{MCQA SFT + LoRA} \\
\midrule
hellaswag       & 0.4345 & 0.2510 & 0.2600  \\
mathQA            & 0.3041 & 0.2394 & 0.2866  \\
aqua      & 0.3192 & 0.2200 & 0.2300  \\
medmcqa        & 0.4210 & 0.1200 & 0.1300  \\
mmlu            & 0.4981 & 0.3000 & 0.2500  \\
sciq            & 0.8315 & 0.4300 & 0.4500  \\

\bottomrule
\end{tabularx}
\caption{Performance accuracy comparison of final and intermediate MCQA models across QA benchmarks}
\label{tab: MCQA2}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Model} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
\qwenb{} & 0.2430 & 0.2295 & 0.3223 & 0.2419 & 0.2311 & 0.2946 & 0.2504 & 0.1879 \\
\qwenbbase{} & 0.2452 & 0.2344 & 0.3213 & 0.2419 & 0.2402 & 0.3013 & 0.2507 & 0.1896 \\
\qwentulumcqamath{} & 0.7565 & 0.5016 & 0.4231 & \textbf{0.3142} & \textbf{0.8426} & 0.3415 & 0.4633 & 0.3238 \\
\qwentulumcqa{} & \textbf{0.7590} & \textbf{0.5038} & 0.4236 & 0.3049 & 0.8416 & 0.3460 & \textbf{0.4817} & 0.3138 \\
MCQA SFT & 0.3089 & 0.2565 & 0.3261 & 0.2394 & 0.4087 & 0.3147 & 0.2510 & 0.1896 \\
MCQA SFT + LoRA & 0.2895 & 0.2651 & 0.2539 & 0.2866 & 0.4087 & 0.2567 & 0.2600 & 0.2282 \\
\qwentulu{} & 0.7193 & 0.4882 & 0.3854 & 0.2821 & 0.8305 & 0.2344 & 0.3623 & \textbf{0.3607} \\
\bottomrule
\end{tabularx}
\caption{Performance comparison of non-RAG MCQA models.}
\label{tab:mcqa_results}
\end{table*}


\subsection{RAG Performance Analysis}
\label{appendix:rag_performance}

We conducted a series of experiments to evaluate the effectiveness of Retrieval-Augmented Generation. These tests assessed how different generator models, retrieval embeddings (a custom-trained model versus the base \texttt{\graniteembed}), and the number of retrieved documents ($k$) impact performance across our evaluation suite.

Our initial experiments, detailed in Table \ref{tab:rag_Qwen_Qwen3_0_6B_Base_results} and Table \ref{tab:rag_Qwen_Qwen3_0_6B_results}, highlight a critical prerequisite for successful RAG implementation. The \texttt{\qwenbbase} model, which is a raw pretrained model, lacks the ability to follow instructions or synthesize retrieved context, resulting in performance near random chance. Even the instruction-tuned \texttt{\qwenb} model fails to show any meaningful improvement, suggesting that a generic instruction-following capability is insufficient for this task.

The first model to demonstrate a significant positive interaction with RAG is the foundational \texttt{\qwentulu} model (Table \ref{tab:rag_vidyc_base_model_tulu_sft_results}). Having been trained on the high-quality \texttt{\tuluds} mixture, it is adept at leveraging retrieved context, showing a substantial performance lift over the baseline models. This indicates that our foundational instruction-following approach creates a model that is "RAG-ready" and benefits markedly from external knowledge.

Interestingly, for our most specialized models, \texttt{\qwentulumcqa} and \texttt{\qwentulumcqamath}, applying RAG often leads to a slight degradation in performance compared to their non-RAG counterparts (as seen in Table \ref{tab:mcqa_results}). We hypothesize that these models have become highly optimized for the MCQA format patterns. For them, the introduction of external, potentially noisy context from the retriever can act as a distraction rather than an aid, causing their performance to dip below its non-RAG peak. Despite this, these models are our best-performing models overall. Their absolute scores, even with the slight RAG-induced degradation, are significantly higher than the RAG-augmented \texttt{\qwentulu} model on most benchmarks, justifying their selection as our final models.

\paragraph{Custom Embedding Training}
To create the custom retrieval embeddings referenced in our experiments, we fine-tuned a model specifically on our domain data. We employed an unsupervised strategy using the \texttt{sentence-transformers} library, following a Transformer-based Denoising Auto-Encoder (TSDAE) approach. The training corpus was composed of text chunks from our EPFL and STEM PDF knowledge base. Starting with the \texttt{sentence-transformers/all-MiniLM-L6-v2} base model, we generated training examples by corrupting each chunk via random word deletion. The model was then trained for 5 epochs with a \texttt{MultipleNegativesRankingLoss} objective. This method teaches the model to map a corrupted text to the same embedding space as its original version, thereby learning robust, domain-specific representations. A 10\% validation split and an early stopping protocol were used to prevent overfitting.

%--- The new tables go here ---

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & \textbf{0.2458} & {0.2324} & \textbf{0.3242} & {0.2419} & {0.2331} & {0.2969} & {0.2504} & \textbf{0.1896} \\
Custom Granite (k=5) & 0.2432 & \textbf{0.2314} & 0.3235 & 0.2419 & {0.2331} & 0.2946 & {0.2504} & 0.1862 \\
Custom Granite (k=10) & 0.2438 & 0.2305 & 0.3227 & 0.2419 & 0.2321 & 0.2946 & \textbf{0.2504} & 0.1862 \\
IBM Granite (k=2) & 0.2430 & 0.2296 & 0.3218 & 0.2419 & 0.2311 & 0.2946 & {0.2504} & 0.1879 \\
IBM Granite (k=5) & 0.2432 & 0.2295 & 0.3223 & 0.2419 & 0.2311 & 0.2946 &{0.2504} & 0.1862 \\
IBM Granite (k=10) & 0.2432 & 0.2295 & 0.3225 & 0.2419 & 0.2311 & 0.2946 & {0.2504} & 0.1846 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwenbbase{} generator model.}
\label{tab:rag_Qwen_Qwen3_0_6B_Base_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
Custom Granite (k=5) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
Custom Granite (k=10) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=2) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=5) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=10) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwenb{} generator model.}
\label{tab:rag_Qwen_Qwen3_0_6B_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7435 & 0.4944 & 0.4186 & 0.3118 & 0.8345 & 0.3393 & 0.4333 & 0.3087 \\
Custom Granite (k=5) & 0.7455 & 0.4951 & 0.4121 & 0.3138 & 0.8385 & 0.3348 & 0.4200 & 0.3087 \\
Custom Granite (k=10) & 0.7444 & 0.4946 & \textbf{0.4210} & \textbf{0.3159} & 0.8375 & 0.3281 & 0.4191 & 0.2987 \\
IBM Granite (k=2) & \textbf{0.7483} & \textbf{0.4956} & 0.4196 & 0.3114 & \textbf{0.8416} & 0.3415 & \textbf{0.4563} & \textbf{0.3171} \\
IBM Granite (k=5) & 0.7472 & 0.4953 & 0.4143 & 0.3057 & \textbf{0.8416} & 0.3415 & 0.4328 & 0.3037 \\
IBM Granite (k=10) & 0.7438 & 0.4925 & 0.4162 & 0.3126 & 0.8365 & \textbf{0.3460} & 0.4334 & 0.3003 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulumcqamath{} generator model.}
\label{tab:rag_RikoteMaster_tulu_ft_arc_math_mcqa_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7489 & 0.4971 & \textbf{0.4198} & 0.2972 & 0.8355 & 0.3415 & 0.4485 & 0.3003 \\
Custom Granite (k=5) & 0.7506 & 0.4996 & 0.4181 & \textbf{0.3024} & 0.8385 & \textbf{0.3616} & 0.4429 & 0.3020 \\
Custom Granite (k=10) & 0.7497 & 0.4973 & 0.4176 & 0.2959 & 0.8375 & 0.3393 & 0.4383 & 0.2869 \\
IBM Granite (k=2) & \textbf{0.7523} & \textbf{0.5025} & 0.4153 & 0.2972 & \textbf{0.8396} & 0.3393 & \textbf{0.4726} & \textbf{0.3121} \\
IBM Granite (k=5) & 0.7486 & 0.4989 & 0.4172 & 0.2943 & 0.8365 & 0.3549 & 0.4538 & 0.2852 \\
IBM Granite (k=10) & 0.7483 & 0.4993 & 0.4186 & 0.2963 & 0.8365 & 0.3504 & 0.4504 & 0.2768 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulumcqa{} generator model.}
\label{tab:rag_RikoteMaster_tulu_ft_mcqa_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7125 & \textbf{0.4923} & 0.3715 & \textbf{0.3008} & 0.8325 & 0.2746 & 0.4116 & \textbf{0.3507} \\
Custom Granite (k=5) & 0.7148 & 0.4868 & 0.3756 & 0.2919 & \textbf{0.8345} & 0.2746 & 0.4139 & 0.3440 \\
Custom Granite (k=10) & \textbf{0.7227} & 0.4907 & \textbf{0.3760} & 0.2919 & 0.8254 & \textbf{0.2835} & \textbf{0.4165} & 0.3389 \\
IBM Granite (k=2) & 0.7083 & 0.4816 & 0.3569 & 0.2829 & 0.8325 & 0.2790 & 0.3917 & 0.3490 \\
IBM Granite (k=5) & 0.7182 & 0.4848 & 0.3732 & 0.2780 & 0.8295 & 0.2612 & 0.4000 & 0.3322 \\
IBM Granite (k=10) & 0.7159 & 0.4871 & 0.3732 & 0.2785 & 0.8274 & 0.2701 & 0.4039 & 0.3406 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulu{} generator model.}
\label{tab:rag_vidyc_base_model_tulu_sft_results}
\end{table*}

\clearpage
\subsection{In-depth Experimental details}
\label{apen: experimental details}
The different training parameters, base models and datasets used in each case are shown in this section.

\subsubsection{Foundational training Experimental Details}

We fine-tuned \texttt{Qwen/Qwen3-0.6B-Base} on the \texttt{\tuluds} for one epoch using per-device batch size 2, gradient accumulation 8, learning rate $2\times10^{-5}$, and max gradient norm 1.0. Training employed BF16 precision, gradient checkpointing, and a linear LR scheduler with warmup ratio 0.03. Logging every 20 steps, saving every 100 steps, reporting to WandB and with Seed 42.

\begin{lstlisting}[style=config, language=yaml, caption={SFT fine-tuning hyperparameters}]
seed: 42 
overwrite_output_dir: True

# Training settings
num_train_epochs: 1       
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 0.00002
max_grad_norm: 1.0

# Precision & stability
bf16: True
gradient_checkpointing: True
remove_unused_columns: False

# Logging & tracking
logging_steps: 20
save_steps: 100
disable_tqdm: False

# LR scheduler
lr_scheduler_type: "linear"
warmup_ratio: 0.03
\end{lstlisting}
\subsubsection{SFTs for MCQA Experimental Details}

We fine-tuned the model outputted from the foundational training stage using a two-phase curriculum learning approach, with hyperparameters detailed in Listing \ref{lst:sft_hyperparams}.
First, for general MCQA adaptation, the model was trained on a broad mixture of datasets (\texttt{\unimcqa}) with a learning rate of $6 \times 10^{-6}$.
Subsequently, to enhance mathematical and logical reasoning, the resulting model was further fine-tuned on the \texttt{\allenaiarc} dataset split using a more conservative learning rate of $2 \times 10^{-6}$. For both stages, we trained for one epoch using the AdamW optimizer, BF16 precision, gradient checkpointing, and a cosine learning rate scheduler with 50 warmup steps. The best model was selected based on evaluation loss.
\clearpage
\begin{lstlisting}[style=config, language=yaml, caption={SFT fine-tuning hyperparameters for the two-phase curriculum. The primary difference between stages is the learning rate.}, label={lst:sft_hyperparams}]
# General settings for both stages
seed: 42
num_train_epochs: 1
optim: "adamw_torch"
lr_scheduler_type: "cosine"
warmup_steps: 50
weight_decay: 0.01
max_grad_norm: 1.0

# Batching (effectively managed by smart batching)
per_device_train_batch_size: 1
gradient_accumulation_steps: 1

# Precision & Memory Optimization
bf16: True
gradient_checkpointing: True
remove_unused_columns: True

# Checkpoint Saving & Evaluation
save_strategy: "steps"
save_steps: 200
save_total_limit: 4
load_best_model_at_end: True
metric_for_best_model: "eval_loss"
greater_is_better: False
eval_strategy: "steps"
eval_steps: 200

# Logging
logging_steps: 50
report_to: "wandb"
disable_tqdm: False

# --- Stage-Specific Learning Rates ---
# Stage 1: General MCQA adaptation (\unimcqa)
learning_rate: 6.0e-6

# Stage 2: Math & logic reasoning (\allenaiarc)
# learning_rate: 2.0e-6
\end{lstlisting}
\subsubsection{Quantized Experimental Details}
We conducted model quantization experiments using the \texttt{llm-compressor} library to optimize the MCQA final model for efficient inference. The base model was loaded with automatic device mapping and \texttt{float16} precision to manage memory usage effectively. For calibration data, We applied a two-stage quantization recipe combining SmoothQuant (smoothing strength 0.8) for activation conditioning followed by GPTQ quantization targeting all Linear layers while excluding the language modeling head (lm\_head). Two quantization schemes were evaluated: W8A8 (8-bit weights and 8-bit activations) and W4A8 (4-bit weights and 8-bit activations), both implemented through the oneshot API. The quantization process utilized the full calibration dataset for compression statistics, and the resulting models were saved using the save\_compressed flag. Post-quantization validation was performed through sample text generation to verify model functionality.

\subsubsection{DPO Experimental Details}
We fine-tuned our DPO models using a single epoch training regime optimized for resource efficiency. Training was conducted with a batch size of 6 per device and gradient accumulation over 8 steps to simulate larger batch training. A linear learning rate scheduler was employed with an initial learning rate of 5e-5 and a warmup ratio of 0.1. Optimization was performed using the AdamW algorithm as implemented in PyTorch. Mixed precision training was enabled via \texttt{bf16}, and gradient checkpointing was used to reduce memory consumption. A regularization term with a strength of 0.01 was included via the \texttt{beta} parameter.

We performed training with the DPOTrainer class from Huggingface over the described DPO dataset in section \ref{sec:dpo_data}

% \clearpage
\begin{lstlisting}[style=config, language=yaml, caption={DPO fine-tuning hyperparameters}]
seed: 42

# Training settings
num_train_epochs: 1
per_device_train_batch_size: 6
per_device_eval_batch_size: 6
gradient_accumulation_steps: 8
dataset_num_proc: 6
learning_rate: 0.00005
optim: "adamw_torch"
beta: 0.01

# Precision & stability
bf16: True
gradient_checkpointing: True
remove_unused_columns: False

# Logging & tracking
logging_steps: 10
save_strategy: "epoch"
eval_strategy: "steps"
eval_steps: 100

# LR scheduler
lr_scheduler_type: linear
warmup_ratio: 0.1
\end{lstlisting}

\end{document}


ML PROJECT 02.2025 FOR EPFL COURSE CS-MACHINE-LEARNING
\documentclass[conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{booktabs} % For improved table formatting
\usepackage{caption}  % For customizing captions
\usepackage{anyfontsize}
\usepackage{amsmath, amssymb, siunitx}


\begin{document}
%\fontsize{9pt}{11.1pt}\selectfont
\title{The Impact of Whole-Slide Image Resolution on Foundation Model Performance in Computational Pathology}

\author{
  Mario Rico Ibáñez, Daniel López Gala, Carlos Hurtado Comín\\
  Supervised by: Sevda Öğüt, Cédric Vincent-Cuaz, Vaishnavi Subramanian\\
  \textit{LTS4, EPFL}
}

\maketitle

% FEEDBACK ON CODE
% + codes are well-documented
% - there are not enough comments in your codes 

% GENERAL FEEDBACK
% - no citations

\begin{abstract}
    Digital pathology leverages deep learning to analyze Whole-Slide Images (WSIs), but foundation models like UNI are typically trained at a fixed magnification. In this paper, we systematically evaluate UNI’s robustness across multiple magnifications using three breast histology datasets (BACH, BRACS, BreakHis). Our results show that while UNI generally maintains performance under resolution changes, magnification shifts influence feature quality and classification outcomes. Gated attention and non-linear heads improve consistency, guiding the development of more adaptable digital pathology pipelines.
\end{abstract}

\section{Introduction}

Machine learning is at the forefront of research across diverse domains, including biomedicine. Within this landscape, digital pathology—digitizing and analyzing tissue samples on glass slides using high-resolution Whole-Slide Images (WSIs)—has gained increasing prominence. These WSIs are generated by scanning traditional glass slides at high magnification, producing gigapixel-scale images suitable for computational analysis. Deep learning has revolutionized the workflow in digital pathology, enabling automated disease diagnosis, prognostic assessment, and therapy response prediction.

As deep learning has matured, \textit{foundation models} have emerged as large-scale models, often based on Vision Transformers (ViTs)~\cite{dosovitskiy2020image}, that are pre-trained on vast and diverse datasets and can then be adapted to a broad array of downstream tasks with minimal fine-tuning. These models have become increasingly popular due to their capability to learn general-purpose representations. For instance, the recently introduced UNI model~\cite{chen2024towards}, a foundation model for histopathological images, was trained exclusively on images scanned at a specific magnification (20x, corresponding to approximately 0.42 microns per pixel). Analyzing WSIs across magnifications is critical because diagnostic-relevant features appear at multiple scales, and without understanding how resolution shifts impact performance, models trained at a single magnification risk failing under real-world conditions where scanning parameters naturally vary.

While UNI’s authors report that the model is robust to magnification differences, a systematic study on this is lacking, raising questions about the stability of these representations at varying scales. The existing literature does not comprehensively address whether foundation models pre-trained at a single magnification level can generalize to lower or higher magnifications. Understanding this is critical, as magnification-dependent variability can influence patch-level embeddings and subsequent classification performance. In this work, we seek to fill this gap by analyzing the robustness of the UNI model across multiple magnifications and datasets. We evaluate how feature quality is affected by changes in WSI resolution through a series of experiments on three distinct breast histology datasets (BACH~\cite{aresta2019bach}, BRACS~\cite{brancati2022bracs}, and BreakHis~\cite{spanhol2015dataset}). We compare the performance of downstream tasks across magnification levels on an array of Multiple-Instance Learning (MIL)~\cite{dietterich1997solving} models. 
Our results identify when foundation models work well at different magnifications, helping researchers and clinicians develop and use these models more effectively in real-world practice.

In summary, we (i) present a systematic approach to evaluating resolution effects across datasets and magnification levels, and (ii) apply our approach to the UNI model. These findings pave the way for digital pathology solutions that maintain accuracy and reliability, regardless of magnification differences.

\section{Tissue sample Classification}

Our approach to evaluating resolution effects on UNI embeddings is based on an image classification pipeline. We assess UNI's robustness to differences in magnification by training a set of models over the same dataset processed at different magnifications and comparing performance on an image classification task. Classifying an image involves three steps: First, we split the images into equally sized patches; Second, we get the UNI model embeddings from those patches; Last, we classify the image from the image patch embeddings with Multiple-Instance Learning (MIL).

\subsection{Tiling and Preprocessing}

Tiling is critical in the preprocessing pipeline, uniformly dividing the regions of interest (RoIs) of WSIs across multiple datasets and magnification levels. This process ensures efficient patch extraction while maintaining consistency in resolution and magnification, which is necessary to have a consistent input for our models. This section describes the methodology used to generate and validate the tiles.

We generate tiles by dividing the RoI images into non-overlapping patches. The grid generation ensures no tiles overlap based on patch size, desired magnification level, and base magnification level specific to the dataset. An image's microns-per-pixel (MPP) defines the base magnification level. The desired magnification level is achieved by downsampling the image.
We calculate the patch pixel size by adjusting the sampling grid using the MPP values corresponding to the desired magnification so that they are proportional to 224x224 pixels. Each dataset has a predefined base MPP value(s). Tiles are scaled proportionally for uniformity across magnifications.
We save the tiling and relevant metadata including the coordinates, magnification level, and other attributes in a structured JSON for each processed image.

Based on MIL principles, we create non-overlapping tiles, even if this results in some tiles containing padding. MIL is a machine learning framework in which data is organized into bags containing multiple instances. Labels are assigned at the bag level, and the model must learn to infer the information at the instance level that contributes to the bag prediction. A key assumption in MIL is that instances are independently and identically distributed (i.i.d.). 
Ensuring that tiles do not overlap helps maintain this assumption, preventing information leakage between tiles and increasing the reliability of downstream analyses. This design choice is particularly relevant to our project, as it is consistent with the MIL foundations and ensures that the extracted tiles provide robust and unbiased input to train our models.

We developed a verification pipeline to ensure this process was correct and that the generated coordinates corresponded to tiles without overlapping.
\begin{itemize}
    \item \textbf{Visualization}: Tiles are drawn on the WSIs as rectangles of different colors. This way, we can verify proper coverage of the image and alignment with the computed grid.
    \item \textbf{Overlap Validation}: An overlap detection algorithm calculates the percentage of overlap between any two tiles, logging the results to confirm no overlap between tiles.
\end{itemize}

Figure \ref{fig:tiling_visualization} shows an example of this process, comparing a RoI from the BRACS dataset across different magnifications (5x, 10x, and 20x). Rectangles of different colors represent individual tiles.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{assets/n009_5x_tiles.png}
    \includegraphics[width=0.3\textwidth]{assets/n009_10x_tiles.png}
    \includegraphics[width=0.3\textwidth]{assets/n009_20x_tiles.png}
    \caption{Comparison of tiling applied to a Region of Interest (RoI) from the BRACS dataset at 5x (left), 10x (center), and 20x (right) magnifications. Each rectangle represents a tile, ensuring non-overlapping coverage.}
    \label{fig:tiling_visualization}
\end{figure*}


\subsection{UNI Embedding and Modeling}

Once we have the image patches, we get the UNI embedding for each patch. For each image, we aggregate the patches to obtain a single embedding. In Section ~\ref{sec:experiments} we provide more details on the approaches and datasets used for classification.

% describe models tested and why: knn, deep learning and their setups

\section{Experimental setup}
\label{sec:experiments}

\subsection{Datasets}

\textbf{BACH}. A dataset of $400$ microscopy images, i.e., regions of interest of the breast-tissue whole-slide images. The $400$ samples are uniformly distributed as: \textit{Normal} ($100$), \textit{Benign} ($100$), \textit{in situ carcinoma} ($100$), and \textit{invasive carcinoma} ($100$). All the images are the same size and at a pixel scale of $0.42$ MPP, a 20x magnification. We classify the images between the 4 classes and compare performance over the magnification levels: 20x, 10x, and 5x.


\textbf{BRACS}. A dataset of $4539$ labeled regions of interest of breast-tissue whole-slide images of $189$ patients. The tumor samples are distributed as \textit{Benign}: \textit{Normal} ($484$), \textit{Pathological Benign} ($836$), or \textit{Usual Ductal Hyperplasia} ($517$); \textit{Atypical}: \textit{Flat Epithelial Atypia} ($756$), and \textit{Atypical Ductal Hyperplasia} ($507$); and \textit{Malignant}: \textit{Ductal Carcinoma in situ} ($790$), and \textit{Invasive Carcinoma} ($649$). The ROI images are of varying sizes and $0.25$ MPP, i.e., 40x. We classify the images between the 7 classes and compare performance over the magnification levels: 40x, 20x, 10x, and 5x.

\textbf{BreakHis}. A dataset of $1995$ labeled microscopic images of breast tumor tissue collected from $82$ patients. The samples are distributed as \textit{Benign}: \textit{Adenosis} ($114$), \textit{Fibroadenoma} ($253$), \textit{Phyllodes Tumor} ($109$), or \textit{Tubular Adenoma} ($149$); and \textit{Malignant}: \textit{Ductal Carcinoma} ($864$), \textit{Lobular Carcinoma} ($156$), \textit{Mucinous Carcinoma} ($205$), or \textit{Papillary Carcinoma} ($145$). All the images are the same size and at a magnification level of 40x. We classify the images between the 8 classes and compare performance over the magnification levels: 40x, 20x, 10x, and 5x.

\subsection{Models}
\label{sec:experimental_setup}

To evaluate the performance of embeddings generated by the UNI model at various magnification levels, we implemented a structured training, validation, and testing protocol. Our experiments consider two primary aggregation strategies—\textbf{mean-pooling} and \textbf{Gated Attention}—combined with two different classification heads: a linear classifier and an MLP with a 128-dimensional hidden layer. We also probe UNI embeddings with k-NN to define a non-parametric baseline and explore class separability without transforming the embeddings.

Regarding aggregation, recall that each image $I$ is subdivided into $T$ non-overlapping tiles. The UNI model is applied to each tile, producing a set of embeddings $\{h_k\}_{k=1}^T$. Under a MIL framework, these tile-level embeddings must be integrated into a single image-level representation before classification. We therefore examine the following aggregation methods:

\begin{itemize}
    \item \textbf{Mean-Pooling}
    \[
    h_I = \frac{1}{T} \sum_{k=1}^{T} h_k.
    \]

    \item \textbf{Gated Attention}
    \[
    h_I = \sum_{k=1}^{T} \alpha_k h_k,
    \]
    where the attention weights $\alpha_k$ are defined as:
    \[
    \alpha_k = \frac{\exp \left( \mathbf{w}^\top \left( \tanh(\mathbf{V} h_k) \odot \mathrm{sigm}(\mathbf{U} h_k) \right) \right)}{\sum_{j=1}^{T} \exp \left( \mathbf{w}^\top \left( \tanh(\mathbf{V} h_j) \odot \mathrm{sigm}(\mathbf{U} h_j) \right) \right)},
    \]
    where $\mathbf{U} \in \mathbb{R}^{d \times D}$, $\mathbf{V} \in \mathbb{R}^{d \times D}$ are trainable weight matrices, and $\mathbf{w} \in \mathbb{R}^d$ is a learnable weight vector. The symbol $\odot$ denotes element-wise multiplication, while $\tanh$ and $\mathrm{sigm}$ represent the hyperbolic tangent and sigmoid functions, respectively~\cite{ilse2018attention}.
\end{itemize}

By integrating tile-level information through these aggregation methods, we obtain a comprehensive feature representation $h_I$ for the entire image. This feature vector is then passed to either a linear classifier or an MLP classifier. The MLP consists of a single hidden layer of size 128, followed by a non-linear activation and a final linear layer mapping to the output classes.

To facilitate a fair comparison of models and magnification levels, we employ the weighted F1 score, a metric that mitigates the impact of class imbalance on the evaluation. To enhance reproducibility and reliability, we use a 5-fold cross-validation scheme. Models are trained until convergence with early stopping based on validation set performance, employing a patience of 20 epochs to prevent overfitting. After selecting the best model configuration (aggregation strategy and classifier type) for each magnification level and dataset, we use the chosen model to generate predictions on the held-out test set.

%---------------------------------------------------------
% Draft for the Results Section
%---------------------------------------------------------
\section{Results}
\label{sec:results}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/validation_metrics.png}
    \caption{Validation weighted F1 scores comparing Mean vs. Gated Attention aggregation and Linear vs. MLP classification heads across different magnification levels. Shaded areas represent standard deviations over 5-fold cross-validation folds. The figure shows that GA and MLP configurations generally yield stronger results.}
    \label{fig:validation_metrics_aggregation}
\end{figure}

Figure~\ref{fig:validation_metrics_aggregation} presents the validation weighted F1 scores under different combinations of aggregation (Mean vs. GA) and classifier heads (Linear vs. MLP) across various magnification levels. Each subplot (Mean Linear, Mean MLP, GA Linear, GA MLP) shows trends for the three datasets, enabling a direct comparison.
We observe that aggregation through gated attention performs marginally better than mean pooling, indicating that learning instance-level importance better captures discriminative patterns in histopathological images. Moreover, the MLP head generally provides a performance boost over the linear head. The additional nonlinearity helps model complex tissue patterns that simple linear boundaries cannot.

Our final evaluation focuses on the test sets of the considered datasets. After using the validation phase to select the best combination of aggregation strategy, classifier head, and magnification level, we assessed the chosen models on the held-out test data.
Figure~\ref{fig:test_aug_performance} illustrates the weighted F1 scores on the test sets across various magnification factors for BACH, BRACS, and BreakHis. Classification performance on different magnification levels is generally consistent across datasets. Notably, \textit{BreakHis} consistently benefits from higher magnification levels. In contrast, \textit{BACH} and \textit{BRACS} demonstrate relatively stable performance across magnification factors. Both show slightly lower performance on higher resolutions, which may imply that relevant features for classification may not be captured with finer-grained tiling.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/test_metrics.jpeg}
    \caption{Best test weighted-F1 scores across varying magnification factors for BACH, BRACS, and BreakHis. We observe comparable performance across magnification levels for all datasets, with BreakHis benefiting from higher-resolution tiling.}
    \label{fig:test_aug_performance}
\end{figure}

Finally, tables~\ref{tab:knnvalidation_metrics_bach}, \ref{tab:knnvalidation_metrics_bracs}, and \ref{tab:knnvalidation_metrics_breakhis} show a strong performance of our k-NN probing relative to the previously discussed parametric models. This indicates that the UNI embeddings of images in the same class are clustered. Moreover, this explains why the marginal gain of the MLP head is not that significant, as the data are already almost linearly separable.



\section{Ablation Studies}
\label{sec:ablation}

We conducted ablation studies to understand the factors influencing performance. We isolate the impact of magnification levels, aggregation strategies, and classification models. Our results for this section are in the Appendix, in tables~\ref{tab:validation_metrics_bach}-\ref{tab:knnvalidation_metrics_breakhis}
The ablation studies highlight several findings that are consistent with the results discussed in~\ref{sec:results}, mainly that aggregation method matters: GA outperforms simple mean-pooling, indicating the importance of learning instance-level weights; and model complexity slightly pays off: The MLP classifier generally surpasses the linear alternative, by learning non-linear relationships in the data.

We also observe that introducing dropout regularization does not significantly impact the models' classification performance. Moreover, some models perform better without regularization. Another interesting finding is that k-NN performance is best on cosine similarity and with small values of k. This is consistent with the literature on the curse of dimensionality, where most of the data lies at the class boundaries, and cosine similarity works better than Euclidean distance on high dimensional data~\cite{lee-1999-measures}.


\section{Conclusion}

In this paper, we investigated how the UNI foundation model for histopathology—trained exclusively at a fixed magnification—performs when applied to WSIs at multiple resolutions. Our experiments across BACH, BRACS, and BreakHis datasets show that UNI can maintain competitive performance despite changes in magnification levels. Future work should explore domain adaptation strategies, incorporate additional tissue types, and assess model resilience against more pronounced variations in imaging parameters, ultimately aiming to create universally applicable foundation models in computational pathology.

\section{Ethical risks}

% see https://github.com/epfml/ML_course/blob/main/projects/project2/project2_description.pdf

The primary ethical risk identified in this project is related to the datasets used, which include WSIs containing sensitive patient information, particularly concerning cancer diagnosis. These datasets, i.e., BACH, BRACS, and BreakHis, are critical to advancing computational pathology but must be handled with the highest care to protect patient privacy.

\textbf{Risk description:} The WSIs used in this study are derived from \textbf{patient pathology slides}. Although the images are anonymized, there is a residual risk of re-identification, particularly when combining metadata such as patient IDs, tumor characteristics, or diagnostic results. This risk affects patients, the primary stakeholders, and healthcare providers, responsible for maintaining data confidentiality. Negative impacts include patient privacy breaches and potential misuse of sensitive health information.

\textbf{Severity and likelihood:} The severity of this risk is high due to the sensitive nature of cancer diagnostic data which, if compromised, could have significant personal, social, or professional consequences. However, the likelihood of occurrence is low due to the anonymization practices applied to the datasets and the established protocols for secure data sharing.

\textbf{Risk Mitigation:} We have taken the following measures to address this risk:

\begin{itemize}
    \item Ensure that the datasets used in this project are fully anonymized before use. This includes removing all identifiable patient information and restricting access to authorized collaborators only.
    \item Follow the ethical guidelines for medical datasets, including compliance and data licensing agreements.
    \item Use EPFL resources such as Izar and the Research Computing Platform (RCP), which prevent unauthorized use and encrypt all data storage.
\end{itemize}

\textbf{Evaluation process:} This risk was evaluated by a close review of the documentation of the dataset, ensuring that all images are anonymized and free of identifiable metadata.

By implementing these safeguards, we minimize the ethical risks associated with patient re-identification while contributing to advancing computational pathology research.

\bibliographystyle{unsrt}
\bibliography{literature}

\onecolumn
\appendix

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.933 $\pm$ 0.058 & 0.940 $\pm$ 0.036 & 0.944 $\pm$ 0.028 & 0.939 $\pm$ 0.043 & 0.928 $\pm$ 0.049 & 0.946 $\pm$ 0.033 & 0.952 $\pm$ 0.029 & \textbf{0.959 $\pm$ 0.024} & 0.955 $\pm$ 0.033 & 0.944 $\pm$ 0.014 & 0.947 $\pm$ 0.025 & 0.943 $\pm$ 0.028 \\
10x & 0.953 $\pm$ 0.011 & 0.953 $\pm$ 0.011 & 0.947 $\pm$ 0.018 & 0.959 $\pm$ 0.018 & \textbf{0.966 $\pm$ 0.020} & 0.959 $\pm$ 0.021 & 0.941 $\pm$ 0.018 & 0.944 $\pm$ 0.008 & 0.944 $\pm$ 0.018 & 0.953 $\pm$ 0.019 & 0.956 $\pm$ 0.013 & 0.956 $\pm$ 0.007 \\
20x & 0.934 $\pm$ 0.020 & 0.938 $\pm$ 0.024 & 0.947 $\pm$ 0.024 & 0.953 $\pm$ 0.022 & 0.944 $\pm$ 0.018 & 0.956 $\pm$ 0.020 & 0.922 $\pm$ 0.028 & 0.928 $\pm$ 0.044 & 0.934 $\pm$ 0.028 & 0.950 $\pm$ 0.030 & 0.960 $\pm$ 0.026 & \textbf{0.969 $\pm$ 0.022} \\

\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, BACH.}
\label{tab:validation_metrics_bach}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.711 $\pm$ 0.037 & 0.757 $\pm$ 0.013 & 0.755 $\pm$ 0.010 & 0.769 $\pm$ 0.006 & 0.767 $\pm$ 0.008 & 0.769 $\pm$ 0.006 & 0.733 $\pm$ 0.039 & 0.768 $\pm$ 0.010 & 0.769 $\pm$ 0.010 & \textbf{0.783 $\pm$ 0.009} & 0.776 $\pm$ 0.011 & 0.776 $\pm$ 0.010 \\
10x & 0.765 $\pm$ 0.018 & 0.764 $\pm$ 0.013 & 0.764 $\pm$ 0.012 & 0.783 $\pm$ 0.014 & 0.787 $\pm$ 0.012 & 0.785 $\pm$ 0.015 & 0.768 $\pm$ 0.005 & 0.770 $\pm$ 0.012 & 0.769 $\pm$ 0.008 & \textbf{0.798 $\pm$ 0.015} & 0.792 $\pm$ 0.011 & 0.794 $\pm$ 0.014 \\
20x & 0.777 $\pm$ 0.006 & 0.773 $\pm$ 0.007 & 0.774 $\pm$ 0.010 & 0.797 $\pm$ 0.009 & 0.800 $\pm$ 0.007 & 0.795 $\pm$ 0.010 & 0.769 $\pm$ 0.007 & 0.764 $\pm$ 0.002 & 0.769 $\pm$ 0.008 & 0.801 $\pm$ 0.009 & 0.796 $\pm$ 0.007 & \textbf{0.802 $\pm$ 0.005} \\
40x & 0.760 $\pm$ 0.007 & 0.756 $\pm$ 0.008 & 0.748 $\pm$ 0.036 & 0.787 $\pm$ 0.008 & \textbf{0.787 $\pm$ 0.006} & 0.782 $\pm$ 0.006 & 0.742 $\pm$ 0.008 & 0.732 $\pm$ 0.011 & 0.743 $\pm$ 0.011 & 0.781 $\pm$ 0.004 & 0.782 $\pm$ 0.010 & 0.786 $\pm$ 0.008 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, for BRACS.}
\label{tab:validation_metrics_bracs}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.811 $\pm$ 0.036 & 0.833 $\pm$ 0.022 & 0.848 $\pm$ 0.022 & 0.884 $\pm$ 0.018 & 0.881 $\pm$ 0.018 & 0.878 $\pm$ 0.020 & 0.803 $\pm$ 0.042 & 0.845 $\pm$ 0.027 & 0.827 $\pm$ 0.027 & \textbf{0.886 $\pm$ 0.021} & 0.879 $\pm$ 0.020 & 0.881 $\pm$ 0.024 \\
10x & 0.874 $\pm$ 0.058 & 0.910 $\pm$ 0.026 & 0.897 $\pm$ 0.049 & \textbf{0.938 $\pm$ 0.025} & 0.938 $\pm$ 0.018 & 0.934 $\pm$ 0.018 & 0.906 $\pm$ 0.032 & 0.906 $\pm$ 0.028 & 0.911 $\pm$ 0.024 & 0.936 $\pm$ 0.025 & 0.933 $\pm$ 0.022 & 0.933 $\pm$ 0.022 \\
20x & 0.888 $\pm$ 0.036 & 0.906 $\pm$ 0.062 & 0.944 $\pm$ 0.010 & 0.966 $\pm$ 0.012 & 0.963 $\pm$ 0.008 & 0.967 $\pm$ 0.008 & 0.930 $\pm$ 0.012 & 0.934 $\pm$ 0.010 & 0.939 $\pm$ 0.013 & 0.965 $\pm$ 0.011 & 0.965 $\pm$ 0.007 & \textbf{0.969 $\pm$ 0.010} \\
40x & 0.840 $\pm$ 0.047 & 0.875 $\pm$ 0.060 & 0.929 $\pm$ 0.018 & 0.964 $\pm$ 0.017 & 0.965 $\pm$ 0.011 & \textbf{0.965 $\pm$ 0.015} & 0.912 $\pm$ 0.036 & 0.908 $\pm$ 0.013 & 0.907 $\pm$ 0.027 & 0.955 $\pm$ 0.011 & 0.962 $\pm$ 0.009 & 0.959 $\pm$ 0.020 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, for BreakHis.}
\label{tab:validation_metrics_breakhis}
\end{table*}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x & \textbf{0.928 $\pm$ 0.033} & 0.925 $\pm$ 0.046 & 0.899 $\pm$ 0.051 & 0.879 $\pm$ 0.063 & 0.874 $\pm$ 0.077 & 0.896 $\pm$ 0.068 & 0.915 $\pm$ 0.023 & 0.899 $\pm$ 0.046 & 0.864 $\pm$ 0.073  & 0.854 $\pm$ 0.061 & 0.823 $\pm$ 0.093 & 0.832 $\pm$ 0.078 \\
10x & 0.922 $\pm$ 0.037 & \textbf{0.928 $\pm$ 0.038} & 0.903 $\pm$ 0.031 & 0.897 $\pm$ 0.026 & 0.896 $\pm$ 0.017 & 0.887 $\pm$ 0.028 & 0.884 $\pm$ 0.031 & 0.865 $\pm$ 0.050 & 0.851 $\pm$ 0.055 & 0.818 $\pm$ 0.056 & 0.813 $\pm$ 0.058 & 0.783 $\pm$ 0.083 \\
20x & \textbf{0.906 $\pm$ 0.037} & 0.887 $\pm$ 0.013 & 0.856 $\pm$ 0.038 & 0.844 $\pm$ 0.040 & 0.842 $\pm$ 0.037 & 0.817 $\pm$ 0.047 & 0.893 $\pm$ 0.023 & 0.827 $\pm$ 0.023 & 0.798 $\pm$ 0.022 & 0.746 $\pm$ 0.027 & 0.732 $\pm$ 0.050 & 0.739 $\pm$ 0.041 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BACH.}
\label{tab:knnvalidation_metrics_bach}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x  & 0.729 $\pm$ 0.013 & 0.735 $\pm$ 0.013 & 0.741 $\pm$ 0.006 & \textbf{0.746 $\pm$ 0.011} & 0.741 $\pm$ 0.010 & 0.739 $\pm$ 0.005 & 0.718 $\pm$ 0.011 & 0.700 $\pm$ 0.016 & 0.708 $\pm$ 0.010 & 0.706 $\pm$ 0.008 & 0.707 $\pm$ 0.005 & 0.706 $\pm$ 0.011 \\
10x & 0.741 $\pm$ 0.007 & 0.759 $\pm$ 0.014 & \textbf{0.760 $\pm$ 0.011} & 0.750 $\pm$ 0.015 & 0.749 $\pm$ 0.013 & 0.742 $\pm$ 0.005 & 0.731 $\pm$ 0.016 & 0.724 $\pm$ 0.021 & 0.724 $\pm$ 0.015 & 0.719 $\pm$ 0.020 & 0.716 $\pm$ 0.016 & 0.708 $\pm$ 0.019 \\

20x & 0.766 $\pm$ 0.015 & \textbf{0.770 $\pm$ 0.022} & 0.769 $\pm$ 0.016 & 0.765 $\pm$ 0.021 & 0.762 $\pm$ 0.026 & 0.756 $\pm$ 0.025 & 0.758 $\pm$ 0.022 & 0.743 $\pm$ 0.020 & 0.742 $\pm$ 0.015 & 0.729 $\pm$ 0.018 & 0.727 $\pm$ 0.020 & 0.715 $\pm$ 0.021 \\

40x & 0.756 $\pm$ 0.017 & \textbf{0.762 $\pm$ 0.016} & 0.757 $\pm$ 0.012 & 0.754 $\pm$ 0.008 & 0.748 $\pm$ 0.013 & 0.741 $\pm$ 0.016 & 0.751 $\pm$ 0.025 & 0.738 $\pm$ 0.023 & 0.725 $\pm$ 0.016 & 0.726 $\pm$ 0.010 & 0.708 $\pm$ 0.019 & 0.703 $\pm$ 0.023 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BRACS.}
\label{tab:knnvalidation_metrics_bracs}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x  & 0.862 $\pm$ 0.013 & \textbf{0.866 $\pm$ 0.016} & 0.849 $\pm$ 0.020 & 0.842 $\pm$ 0.019 & 0.826 $\pm$ 0.005 & 0.814 $\pm$ 0.009 & 0.861 $\pm$ 0.015 & 0.845 $\pm$ 0.015 & 0.825 $\pm$ 0.022 & 0.824 $\pm$ 0.012 & 0.806 $\pm$ 0.012 & 0.783 $\pm$ 0.009 \\
10x & \textbf{0.927 $\pm$ 0.016} & 0.925 $\pm$ 0.025 & 0.915 $\pm$ 0.026 & 0.906 $\pm$ 0.026 & 0.900 $\pm$ 0.025 & 0.891 $\pm$ 0.028 & 0.925 $\pm$ 0.017 & 0.916 $\pm$ 0.018 & 0.902 $\pm$ 0.025 & 0.883 $\pm$ 0.020 & 0.878 $\pm$ 0.024 & 0.869 $\pm$ 0.035 \\
20x & 0.953 $\pm$ 0.014 & 0.947 $\pm$ 0.014 & 0.942 $\pm$ 0.015 & 0.939 $\pm$ 0.012 & 0.931 $\pm$ 0.008 & 0.916 $\pm$ 0.008 & 0.950 $\pm$ 0.014 & \textbf{0.956 $\pm$ 0.010} & 0.944 $\pm$ 0.011 & 0.938 $\pm$ 0.004 & 0.924 $\pm$ 0.008 & 0.906 $\pm$ 0.013 \\
40x & 0.948 $\pm$ 0.009 & 0.947 $\pm$ 0.014 & 0.945 $\pm$ 0.016 & 0.937 $\pm$ 0.018 & 0.931 $\pm$ 0.022 & 0.925 $\pm$ 0.021 & 0.948 $\pm$ 0.013 & \textbf{0.952 $\pm$ 0.011} & 0.947 $\pm$ 0.015 & 0.936 $\pm$ 0.013 & 0.929 $\pm$ 0.014 & 0.906 $\pm$ 0.024 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BreakHis.}
\label{tab:knnvalidation_metrics_breakhis}
\end{table*}


\end{document}

BACHELOR THESIS PAPER FOR IEE ICC WORKSHOP 2025



\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{comment}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{bm}
\usepackage[left=1.62cm,right=0.64in,top=0.75in]{geometry}

%\usepackage{geometry}
%\setlength{\textheight}{23cm}  % Adjust text height for consistent bottom margins

%\usepackage{geometry}
\usepackage{array}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{balance}

%\geometry{a4paper, margin=1in}

%\geometry{a4paper, margin=1in}
\bibliographystyle{IEEEtran}


%\usepackage[skip=2pt]{caption} % Ajusta el espaciado de los subtítulos
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization }


\author{
\IEEEauthorblockN{Mario Rico Ibáñez$^1$; Azim Akhtarshenas$^1$; David L\'opez-P\'erez$^1$; Giovanni Geraci$^2$}
\IEEEauthorblockA{\textit{$^1$Universitat Politècnica de València, Valencia, Spain}} 
\IEEEauthorblockA{\textit{$^2$Telefónica Research and Universitat Pompeu Fabra, Barcelona, Spain}} 
%\IEEEauthorblockA{\textit{$3$}} 
\textit{mriciba@upv.edu.es}}

\maketitle


\begin{abstract}

\footnotetext{This research is supported by the Generalitat Valenciana
through the CIDEGENT PlaGenT, Grant CIDEXG/2022/17, Project iTENTE, and the action CNS2023-144333, financed by
MCIN/AEI/10.13039/501100011033 and the European Union
“NextGenerationEU”/PRTR.}
Unmanned aerial vehicle (UAV)-based base stations offer a promising solution in emergencies where the rapid deployment of cutting-edge networks is crucial for maximizing life-saving potential. 
Optimizing the strategic positioning of these UAVs is essential for enhancing communication efficiency. 
This paper introduces an automated reinforcement learning approach that enables UAVs to dynamically interact with their environment and determine optimal configurations. 
By leveraging the radio signal sensing capabilities of communication networks, 
our method provides a more realistic perspective, 
utilizing state-of-the-art algorithm ---proximal policy optimization--- to learn and generalize positioning strategies across diverse user equipment (UE) movement patterns. 
We evaluate our approach across various UE mobility scenarios, including static, random, linear, circular, and mixed hotspot movements.
The numerical results demonstrate the algorithm's adaptability and effectiveness in maintaining comprehensive coverage across all movement patterns.
 
\textbf{UAV, Aerial Base Station, DRL, PPO, Sensing}
\end{abstract}

%\vspace{-9pt}

\section{Introduction} 

Effective communication during disasters is crucial for successful search and rescue operations. 
However, this can be particularly challenging when existing networks are compromised, 
leaving user equipment (UE) in urgent need of assistance. 
One innovative approach to address this issue is the use of unmanned aerial vehicles (UAVs),
which offers a practical solution for quickly establishing emergency communication networks~\cite{uav_emergency}. 
UAVs, acting as flying base stations (BSs), can play a critical role in scenarios where most ground BSs are overloaded, collapsed, or entirely absent. 
Such situations were evident during events like 9/11~\cite{11s_comm}, Typhoon Haiyan~\cite{aquino2013typhoon}, and in many urban, suburban, and rural search and rescue operations. 
In these critical circumstances, search and 
the rapid deployment of broadband and/or ultra-reliable low-latency communication (URLLC) networks is essential to support first responders as quickly as possible, 
maximizing the chances of saving lives.

\subsection{Related Works}

Extensive recent work on UAV-based emergency communication networks has primarily focused on UAV positioning optimization, 
which we discuss along with their drawbacks below.

%Traditional methods
The author in~\cite{cicek2019uav} surveyed UAV-BS location optimization,
introduced a generic mixed-integer non-linear programming (MINLP) framework,
proposed a taxonomy of solutions,
and outlined future research directions for 5G and beyond.
In \cite{8038869}, 
the authors developed a framework to optimize 3D UAV placement, mobility, device-to-UAV association, and uplink power control for IoT devices, 
aiming to minimize energy consumption while ensuring network connectivity.
%
Unfortunately, all these works lack real-time decision-making capabilities due to their static nature and their inability to model the complexity of real-world environments,
which are inherently non-convex, non-linear, and stochastic.

%AI and ML
As a promising alternative, 
the integration of artificial intelligence (AI)---particularly reinforcement learning (RL)---into UAV networks enables autonomous, real-time optimization of 3D positioning, flight paths, and resource management.
For instance, the authors in \cite{8877247} addressed emergency scenarios by proposing a Q-learning-based solution to optimize the 3D positioning and transmit power of a network with multiple UAV-based small cells,
with the objective of maximizing network coverage while accounting for UE mobility.
Similarly, the study in \cite{ghanavi2018efficient} improved terrestrial networks using an aerial BS,
aiming to enhance quality of service (QoS) by mitigating performance degradation due to UE mobility.
This work employed Q-learning to determine the optimal aerial BS placement based on past experiences.
%
However, Q-learning suffers from the curse of dimensionality, 
making it impractical for large-scale problems wherein the state-action space is vast—as is the case in our scenario, 
making deep RL (DRL) a more suitable alternative where the Q-table is approximated by a deep neural network (DNN).

%DRL papers
Assuming simple UE mobility scenarios, 
the authors in \cite{8432464} developed a DRL-based approach for UAV flight control,
aiming to maximize energy efficiency while balancing network coverage and UE data rates.
%
%In \cite{liu2020machine}, 
%the authors integrated reconfigurable intelligent surfaces (RIS) with UAV networks to further reduce energy consumption and improve spectrum efficiency using non-orthogonal multiple access (NOMA). 
%They optimize UAV movement, RIS phase shifts, power allocation, and NOMA decoding order with a decaying deep Q-network (D-DQN) algorithm.
%However, while these methods offer promising solutions, they do not fully address the unique challenges posed by emergency scenarios, such as the need for real-time adaptation to rapidly changing environments, the inability to rely on precise UE location data, and the requirement to operate effectively across diverse and unpredictable mobility patterns of first responders.
%
Ding~\textit{et al.} in \cite{ding20203d} modeled UAV energy consumption as a function of 3D movement,
and designed a DRL algorithm using deep deterministic policy gradient (DDPG) 
to optimize flight direction and speed for throughput maximization while satisfying energy constraints.
%Further exploring UAV positioning, the authors in~\cite{gopi2021reinforcement} aim to optimize the positions of UAV-macro BSs using reinforcement learning. Their approach prevents collisions between multiple exploratory UAVs and restricts their movements to a predefined area. By applying Q-learning, UAV BS positions are optimized based on UE data rates, with the framework transitioning from exploratory to exploitative movements to maximize overall data rates.
%In \cite{gopi2021reinforcement}, the authors used RL to optimize UAV-macro BS positions, preventing collisions and confining movements to a specific area. They applied Q-learning to enhance UAV BS positions based on UE data rates, transitioning from exploration to exploitation to maximize overall data rates.
%
%remove
%{\color{red}In \cite{liu2019reinforcement},a novel UAV optimization method based on quality of experience (QoE) was proposed, involving three steps: UE cell partitioning with a genetic algorithm-based K-means (GAK-means), UAV 3D positioning optimization with a Q-learning algorithm, and tracking roaming UEs with a Q-learning-based movement strategy.}
%
Focusing on convergence efficiency, 
the authors in \cite{parvaresh2023continuous} proposed a continuous actor-critic DRL (ACDRL) approach,
which reduced UAV-BS placement convergence time by 85\,\% compared to traditional DRL-based methods.
%
These studies, nonetheless, rely on discrete state-action spaces, often leading to suboptimal performance.
More critically, the above-mentioned DRL methods suffer from training instability due to high policy update variance and hyperparameter sensitivity,
limiting their effectiveness in dynamic environments.

%TRPO
To address this issue, 
trust region policy optimization (TRPO) was introduced,
enforcing a strict KL-divergence constraint to ensure stable policy updates~\cite{schulman2015trust}.
For instance, the authors in \cite{ho2021uav} applied TRPO to address instabilities in DDPG,
improving both training stability and learning efficiency.
Their approach optimized UAV control for energy efficiency and data rate maximization in dynamic wireless environments,
enhancing UAV-based wireless service provisioning.
%
However, TRPO’s KL-divergence hard constraint makes it computationally expensive.

Proximal policy optimization (PPO)  overcomes TRPO’s limitations by replacing its strict KL-divergence constraint with a clipped objective function, 
improving both efficiency and computational simplicity~\cite{schulman2017proximal}.
Saxena~\textit{et al.} in \cite{saxena2019optimal} introduced flow-level models (FLM) to evaluate UAV-BS network performance across multiple metrics,
and proposed a PPO-based approach to optimize traffic-aware UAV trajectories.
Their offline training aimed to maximize cumulative performance metrics,
while online simulations demonstrated a three-fold increase in average UE throughput and more balanced BS traffic loads compared to initial UAV placements.
Unfortunately, their method assumes perfect knowledge of UE positions,
which limits its applicability in real-world environments where UE mobility and uncertainty must be considered.
\cite{li2020trajectory} was among the first works to propose a PPO-based approach for UAV trajectory design, 
optimizing the sum rate for all UEs while considering their movement.
Their PPO-based solution outperformed Q-learning when UAVs followed specific paths
where UE velocity and distribution remained relatively stable.
To further adapt to dynamic UE distributions, 
they introduced a random-training algorithm (RT-PPO).
However, their approach restricts UAV locations to a set of predefined points,
which simplifies the problem but limits flexibility.

%Extensive work has been done in recent years on aspects related to ---but not always entirely aligned with--- UAV-based emergency communication networks. While many studies focus on optimizing UAV placement and movement, they often do so in contexts that are not specific to emergency scenarios involving first responders.
%For instance, in~\cite{8038869}, the authors proposed a framework to jointly optimize 3D UAV placement, mobility, device-to-UAV association, and uplink power control for serving static Internet of Things (IoT) devices, aiming to minimize total transmit power. The framework determines optimal UAV locations and device-to-UAV associations based on IoT device activities, and then derives optimal UAV trajectories to minimize energy consumption while maintaining connectivity. 
%Further expanding on UAV optimization, 
%the work in~\cite{wang2024joint} introduced the height of the UAV as an optimization variable, 
%which had been widely overlooked in the literature. 
%They formulated their optimization problem as a constrained markov decision process (CMDP) and used a Lagrangian-based PPO-CMDP algorithm to derive the optimal UAV service height among other key variables. 
%
\subsection{Motivation and Contributions}

Although previous studies on UAV positioning have provided valuable insights,
many rely on unrealistic assumptions, including static UEs, known UE positions, predefined UAV paths, and discrete UAV movements.
These assumptions fail to capture the stochastic nature of UE mobility, the uncertainty of UE locations, and the flexibility of UAVs to navigate—especially in emergency scenarios.
As a result, existing approaches lack adaptability to dynamic environments, limiting their real-world applicability.

To address these challenges, we leverage PPO~\cite{schulman2017proximal}
to develop a more realistic and adaptive framework for UAV trajectory planning.
Specifically, we eliminate state-action space discretization and replace GPS-based positioning
with measurements over practical reference signals, 
allowing UAVs to position themselves based on real-time UE signal characteristics.

Our key contributions are:
\begin{itemize}
\item Enabling flexible UAV movement to accurately model continuous trajectory adjustments in real-world deployments.
\item Utilizing UE reference signals and \ac{AOA} measurements, instead of GPS data, improving robustness in scenarios with unknown UE locations.
\item Evaluating performance across diverse UE mobility patterns to validate PPO’s effectiveness in dynamic emergency scenarios.
\end{itemize}

\section{System Model}
\label{sec:system_model}

\subsection{System Description}

In this paper, 
we consider an emergency cellular network comprised of multiple UAVs serving as mobile BSs,
which attempt to serve the UEs of a team of first responders. 
This cellular network operates in both downlink and uplink using time division multiple access (TDMA). 

Let $\mathcal{U} = \{u_{\mathit{1}}, \ldots,u_{\mathit{u}}, \ldots, u_{U}\}$, represent the set of first responders, 
each one with his/her own UE, 
and its cardinality (i.e., the number of elements in the set $\mathcal{U}$) be denoted by $U$.  
The geographical position of the first $u^{th}$ responder is determined by
$\rho^\mathrm{U}_u = \{x^\mathrm{U}_u, y^\mathrm{U}_u, z^\mathrm{U}_u\}$,
and the position matrix of all first responders is represented as 
$\bm{\rho}^\mathrm{U} = \{\rho^\mathrm{U}_\mathit{1}, \ldots, \rho^\mathrm{U}_\mathit{u}, \ldots,  \rho^\mathrm{U}_U\}$. 

\begin{comment}   
{\color{red}Let $\mathcal{D}$ represent the set of UAVs,
each one with its own BS, 
with its cardinality denoted by $D$,
i.e.
\begin{equation}
    \mathcal{D} = \{d_{\mathit{1}}, \ldots,  d_{\mathit{d}}, \ldots,  d_{D}\},
\end{equation}
where the location information of UAV $d$ is determined by:
\begin{equation}
    \rho^\mathrm{D}_d = \{x^\mathrm{D}_d, y^\mathrm{D}_d, z^\mathrm{D}_d\},
\end{equation}
and the position matrix of all UAVs is represented as:
\begin{equation}
    \bm{\rho}^\mathrm{D}= \{\rho^\mathrm{D}_\mathit{1}, \ldots, \rho^\mathrm{D}_\mathit{d}, \ldots, \rho^\mathrm{D}_D\}.
\end{equation}
The rest of equations are also revised!!!!}\\
\end{comment}
Let $\mathcal{D} = \{d_1 , . . . , d_d , . . . , d_D\}$, denote the set of drones or UAVs, 
each one with its own BS, 
with its cardinality denoted by $D$. So, the location information of the $d^{th}$ UAV is represented by 
$\rho^\mathrm{D}_d = \{x^\mathrm{D}_d, y^\mathrm{D}_d, z^\mathrm{D}_d\}$,
and the position matrix of all UAVs is represented as 
$\bm{\rho}^\mathrm{D} = \{\rho^\mathrm{D}_1,...,\rho^\mathrm{D}_d,...,\rho^\mathrm{D}_D\}$.

\subsection{Channel Model}

We assume the network operates with a bandwidth $B$ at a frequency $f$. 
All radio links in the network experience slow and fast channel gains. 
We denote by $G_{u,d,k}$ the overall channel gain between the $u^{th}$ UE and the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource. 
This gain can be further decomposed into antenna gain ($G^{\rm a}$), path gain ($G^{\rm p}$), outdoor-to-indoor gain ($G^{\rm e}$), shadow gain ($G^{\rm s}$), and fast-fading gain ($G^{\rm{ff}}$),
i.e.
\begin{equation}
    G_{u,d,k} = G^{\rm a}_{u,d} \cdot G^{\rm p}_{u,d} \cdot G^{\rm e}_{u,d} \cdot G^{\rm s}_{u,d} \cdot \left|G^{\rm ff}_{u,d,k}\right|^2.
\end{equation}
In this study, 
we use the Urban Macro models specified by the third generation partnership project (3GPP) in TR25.814 to drive the calculation of each one of the presented channel components, 
with the following amendments: 
BS antennas are considered to be omnidirectional and the multi-path fading follows a Rician model. 
It is worth noting that the overall channel gain $G_{u,d,k}$ depends on both the position of the UAV $\rho^\mathrm{D}_d$ (our optimization variable) and the position of the first responder $\rho^\mathrm{U}_u$,
where this last one is unknown to us. 
Thus, the channel gain $G_{u,d,k}$  can be represented as 
$G_{u,d,k}(\rho^\mathrm{U}_u,\rho^\mathrm{D}_d)$.

The power received by the $u^{th}$ first responder from the BS of the $d^{th}$ UAV  within the $k^{th}$ frequency resource can be expressed as~\cite{meyers1946nonlinearity}:  
\begin{equation}
    P^{\rm{rx}}_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d) = P^{\rm{tx}}_{d,k} \cdot G_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d),
    \label{eq_intro:Received_power}
\end{equation}
where $P^{\rm{tx}}_d$ is the power transmitted by the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource.

The parameter that indicates the quality of the signal received by the $u^{th}$ first responder from the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource is the signal-to-interference-plus-noise ratio (SINR) $\gamma_{u,k}$, 
and can be calculated as~\cite{mozaffari2015drone, zhang2018downlink}:
\begin{eqnarray}
 	\gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) = \frac{P^{\mathrm rx}_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d)}{\sum_{\substack{ d\prime=0 \\ d\prime \neq d }}^{D} P^{\mathrm rx}_{u,d\prime,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d)+ \sigma^2_k},
	\label{eq_intro:sinr}
\end{eqnarray}
where $\sigma^2_k$ is the noise power in the $k^{th}$ frequency resource.

By applying the Shannon-Hartley theorem,
the data transmission rate of the $u^{th}$ first responder connected to the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource can be computed as~\cite{Djordjevic2022}:
\begin{eqnarray}
 	R_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) = B_k \log_2(1+\gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D})),
    \label{eq_intro:rate}
\end{eqnarray}
where $B_k$ is the bandwidth of the $k^{th}$ frequency resource.
If a scheduler is used to fairly distribute the available resources among the first responders in the cell, 
such as round-robin~\cite{rasmussen2008round},  
the transmission rate can derived as:
\begin{equation}
    R_u(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) =  \frac{B}{U} \log_2(1+\bar \gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D})),
    \label{eq_intro:mean_rate}
\end{equation}
where $\frac{B}{U}$ indicates the portion of the bandwidth that is allocated to each UE, on average, in the communication.  
Note that the average SINR $\bar \gamma_{u,d,k}$ of the UE in its allocated frequency resources is used as its effective SINR.

It should be noted that,
unlike many other approaches, 
our proposed scheme does not rely on GPS data from UEs for optimization, 
due to the uncertainty of its availability and reliability in emergency situations. 
Instead, 
we assume the use of reference signals, 
and angle of arrival (AoA) estimations over them, 
as proxies. 
In more details, 
we assume that the BS of the $d^{th}$ UAV  can estimate 
---using an antenna array and signal processing--- 
the AoA $\alpha_{u,d}$ of the reference signals received from the $u^{th}$ UE.
To comprehensively evaluate our approach, 
we adopt a two-phase analysis. 
First, we assume perfect AoA estimation by the BS of the $d^{th}$ UAV to establish a performance benchmark, 
allowing us to understand the full potential of our algorithm in ideal conditions. 
Then, we introduce Gaussian noise to simulate real-world imperfections in AoA estimation, 
assessing the algorithm’s robustness and effectiveness under more practical conditions.
\section{Problem Statement}
The objective of this work is to determine, in real-time, the UAV positions $\bm{\rho}^\mathrm{D}$ that maximize the total fair transmission rate \( R_{\text{fair}} \) of first responders.
This rate is defined as the sum of the logarithms of the transmission rates of all UEs~\cite{9878252},
that is:
\begin{equation}
    R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})  = \sum_{u \in \mathcal{U}}  \log_{10} (R_{u}
     (\rho^\mathrm{U}_u, \bm{\rho}^\mathrm{D})).
     \label{eq_intro:fair_rate}
\end{equation}
This ensures a balanced approach where increases in rates for UEs with lower rates are given more importance compared to those with higher rates, 
thus promoting fairness.
%Let us emphasize that it depends on the positions of the UAVs \(\bm{\rho}^\mathrm{D}\) and the positions of the UEs, \(\bm{\rho}^\mathrm{U}\), 
%as indicated earlier, 
%The implementation of a sum of the logarithms of individual transmission rates, \( R_{\text{fair}} \), is a strategy that makes the calculation fairer{\color{blue}This is because the logarithm is a concave function, which implies that additional increases in the transmission rate of a UE with an already high rate contribute less} to \( R_{\text{fair}} \) compared to increases in the transmission rate of a UE with a low rate.

With this in mind,
our optimization problem can then be formally formulated as:
\begin{equation}
    \text{max}_{\bm{\rho}^\mathrm{D}} R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D}).
\end{equation}

The problem's high dimensionality, stochasticity, and non-linearity makes this real-time UAV trajectory optimization highly challenging.
The continuous movement of first responders and fluctuations in the radio channel demand adaptive decision-making,
which traditional optimization techniques struggle to handle effectively.
To address these challenges, 
and given the advantages of PPO highlighted in the introduction,
we adopt a PPO algorithm to dynamically adjust UAV positions.
PPO enables our UAVs to continuously optimize $R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})$,
adapting to UE mobility patterns and radio condition variations in real-time.
The following section provides a detailed RL formulation and our PPO implementation.

\begin{comment}
The complexity of this problem, 
characterized by high dimensionality, stochasticity, and non-linearity, 
poses significant challenges for traditional optimization methods. 
The continuous movement of first responders and fluctuations in the radio channel demand constant adaptation, 
making these methods struggle with the system's dynamics and complexity. 
Given these challenges, 
machine learning (ML) emerges as a promising approach, 
having demonstrated remarkable performance in various complex tasks. 
However, while ML excels in classification and regression problems,
the sequential decision-making nature of our scenario calls for a more specialized solution.
This leads us to RL, 
a subfield of ML based on Markov decision processes, 
which offers a powerful framework for solving such problems \cite{sutton2018reinforcement}.
RL is particularly well-suited for our scenario as it excels at real-time adaptation through direct interaction with the environment. 
This enables RL to find optimal and robust policies that dynamically maximize the total fair transmission rate $R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})$, 
while continuously adjusting to changes in first responders' positions and radio conditions.
\end{comment}

\section{DRL-based PPO Algorithm}

In RL, 
an agent interacts with an environment,
learning to make decisions by receiving feedback in the form of rewards.  
This interaction is formalized through four key components:  
\begin{itemize}
    \item \textbf{States (S):} The possible situations the agent can observe.  
    \item \textbf{Actions (A):} The decisions the agent can take.  
    \item \textbf{Rewards (R):} The feedback the agent receives after taking an action.  
    \item \textbf{Policy ($\pi$):} The strategy the agent follows to decide which action to take in each state.  
\end{itemize}

Like its predecessor TRPO, 
PPO is an on-policy, model-free algorithm,
and belongs to the actor-critic family~\cite{schulman2015trust}.  
It extends the REINFORCE algorithm~\cite{williams1992simple} by incorporating a value function estimator,  
which stabilizes training and improves sample efficiency.  

Specifically,
PPO employs an advantage function to determine how much better a particular action is compared to the average action in a given state.  
This advantage function is defined as:  
\begin{equation}
     \hat{A}_t = r_t - V(s_t), 
\end{equation}  
where \( s_t \) and \( r_t \) represent the state and reward at time step \( t \),  
and \( V(s) \) is the value function.  
This advantage-based approach, 
combined with a value function, 
gives PPO its actor-critic nature,  
allowing simultaneous optimization of both the policy \( \pi \) and the value function \( V(s) \).  

Compared to traditional policy gradient methods~\cite{sutton2018reinforcement},
PPO is designed to address three key challenges:  
\begin{itemize}
    \item PPO improves training stability by using a clipped surrogate objective, 
    which limits the magnitude of policy updates and prevents large, destabilizing changes that can occur in traditional policy gradient methods.
    \item Despite being an on-policy method, 
    PPO is relatively sample-efficient, achieving good performance with fewer environment interactions compared to other RL methods.
    \item PPO also operates in both continuous  states-actions spaces, eliminating the need for discretization and providing a realistic and accurate representation.
\end{itemize}

\subsection{PPO Key Details}

PPO uses a clipped objective to ensure efficiency and stability,
while being easier to implement compared to other methods. 
The algorithm optimizes the policy by implementing a ``surrogate" objective function~\cite{schulman2017proximal}. 
The surrogate objective is given by:
\begin{equation}
    \begin{aligned}
    L^{\text{CLIP}}(\theta) &= \\
    & \hat{\mathbb{E}}_t[\min\big(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\big)],
    \end{aligned}
    \label{eq:clipperd_loss}
\end{equation}
where \(\theta\) represents the weight distribution of DNNs used in the algorithm, 
\( \hat{A}_t \) is the advantage function estimator at time-step \( t \), 
\( \epsilon \) is a hyperparameter controlling the extent of the policy update, 
and \(r_t(\theta)\) measures how much the policy has changed between updates.
Specifically, \(r_t(\theta)\) is the ratio of the probability of taking action \(a_t\) given state \(s_t\) under the current policy \(\pi_\theta\) to the probability of taking that same action under the old policy \(\pi_{\theta_{\text{old}}}\)~\cite{schulman2017proximal}:
\begin{equation}
    r_t(\theta) =\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}.
\end{equation}

PPO differs from TRPO by using the clipping mechanism instead of KL divergence.
The clipping ensures that the policy's updates do not deviate too far,
effectively saturating the objective value when the policy's update becomes too large. 
This discourages excessive changes to the policy by taking the minimum of the clipped and unclipped objectives.
Additionally, PPO incorporates generalized advantage estimation (GAE)~\cite{schulman2015high} to refine the advantage function. 
GAE applies a discount factor \( \lambda \) to balance the consideration of immediate and future rewards, 
which results in more efficient policy learning. 
This enables PPO to navigate complex environments, 
such as those encountered in UAV flights for first responders.

%In summary, PPO represents a cutting-edge approach in reinforcement learning, combining stability, efficiency, and simplicity. Its robust design makes it highly effective for addressing challenging sequential decision-making problems in real-world applications.

\subsection{Proposed PPO Implementation}

Without loss of generality,
considering a single UAV through subsequent experiment, 
we aim to analyze how the PPO optimizes the $d^{th}$ UAV position to serve the UEs of the first responders efficiently.
To this end, 
we define the state-space environment of our problem as follows:
\begin{equation}
    \tilde{\rho} = [\rho_{d,t}^{D}, \rho_{d,t-1}^{D}, \dots, \rho_{d,t-M}^{D}],
\end{equation}
\begin{equation}
    \tilde{\gamma}_u = [\tilde{\gamma}_{u,t}, \tilde{\gamma}_{u,t-1}, \dots, \tilde{\gamma}_{u,t-M}] ,
\end{equation}
\begin{equation}
    \mu_{\alpha} = [\mu_{\alpha,t}, \mu_{\alpha,t-1}, \dots, \mu_{\alpha,t-M}],
\end{equation}
\begin{equation}
    \sigma_{\alpha} = [\sigma_{\alpha,t}, \sigma_{\alpha,t-1}, \dots, \sigma_{\alpha,t-M}],
\end{equation}
\begin{equation}
    \text{S} = [\tilde{\rho}, \tilde{\gamma}_u, \mu_{\alpha}, \sigma_{\alpha}],
\end{equation}
where $\tilde{\rho}$ is the time-dependent vector of the positions of $d^{th}$ UAV,
%David: Notation is confusing. It seems Pos only relates to one UAV but in the paper so far it seems that we may have more than one UAV. We always talked about UAV d in the system model. We should work with the genreal case of D UAVs or  indicate at some point that we consider only 1 UAV. Done
$\tilde{\gamma}_u$ is the time-dependent vector of SINR measurements of the $u^{th}$ UE connected to the BS of the $d^{th}$ UAV, 
and \(\mu_{\alpha}\) and \(\sigma_{\alpha}\) denote the mean and standard deviation of the AoAs \(\alpha_{u,d}\ \forall u\) of the reference signals transmitted by the $u^{th}$ UEs of the first responders and received by the BS of the $d^{th}$ UAV. 
%David: That we are using radio signals and not position is one of our differentiators. We should give more importance to this. Present the capability of the BS to estiamte the angle of arrival from each UE in the system model having a given nomenclature and the you can present here that we get the average and the standard deviation. I repeat we must emphasize this part in the system model.  Working on it

Putting all these variables together, 
S is the state vector formed by Pos, $\tilde{\gamma}_u$, $\mu_{\alpha}$, and $\sigma_{\alpha}$. 
Note that the parameter $M$ represents the memory length, 
i.e. we store the last $M$ values in each vector. 

%The objective of utilizing these states is for the drone to infer the position of the UEs without directly being given their positions through the BS.
As we have a continuous action space, 
the agent has two variables to choose from: 
direction and movement magnitude. 
The direction is defined by the angle $\alpha_d$, 
and the magnitude is defined by the distance $r$. 
Hence, the action space is defined as $\mathcal{A} = \{(\alpha_d, r)\}$, 
where $\alpha_d \in [-180, 180)$ and $r \in [0, r_{max}]$. 
Here, $r_{max}$ is the maximum distance the UAV can travel in one action, 
and the angle $\alpha_d$ is defined with respect to the east. 
It is important to note that our implementation leverages the PPO's ability to work with continuous action spaces. 
By avoiding discretization, 
the agent can explore the full continuum of possible positions, 
enabling it to find optimal locations that maximize system throughput with high precision. 
This is particularly beneficial in our scenario, 
where small adjustments in UAV position can significantly impact network performance.

Through our proposed framework, 
the reward function is designed to maximize the total fair transmission rate $R_{\text{fair}}$, 
as introduced earlier. 
To ensure consistency and comparability of the reward values during training, 
a min-max normalization is applied. 
% \begin{equation}
% \text{Reward} = 2 \times (R_{fair} - 4) / (6.104 - 4) - 1
% \end{equation}
%David: We should have  $R_{\text{fair}}$ in the equation.
%David: I dont like this. Do not use numbers but variables, and we will say later in the results section that after optimizing this variables their values are ...  
%David: Explain with the 2 and the -1. 
This normalization centers the reward values and scales them, 
improving the stability and efficiency of the RL algorithm.

\section{Numerical Results and Discussion}

This section evaluates our UAV-based BS flights algorithm across various dynamic emergency scenarios, focusing on convergence, learned flights strategies, and network performance, particularly the total fair transmission rate $R_{fair}$.

\subsection{Experimental Setup}

\subsubsection*{Scenario}

To evaluate our algorithm's effectiveness, 
we consider a 200m × 200m area,
and that the BS of $d^{th}$ UAV operates a bandwidth $B=$10\,MHz at frequency $f=$ 2\,GHz band.
We use the system model presented in Section \ref{sec:system_model},
embracing the 3GPP UMa model in TR25.814.

\subsubsection*{UE Mobility Models}
We implement various UE mobility patterns within the designated area, increasing in complexity, where UEs reflect off boundaries upon contact. The UE mobility patterns are as follows:
\begin{itemize}
    \item \textit{Static UEs (No Move)}: A cluster of 10 UEs in a 10m × 10m square at the center, representing stationary scenarios.
    
    \item \textit{Linear Motion (Straight Walk)}: The same cluster moving in a straight line at 8 m/s, simulating unified group movement 
    ---the direction of the movement is randomly selected.
    
    \item \textit{Circular Motion}: The same cluster initialized at $x \in [0, 10]$, $y \in [-190, -200]$, 
    moving in a circular path (radius 200m, center at origin), 
    representing a more complex search pattern.
    
    \item \textit{Crossed Linear Motion}: Two clusters of 5 UEs each, 
    moving perpendicularly (90\textdegree\ gap) or in opposite directions (180\textdegree\ gap), simulating multiple independent groups.

    \item \textit{Random Hotspot and Mixed Movement}: Two groups of 5 UEs each: one cluster moving uniformly and another set of independently moving UEs. 
    Velocities are randomly assigned within 0 to 8 m/s, 
    simulating both coordinated and unpredictable movements in dynamic scenarios.
\end{itemize}
\subsubsection*{PPO Algorithm Configuration}

Our proposed algorithm based on PPO is trained using the following key parameters and configuration. 
The PPO algorithm is trained for 12,000 episodes, 
with each episode consisting of 128 frames. 
We employed a learning rate of 3e-4, 
and implemented three hidden layers in both the actor and critic networks, 
each containing 128 neurons. 
The algorithm used a discount factor $\gamma$ of 0.99 and a GAE parameter $\lambda$ of 0.95, 
balancing immediate and future rewards effectively. 
To ensure stable learning, 
we applied gradient clipping with a maximum norm of 1.0, 
and used a clip $\epsilon$ of 0.2 for the surrogate objective. 
Our model incorporated a memory size $M$ of 4, 
enabling the agent to have an idea of what the UE movement is without having the UE position directly.

\subsubsection*{Results and Discussion}

Figure~\ref{fig:throughput_analysis} illustrates the average throughput per episode during the evaluation phase for the various movement scenarios.
For reference, 
given the configuration,
the maximum achievable UE rate is around 8\,Mbps.
\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{Figs/NUMERICAL RESULT/throughput_analysis.pdf}
\caption{Average throughput per evaluation episode for different movement types}
\label{fig:throughput_analysis}
\end{figure}
The results reveal distinct trends for each movement type, highlighting the algorithm's adaptability and effectiveness across diverse scenarios. In scenarios with a single cluster of first responders (``No Move'', ``Straight Random Walk'', and ``Circular Movement''), the agent consistently achieves throughputs exceeding 7\,Mbps. This performance is maintained even with randomly varying speeds, demonstrating the agent's robust ability to optimize its position based solely on reference signal information. It is worth noting that the complex scenarios resulted in performance degradation and failed to achieve the maximum rate.
For scenarios involving two clusters of first responders (``Straight 90'' and ``Straight 180''), the agent effectively learns to optimize its position, progressively improving throughput over the episodes. This improvement indicates the algorithm's capacity to handle more complex spatial distributions of UEs, balancing performance between multiple groups. The ``Straight 180" scenario is the most challenging here, as the UE clusters get farther apart compared to the ``Straight 90" scenario, resulting in lower performance.
The ``Hotspot Random'' scenario, representing the most complex movement pattern, shows a gradual improvement in throughput, albeit with more variability. This scenario challenges the agent with both clustered and dispersed UEs moving at random velocities, yet the algorithm still manages to enhance performance over time.
These results collectively underscore the algorithm's adaptability across diverse movement patterns and its capacity to enhance network performance using only information derived from reference signals without relying on UE location information.

To further evaluate the effectiveness of our proposed approach, 
we compare the performance of our PPO-based agent against another baseline scenario where the UAV is statically positioned at the center of the environment. 
Table~\ref{tab:comparison} presents this comparison for each movement scenario.
\begin{table}[htbp]
\centering
\caption{Comparison of Average Throughput: PPO vs. Static}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Scenario} & \textbf{PPO} & \textbf{Static} & \textbf{Diff.}&\textbf{Gain} \\
& \textbf{(Mbps)} & \textbf{(Mbps)} & \textbf{(Mbps)}& \textbf{(\%)} \\
\midrule
No Move & 8.00 & 7.95 &+0.05 &2\% \\
Straight Random & 7.87 & 4.59 & +3.28&71\% \\
Circular & 7.08 & 3.59 & +3.49&97\% \\
Straight 90\textdegree & 6.27 & 5.33 & +0.94&17\% \\
Straight 180\textdegree & 5.91 & 5.31 & +0.60&11\% \\
Hotspot Random & 5.19 & 4.77 & +0.42&8\%\\
\bottomrule
\end{tabular}
\end{table}
The results for the ``Straight 90°" and ``Straight 180°" scenarios are particularly noteworthy, with the PPO algorithm outperforming the static solution by 17.64\% and 11.30\%, respectively.
This improvement may be surprising, as one might expect a centrally located static UAV to be an optimal solution for two groups moving in perpendicular or opposite directions. However, the fact that our PPO-based approach, which takes radio propagation conditions and network performance into account through the reward function, identifies a strategy that surpasses this geometrically optimal static position suggests that the algorithm is effectively leveraging its mobility to optimize coverage in a dynamic and non-obvious way.

Furthermore, we analyze the convergence when Gaussian noise is added to the AoA estimation. 
Table~\ref{tab:throughput_std_straight_random} presents the results of this simulation for the ``Straight Random" movement scenario.
\begin{table}[htbp]
    \centering
    \caption{Throughput values for the ``Straight Random" scenario with different noise levels}
    \label{tab:throughput_std_straight_random}
    \small
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        \textbf{STD} & 0 & 1 & 5 & 100 & 50 & 100 \\
        \midrule
        \textbf{PPO (Mbps)} & 7.87 & 7.84 & 7.82 & 7.80 & 7.70 & 7.16 \\
        \bottomrule
    \end{tabular}
\end{table}
The results demonstrate the algorithm's robustness to noise in AoA estimation. Even with a significant noise level (STD = 100), the throughput remains high at 7.16 Mbps, only a 0.71 Mbps decrease from the noise-free scenario. This resilience to noise underscores the practical viability of our approach in real-world deployments, where perfect AoA estimations may be unlikely. The algorithm's ability to maintain high throughput even with noisy AoA measurements highlights its potential for reliable performance in challenging emergency communication scenarios.

%% \subsection{Sum of Rewards Analysis}
% Figure~\ref{fig:reward_analysis} shows the sum of rewards per step for different movement setting during training.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{Figs/NUMERICAL RESULT/reward_analysis.pdf}
% \caption{Sum of Rewards per Step for Different Movement Types}
% \label{fig:reward_analysis}
% \end{figure}
% Key observations corresponded to each scenario are as follows:
% \begin{itemize}
% \item All scenarios trend towards higher rewards over time.
% \item ‘‘No Move’’ scenario shows the most stable and highest rewards.
% \item The algorithm adapts well to complex scenarios like ‘‘Straight Walk’’ and ‘‘Circular Movement’’.
% \item ‘‘Random Hotspot’’ exhibits the highest volatility and lowest rewards.
% \item ‘‘Crossed Straight Walk’’ shows rapid improvement and stabilization.
% \end{itemize}
% %\subsection{Average Distance Analysis}
% Figure~\ref{fig:distance_analysis} presents the average distance per step for different movement setting.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{Figs/NUMERICAL RESULT/distance_analysis.pdf}
% \caption{Average Distance per Step for Different Movement Types}
% \label{fig:distance_analysis}
% \end{figure}
% Key insights concluded from each scenario are as follows:
% \begin{itemize}
% \item General trend towards lower average distances per step.
% \item ‘‘No Move’’ scenario quickly converges to minimal movement.
% \item ‘‘Straight Walk’’ and Circular Movement’’ show gradual decrease in distances.
% \item ‘‘Crossed Straight Walk’’ maintains a relatively constant average distance.
% \item ‘‘Random Hotspot’’ exhibits the highest and most volatile distances.
% \item Decreasing trends indicate the UAV learns to make more precise movements.
% \end{itemize}
% %\subsection{Summary of Results}

% These results demonstrate the effectiveness of our PPO-based approach in learning optimal UAV-based BS positioning strategies across a diverse range of UE mobility scenarios. The algorithm shows particular strength in adapting to predictable movement patterns while maintaining the ability to handle more chaotic scenarios. This adaptability is crucial for real-world applications where UE mobility patterns may vary or change over time.

\balance

\section{Conclusion}

%This paper demonstrates the effectiveness of UAV-MBS for emergency communications, emphasizing the crucial role of optimal UAV positioning in enhancing emergency network performance. To ensure a realistic model,we introduced a continuous action space DRL approach, where the agent selects both the direction and movement magnitude dynamically. Our method, leveraging PPO, enables UAVs to adapt to various UE movement patterns and network topologies effectively. A key contribution of this work is the use of widely available RSs, rather than relying solely on UAV and UE positions,  to drive the state-action space. This approach enhances the model's realism, especially in scenarios where GPS or accurate UE location data may be unavailable. The results confirm the algorithm’s adaptability and effectiveness in maintaining comprehensive coverage across different scenarios, representing a significant advancement in real-time UAV positioning for improved emergency response.
This paper studies the UAV-based BS problem in emergency communications by emphasizing the importance of optimal UAV flights to enhance network performance. It introduces a continuous action space DRL approach using PPO, allowing UAVs to dynamically select both direction and movement magnitude. The key innovation is the use of widely available reference signals to drive the state-action space, improving model realism in scenarios where GPS or accurate UE location data is unavailable. The results confirm the algorithm's adaptability and effectiveness in maintaining comprehensive coverage across various scenarios, marking a significant advancement in real-time UAV flights for improved emergency response.
As a future work, 
we will consider deploying multiple UAVs as aerial BSs to further enhance network coverage and reliability,
and explore advanced coordination strategies. 
%As the future work, we plan to explore advanced coordination strategies and dynamic resource allocation among the UAVs. Additionally, we aim to integrate more complex UE mobility models and environmental factors to improve the robustness and scalability of our approach.}

\bibliography{bibliography}



\end{document}

BACHELOR THESIS 06.2024 FOR TELECOM ENGINEERING 

UNIVERSITAT POLITÈCNICA DE VALÈNCIA
Escuela Técnica Superior de Ingeniería de
Telecomunicación
Posicionamiento de estaciones base en tiempo real basado
en aprendizaje de refuerzo profundo para futuras redes 6G.
Trabajo Fin de Grado
Grado en Ingeniería de Tecnologías y Servicios de
Telecomunicación
AUTOR/A: Rico Ibáñez, Mario
Tutor/a: Naranjo Ornedo, Valeriana
Cotutor/a: López Pérez, David
CURSO ACADÉMICO: 2023/2024Resumen
Las comunicaciones móviles son esenciales en la sociedad actual, definiendo cómo nos comunicamos y relacionamos, e influyendo en cómo vivimos. La optimización de este bien tan valioso
es esencial para proporcionar la mejor experiencia posible de forma sostenible. Para mejorar las
prestaciones de la red y su eficiencia energética, se espera que las futuras generaciones de comunicaciones incorporen estaciones base capaces de reposicionarse de forma sencilla, sin intervención
humana, mediante vehículos terrestres o aeronaves no tripuladas, como drones.
En esta tesis, proponemos una solución inteligente que permite a dichas estaciones base móviles
adaptarse y aprender los patrones de movimiento de los usuarios, y encontrar su posición óptima
en tiempo real. Para cuantificar la experiencia del usuario, utilizamos la tasa de datos media de
los usuarios. Para guiar dichas decisiones y aprender políticas óptimas, hemos utilizado técnicas
avanzadas de aprendizaje, en más detalle, aprendizaje reforzado profundo (DRL, del inglés deep
reinforcement learning).
Como primer punto de trabajo, es importante enfatizar el uso, desarrollo y optimización de un
entorno de simulación avanzado de redes de comunicaciones móviles 4G/5G/6G, llamado Giulia,
basado en Python. Dicha optimización implicó la transformación de un número sustancial de operaciones a métodos más eficientes, como el uso de GPU a través de la biblioteca PyTorch. Como
mayor contribución de este trabajo, implementamos técnicas avanzadas de DRL, adaptándolas a
nuestro problema específico, que combinan la potencia de las redes neuronales profundas (DNN,
del inglés deep neural networks) con la optimización del aprendizaje reforzado. A través del trabajo
de esta tesis hemos sentado unas bases sólidas y conocimiento acerca de la integración de DRL con
entornos de simulación complejos para la toma de decisiones en tiempo real. Hemos demostrado
que el uso de estadísticas acerca de la potencia de señal recibida, el ángulo de llegada y la tasa de
datos de los usuarios es información suficiente para aprender patrones de movimiento de grupos
de usuarios, y optimizar en tiempo real el movimiento de estaciones base móviles que pretenden
maximizar la experiencia de dichos usuarios. Los resultados de la tesis indican que DRL puede ser
utilizado en las siguientes generaciones de redes de comunicaciones que pueden ser controladas
por agentes autónomos en tiempo real.Resum
Les comunicacions mòbils són essencials en la societat actual, definint com ens comuniquem i
ens relacionem, i influint en com vivim. L’optimització d’aquest bé tan valuós és essencial per a
proporcionar la millor experiència possible de manera sostenible. Per a millorar les prestacions
de la xarxa i la seua eficiència energètica, s’espera que les futures generacions de comunicacions
incorporen estacions base capaces de reposicionar-se de forma senzilla, sense intervenció humana,
mitjançant vehicles terrestres o aeronaus no tripulades, com drons.
En aquesta tesi, proposem una solució intel∙ligent que permet a aquestes estacions base mòbils
adaptar-se i aprendre els patrons de moviment dels usuaris, i trobar la seua posició òptima en temps
real. Per a quantificar l’experiència de l’usuari, utilitzem la taxa de dades mitjana dels usuaris.
Per a guiar aquestes decisions i aprendre polítiques òptimes, hem utilitzat tècniques avançades
d’aprenentatge, en més detall, aprenentatge reforçat profund (DRL, de l’anglés deep reinforcement
learning).
Com a primer punt de treball, és important enfatitzar l’ús, desenvolupament i optimització d’un
entorn de simulació avançat de xarxes de comunicacions mòbils 4G/5G/6G, anomenat Giulia, basat
en Python. Aquesta optimització va implicar la transformació d’un nombre substancial d’operacions
a mètodes més eficients, com l’ús de GPU a través de la biblioteca PyTorch. Com a major contribució d’aquest treball, implementem tècniques avançades de DRL, adaptant-les al nostre problema
específic, que combinen la potència de les xarxes neuronals profundes (DNN, de l’anglés deep
neural networks) amb l’optimització de l’aprenentatge reforçat. A través del treball d’aquesta tesi
hem assentat unes bases sòlides i coneixement sobre la integració de DRL amb entorns de simulació complexos per a la presa de decisions en temps real. Hem demostrat que l’ús d’estadístiques
sobre la potència de senyal rebuda, l’angle d’arribada i la taxa de dades dels usuaris és informació suficient per a aprendre patrons de moviment de grups d’usuaris, i optimitzar en temps real el
moviment d’estacions base mòbils que pretenen maximitzar l’experiència d’aquests usuaris. Els
resultats de la tesi indiquen que DRL pot ser utilitzat en les següents generacions de xarxes de
comunicacions que poden ser controlades per agents autònoms en temps real.
3Abstract
Mobile communications are essential in today’s society, defining how we communicate and interact, and influencing how we live. The optimization of this valuable asset is crucial to provide
the best possible experience in a sustainable way. To improve network performance and energy
efficiency, it is expected that future generations of communications will incorporate base stations
capable of repositioning easily, without human intervention, using ground vehicles or unmanned
aerial vehicles (UAVs), such as drones.
In this thesis, we propose an intelligent solution that allows these mobile base stations to adapt
and learn user movement patterns, and find their optimal position in real-time. To quantify the
user experience, we use the average data rate of users. To guide these decisions and learn optimal
policies, we have used advanced learning techniques, specifically, deep reinforcement learning
(DRL).
As the first point of work, it is important to emphasize the use, development, and optimization of an
advanced simulation environment for 4G/5G/6G mobile communication networks, called Giulia,
based on Python. This optimization involved transforming a substantial number of operations into
more efficient methods, such as using GPU through the PyTorch library. As the main contribution
of this work, we implemented advanced DRL techniques, adapting them to our specific problem,
which combine the power of deep neural networks (DNN) with the optimization of reinforcement
learning. Through the work of this thesis, we have laid a solid foundation and knowledge about
the integration of DRL with complex simulation environments for real-time decision-making. We
have demonstrated that the use of statistics on received signal strength, angle of arrival, and user
data rate is sufficient information to learn movement patterns of user groups and optimize in realtime the movement of mobile base stations that aim to maximize the experience of these users.
The results of the thesis indicate that DRL can be used in the next generations of communication
networks that can be controlled by autonomous agents in real-time.
4RESUMEN EJECUTIVO
La memoria del TFG del GTIST debe desarrollar en el texto los siguientes conceptos, debidamente justificados y discutidos, centrados en el ámbito
de la IT
CONCEPT (ABET) CONCEPTO (traducción) ¿Cumple?
(S/N)
¿Dónde?
(páginas)
1. IDENTIFY: 1. IDENTIFICAR:
1.1. Problem statement and opportunity 1.1. Planteamiento del problema y oportunidad S 1.
1.2. Constraints (standards, codes, needs,
requirements & specifications)
1.2. Toma en consideración de los condicionantes
(normas técnicas y regulación, necesidades,
requisitos y especificaciones)
S
2-5,
9-36,
37-45.
1.3. Setting of goals 1.3. Establecimiento de objetivos S 6-7.
2. FORMULATE: 2. FORMULAR:
2.1. Creative solution generation (analysis) 2.1. Generación de soluciones creativas (análisis) S 45-53,
57-66.
2.2. Evaluation of multiple solutions and
decision-making (synthesis)
2.2. Evaluación de múltiples soluciones y toma de
decisiones (síntesis) S
45-53,
57-66,
70-74.
3. SOLVE: 3. RESOLVER:
3.1. Fulfilment of goals 3.1. Evaluación del cumplimiento de objetivos S 53,
69-74.
3.2. Overall impact and significance
(contributions and practical recommendations)
3.2. Evaluación del impacto global y alcance
(contribuciones y recomendaciones prácticas) S 77-78.
Escuela Técnica Superior de Ingeniería de Telecomunicación
Universitat Politècnica de València
Edificio 4D. Camino de Vera, s/n, 46022 Valencia
Tel. +34 96 387 71 90, ext. 77190
www.etsit.upv.esA mis padres y mi abuela,
por mantener siempre en mi esas ganas de mejorar, apoyo incondicional y un cariño único.
A Alejandro, Sergio y Martina,
los mejores hermanos que he podido tener, gracias por hacerme feliz.
A mis amigos,
por ser un pilar fundamental y una gran fuente de diversión.
A mis tutores, Valery y David,
por confiar en mi y apoyarme en todo lo que pueden.
Todo esto es gracias a vosotros.Índice general
I Memoria
1. Introducción 1
1.1. Motivación y Descripción del Trabajo . . . . . . . . . . . . . . . . . . . . . . . 1
1.2. Introducción Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1. Estado del Arte de La Inteligencia Artificial Aplicada a Drones . . . . . 3
1.3. Establecimiento del Problema . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3.1. Modelo del Sistema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3.2. Objetivo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4. Modelado del Entorno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5. Estructura del Trabajo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2. Aprendizaje Reforzado Profundo 9
2.1. Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1. Proceso de Decisión de Markov . . . . . . . . . . . . . . . . . . . . . . 9
2.1.2. Periodicidad y Estados Terminales . . . . . . . . . . . . . . . . . . . . . 10
2.1.3. Procesos de Decisión de Markov Parcialmente Observables . . . . . . . 12
2.1.4. Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.5. Probabilidad de Trayectoria . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.6. Funciones de Valor y Q . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.6.1. Funciones Q y de Valor con Factor de Descuento . . . . . . . . 14
2.1.6.2. Funciones Óptimas . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.7. Ecuaciones de Bellman . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2. Métodos de Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.1. Clasificación según El Objetivo . . . . . . . . . . . . . . . . . . . . . . 16
2.2.2. Clasificación según El Tipo de Política . . . . . . . . . . . . . . . . . . 16
2.2.3. Clasificación según El Uso de Modelo . . . . . . . . . . . . . . . . . . . 16
2.3. Aprendizaje por Valor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.1. Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.1.1. Exploración versus Explotación . . . . . . . . . . . . . . . . . 19
2.3.2. SARSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4. Aprendizaje por Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4.1. Iteración de Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4.1.1. Evaluación de la Política . . . . . . . . . . . . . . . . . . . . 21
2.4.1.2. Mejora de la Política . . . . . . . . . . . . . . . . . . . . . . . 21
2.4.2. Política de Gradientes . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4.2.1. El Teorema de La Política de Gradiente . . . . . . . . . . . . . 222.4.2.2. Algoritmo REINFORCE . . . . . . . . . . . . . . . . . . . . 24
2.4.3. El Problema de La Expansión Exponencial del Espacio de Estados . . . . 25
2.5. Redes Neuronales Artificiales . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.1. Perceptrón . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.2. Funciones de Activación . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.3. Perceptrón Multicapa . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.5.4. Proceso de Entrenamiento . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.5.4.1. Feedforward . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.5.4.2. Cálculo de Pérdida . . . . . . . . . . . . . . . . . . . . . . . . 29
2.5.4.3. Cálculo de Gradiente y Backpropagation . . . . . . . . . . . . 30
2.6. Estado del Arte Aprendizaje Reforzado Profundo . . . . . . . . . . . . . . . . . 34
3. Giulia 37
3.1. Naturaleza del Simulador . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.2. Cálculo de La Tasa de Transmisión . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.1. Ganancia de Antena . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.2. Modelado de La Pérdida por Distancia . . . . . . . . . . . . . . . . . . . 39
3.2.3. Modelado de Shadowing . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.4. Ganancia de Penetración . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.2.5. Modelado del Desvanecimiento Multicamino . . . . . . . . . . . . . . . 42
3.2.6. Modelado de Potencia de Señal Recibida . . . . . . . . . . . . . . . . . 43
3.2.7. Modelado de La Calidad de Señal . . . . . . . . . . . . . . . . . . . . . 44
3.2.8. Calculo de La Tasa de Transmisión . . . . . . . . . . . . . . . . . . . . 45
3.3. Creación de Un Entorno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.4. Optimizaciones sobre Giulia . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.4.1. Plotting Ineficiente . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.2. Funciones Vectorizadas . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.3. Implementación GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.4.4. Comparación de Eficiencia del Simulador . . . . . . . . . . . . . . . . . 53
4. Metodología 55
4.1. Diseño del Problema de RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.1. Espacio de Estados . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.2. Espacio de Acciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.1.3. Recompensa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2. Algoritmos de RL para Resolver El Problema . . . . . . . . . . . . . . . . . . . 57
4.2.1. Deep Q Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.1. Naturaleza . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.2. Función de Pérdida . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.3. Red Objetivo . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.4. Gradiente . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.5. Deadly Triad . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.6. Replay Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.7. Exploración versus Explotación . . . . . . . . . . . . . . . . . 58
4.2.2. Trust Region Policy Optimization y Proximal Policy Optimization . . . . 59
4.2.2.1. Naturaleza . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.2.2.2. Beneficios . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60ÍNDICE GENERAL
4.2.2.3. Trust Region Policy Optimization . . . . . . . . . . . . . . . . 60
4.2.2.4. Proximal Policy Optimiaztion . . . . . . . . . . . . . . . . . . 61
4.2.2.5. Generalized Advantage Estimation . . . . . . . . . . . . . . . 61
4.3. Implementación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.3.1. Detalles de Implementación . . . . . . . . . . . . . . . . . . . . . . . . 64
4.3.1.1. Selección de Acciones . . . . . . . . . . . . . . . . . . . . . . 64
4.3.1.2. Entropía para La Selección de Acciones . . . . . . . . . . . . 64
4.4. Casos de Uso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1. Movimiento UEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1.1. UEs Estáticos . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1.2. UEs Dinámicos . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2. Inicialización Agentes . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 65
4.5. Parámetros e Hiperparámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.1. Parámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2. Hiperparámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2.1. DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2.2. PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5. Resultados 69
5.1. User Equipments Estáticos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2. UEs en Movimiento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2.1. Movimiento Lineal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.2.1.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 71
5.2.1.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 71
5.2.2. Movimiento Circular . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2.2.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 72
5.2.2.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 73
5.3. Experimentos de Ablación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3.1. Número de Capas y Neuronas . . . . . . . . . . . . . . . . . . . . . . . 75
5.3.2. Tasa de Aprendizaje y Número de episodios . . . . . . . . . . . . . . . . 75
5.3.3. Entropía de Epsilon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6. Conclusiones 77
Bibliografía 79
9Índice de figuras
1.1. Ejemplo situación con UEs en movimiento y red terrestre sin servicio. . . . . . . 2
1.2. Esquema básico entorno RL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.1. Estado inicial. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2. Episodio óptimo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3. Episodio subóptimo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.4. Proceso de decisión de Markov (MDP) con observación completa. . . . . . . . . 12
2.5. Proceso de decisión de Markov parcialmente observable (POMDP). . . . . . . . 12
2.6. Ejemplo cálculo valor V. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.7. Clasificación de diferentes modelos. . . . . . . . . . . . . . . . . . . . . . . . . 17
2.8. Algoritmo de aprendizaje Q y tabla Q. . . . . . . . . . . . . . . . . . . . . . . . 18
2.9. Esquema de un perceptrón. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.10. Esquema de un perceptrón multicapa. . . . . . . . . . . . . . . . . . . . . . . . 28
2.11. Grafo función g. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.12. Grafo computación con gradientes. . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1. Inclinación de una estación base [38]. . . . . . . . . . . . . . . . . . . . . . . . 39
3.2. Diagrama de radiación de la antena [38]. . . . . . . . . . . . . . . . . . . . . . . 40
3.3. Comparación entre el modelo ideal y el modelo 3GPP TR 36.814 de pérdida por
distancia. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4. Mapa desvanecimiento multicamino [38]. . . . . . . . . . . . . . . . . . . . . . 43
3.5. SINR ciudad de Dublín [38]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.6. Código ejemplo bucles for en list comprehension. . . . . . . . . . . . . . . . . . 48
3.7. Mejora por indexamiento directo de vectores. . . . . . . . . . . . . . . . . . . . 48
3.8. Código transformación de posición LCS a GCS en bucle for. . . . . . . . . . . . 50
3.9. Código transformación de posición LCS a GCS sin bucle for. . . . . . . . . . . . 51
3.10. Código fast fading de Numpy. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.11. Código asignación automática dispositivo. . . . . . . . . . . . . . . . . . . . . . 51
3.12. Código fast fading de torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.13. Código mW a dB de torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.14. Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR. . . . . 52
3.15. Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR usando
torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.1. Ejemplo cálculo ángulos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2. Visualización tabla Q DQN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3. Ejemplo PPO. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.4. Sistema de entorno y agente. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634.5. Diagrama de flujo del entrenamiento PPO con torchrl. . . . . . . . . . . . . . . . 63
5.1. Comparación de trayectorias PPO y DQN. . . . . . . . . . . . . . . . . . . . . . 69
5.2. Comparación de recompensas entre DQN (fila de arriba) y PPO (fila de abajo). . 70
5.3. Visualización de episodios con posición de inicialización (0, -100). . . . . . . . . 72
5.4. Comparación de métricas en diferentes inicializaciones durante entrenamiento aleatorio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.5. Comparación de métricas en diferentes inicializaciones . . . . . . . . . . . . . . 73
5.6. Visualización de episodios con movimiento circular con inicialización en (0,0). . 74
5.7. Comparación de métricas en diferentes inicializaciones, incluyendo inicialización
en (0,0) no random. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75Índice de tablas
3.1. Parámetros típicos de una antena sectorial. . . . . . . . . . . . . . . . . . . . . . 39
3.2. PDPs para UEs peatones (Pedestrian, ≤3 km/h). . . . . . . . . . . . . . . . . . . 43
3.3. Resultados después de plotting ineficiente. . . . . . . . . . . . . . . . . . . . . . 47
3.4. Resultados tras vectorización y uso de GPU. . . . . . . . . . . . . . . . . . . . . 53
3.5. Porcentaje de tiempo ahorrado tras optimización. . . . . . . . . . . . . . . . . . 54
4.1. Parámetros del simulador. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.2. Hiperparámetros del algoritmo DQN. . . . . . . . . . . . . . . . . . . . . . . . 67
4.3. Hiperparámetros del algoritmo PPO. . . . . . . . . . . . . . . . . . . . . . . . . 67Listado de siglas empleadas
3GPP 3rd Generation Partnership Project..
6G Sixth-generation Technology for Wireless Communication.
A3C Asynchronous Advantage Actor Critic..
BS Base Station..
CNN Convolutional Neural Network..
DDPG Deep Deterministic Policy Gradient..
DQN Deep-Q-Network..
DRL Deep Reinforcement Learning..
GAE Generalized Advantage Estimation..
GAN Generative Adversarial Network..
ITU International Telecommunication Union..
MAC Media Access Control..
MDP Markov Decision Process..
ML Machine Learning..
MPC Multipath Component..
PDP Power Delay Profile..
POMDP Partially Observable Markov Decision Process..
PPO Proximal Policy Optimization..
ReLU Rectified Linear Unit..
RL Reinforcement Learning..RNN Recurrent Neural Network..
SARSA State-Action-Reward-State-Action..
SINR Signal-to-Interference-plus-Noise Ratio..
TDMA Time Division Multiple Access ..
TRPO Trust Region Policy Optimization..
UAV Unmanned Aerial Vehicle..
UES User Equipment..Parte I
MemoriaCapítulo 1
Introducción
1.1. Motivación y Descripción del Trabajo
Las situaciones de emergencia y accidentes, tales como los desastres naturales, plantean desafíos
significativos para las infraestructuras de comunicaciones. Durante estos eventos, las redes de comunicación se vuelven vitales para una variedad de tareas críticas, incluyendo las operaciones de
rescate, la coordinación de recursos y la comunicación entre equipos de emergencia y afectados.
Sin embargo, las redes de comunicación actuales presentan limitaciones importantes. A menudo,
no pueden desplegarse fácilmente en cualquier localización o cualquier momento, lo que dificulta
su uso efectivo en situaciones de emergencia.
En este contexto, las tecnologías emergentes de la sexta generación (6G) de comunicaciones móviles ofrecen una solución prometedora. Una de las innovaciones más destacadas es el uso de drones,
los cuales permiten el despliegue de redes de comunicación casi en cualquier lugar y en cualquier
momento, superando así las limitaciones de las infraestructuras fijas tradicionales. Estos drones
pueden portar estaciones base móviles, proporcionando cobertura de red en áreas afectadas por
desastres donde la infraestructura tradicional ha sido destruida o es inaccesible.
Sin embargo, la mera disponibilidad de drones no es suficiente para resolver todos los problemas
asociados con la comunicación en situaciones de emergencia. Es esencial que estos drones sean
capaces de adaptarse de manera dinámica y eficiente a las necesidades cambiantes de los equipamiento de usuario (UE, del inglés user equipment) y las condiciones del entorno. Estos drones
pueden ser realmente necesarios en operativos móviles, donde el seguimiento de estos equipos de
operación es crucial para un correcto funcionamiento de la red. Para lograr esto, se requiere una
inteligencia avanzada que permita a los drones tomar decisiones en tiempo real, como se observa
en la Figura 1.1.
En este trabajo, proponemos el desarrollo de un agente de aprendizaje reforzado (RL, del inglés
reinforcement learning) capaz de adaptarse dinámicamente a los movimientos de los UEs y a las
condiciones del entorno en tiempo real. Este agente empleará técnicas avanzadas de aprendizaje
prrofundo por refuerzo (DRL, del inglés deep reinforcement learning) para aprender el movimiento
de los UEs y optimizar los patrones de movimiento de los drones. Con esta capacidad de adaptación,
los drones podrán no solo posicionarse de manera óptima, sino también ajustarse continuamente a
los cambios en el entorno y las necesidades de los UEs, maximizando así la eficiencia de la red y
mejorando significativamente la experiencia del UE.
1CAPÍTULO 1. INTRODUCCIÓN
Figura 1.1: Ejemplo situación con UEs en movimiento y red terrestre sin servicio.
1.2. Introducción Aprendizaje Reforzado
El RL [1] es un subcampo del aprendizaje automático (ML, del inglés machine learning) que se
inspira en el comportamiento humano y animal. En este campo, se desarrollan algoritmos capaces
de aprender a seleccionar las mejores acciones a tomar en un entorno determinado, basándose en
la maximización de recompensas acumuladas a largo plazo. En el marco típico del aprendizaje
reforzado, se identifican los siguientes componentes clave:
Entorno: Es el dominio en el que reside la dinámica del problema que se quiere resolver.
El entorno puede ser, tanto simulado, como real, y define las reglas y condiciones bajo las
cuales el agente opera. Incluye todos los posibles estados y las transiciones entre ellos.
Agente: Es el componente que interactúa con el entorno, tomando acciones en función de las
decisiones del algoritmo de aprendizaje reforzado. Aprendiendo a través de la experiencia
qué acciones son las más beneficiosas en los diferentes estados del entorno.
El esquema típico del RL se observa en la Figura 1.2. Se puede apreciar como el agente y el entorno
se comunican. El agente toma una acción sobre el entorno y observa el nuevo estado además de la
recompensa de haber tomado la acción.AgenteAgente
Entorno
Estado

Recompensa Acciones
Figura 1.2: Esquema básico entorno RL.
Durante los últimos años, se ha empezado a aplicar el ML a casos de uso de telecomunicaciones
relacionados con drones. Sin embargo el uso del RL no ha sido tan común. A continuación, se
describen algunos de ellos.
21.2. INTRODUCCIÓN APRENDIZAJE REFORZADO
1.2.1. Estado del Arte de La Inteligencia Artificial Aplicada a Drones
En la última década, los Vehículos Aéreos No Tripulados (UAVs) han proporcionado soluciones
eficientes y rentables para la recolección de datos y comunicaciones. Su movilidad, flexibilidad y
rápida implementación han permitido su uso extensivo en agricultura, misiones de rescate, ciudades
inteligentes y sistemas de transporte inteligente. El ML ha demostrado ser capaz de mejorar la
automatización y precisión operativa de los UAVs y de muchas de sus aplicaciones.
A medida que los UAVs continúan evolucionando, se han explorado diversas técnicas de ML para
mejorar sus capacidades operativas. Estas técnicas no solo han optimizado las funciones básicas
de los UAVs, sino que también han abierto nuevas oportunidades para aplicaciones cada dia mas
avanzadas y especializadas.
El artículo [2] proporciona una visión general y comprensible de las técnicas de ML utilizadas
en las operaciones y comunicaciones de UAVs, identificando las áreas de crecimiento potencial y
las brechas de investigación. Se destacan cuatro componentes clave en las operaciones y comunicaciones de UAVs donde el ML puede contribuir significativamente: percepción y extracción
de características, interpretación y regeneración de características, planificación de trayectorias y
misiones, y control y operación aerodinámica.
En el ámbito de la percepción y extracción de características, el ML permite la interpretación del
entorno y la extracción de información relevante. Las técnicas supervisadas, como las redes neuronales convolucionales (CNN, del inglés Convolutional Neural Network) y las redes neuronales
recurrentes (RNN, del inglés Recurrent Neural Network), se utilizan para la clasificación y segmentación de imágenes y la predicción de trayectorias, respectivamente. Por ejemplo, en el trabajo
realizado en la Reserva Natural de las Cinco Islas, Australia, [3] se evaluaron tres enfoques para
mapear la vegetación usando imágenes recogidas por un UAV. Se utilizó un algoritmo de clasificación de imágenes basado en CNN, LeNet, para detectar matojos plantados de Lomandra longifolia,
con una precisión del 85 %.
La interpretación y regeneración de características involucra el modelado y digitalización del entorno basado en los datos recogidos. Los Autoencoders y redes generativas adversarias (GAN, del
inglés Generative Adversarial Networks) se han aplicado para la reducción de dimensionalidad y
clasificación de características visuales en imágenes aéreas. En [4], se ha propuesto una técnica
para el modelado del canal aire-tierra en frecuencias de onda milimétrica (mmWave) en una red
inalámbrica de UAVs. Un enfoque cooperativo basado en GAN distribuido permite a cada UAV
aprender la distribución del canal mmWave de manera totalmente distribuida, mejorando la precisión del aprendizaje y la tasa de datos.
El uso de DRL en comunicaciones móviles se centra principalmente en las capas física y la de control de acceso al medio (MAC, del inglés Media Access Control) [5]. En la capa física, la mayoría
de los trabajos abordan problemas como la optimización del beamforming, control de potencia e
interferencias y consumo energético [6]. Se ha demostrado cómo la optimización conjunta de beamforming y control de potencia e interferencias puede mejorar significativamente el rendimiento de
la red en términos de señal a interferencia más ruido (SINR, del inglés Signal to Interference plus
Noise ratio) y tasa de datos en bandas de frecuencia sub-6 GHz y mmWave. En la capa MAC, los
esfuerzos se enfocan en desarrollar schedulers que distribuyan eficientemente los recursos de la
red. Un trabajo notable presenta el modelo LEASCH; un scheduler basado en DRL que ha demostrado ser eficiente independientemente de las diferentes configuraciones de numerología en redes
5G [7].
3CAPÍTULO 1. INTRODUCCIÓN
En cuanto a la capa de red, se han creado agentes que optimicen el enrutamiento de paquetes. Un
ejemplo de ello, es un agente de DRL que se adapta automáticamente a las condiciones actuales
del tráfico y minimiza el retraso en la red [8]. Otro estudio relevante es el presentado en [9], que
explora la aplicación de los algoritmos Deep-Q-Network (DQN) y Deep Deterministic Policy Gradient (DDPG) en un entorno de comunicaciones marítimas. En este caso, se establece una red de
comunicación marítima con UAVs que actúan como estaciones base aéreas. El estudio se centra
en la minimización de la latencia del sistema.
La optimización de la posición de UAVs es otro campo significativo de investigación. En [10],
se utiliza el algoritmo DDPG para controlar múltiples UAVs. Este enfoque busca maximizar la
cobertura de la red, mientras se minimiza el consumo energético. Los UAVs son controlados para
adaptarse dinámicamente a las necesidades de cobertura, manteniendo una alta eficiencia energética. Este trabajo destaca por su capacidad de equilibrar cobertura y consumo energético de manera
efectiva. Es importante enfatizar que este algoritmo mejora la cobertura tomando como dato la
distribución de UEs, y no pretenden aprender y seguir en tiempo real el movimiento de unos UEs
determinados dentro del escenario, como es nuestro caso.
En [11], se aborda la optimización de la trayectoria de un dron en un modelo tridimensional, utilizando DDPG. El objetivo es maximizar el número de UEs cubiertos durante el vuelo. Además de
considerar la eficiencia energética, este estudio introduce la asignación autónoma de frecuencias,
mejorando aún más la eficiencia del sistema. En este caso, el dron tiene conocimiento previo de la
posición de los UEs, a diferencia de nuestro caso, donde dichas posiciones no se conocen y deben
inferirse a través del ángulo de llegada de las señales.
1.3. Establecimiento del Problema
En esta sección se introduce el modelo de sistema y se definen matemáticamente el problema a
resolver en este trabajo.
1.3.1. Modelo del Sistema
Se considera una red celular en la que se comunica tanto en uplink como en downlink de forma
no simultánea, utilizando time division multiple access (TDMA). En esta red, se encuentran UEs
terrestres y drones (UAVs), cuyos conjuntos son U y D, respectivamente. Los drones portan una
estación base (BS, del inglés Base Station), y se comportan como tal.
El conjunto, U , está compuesto por U UEs de manera que:
U = {u1, . . . , uu, . . . , uU }. (1.1)
La posición geográfica del UE, u, se determina por:
ρU
u = {xU
u , yU
u , zU
u }, (1.2)
y la matriz de posiciones de todos los UEs se define de la siguiente manera:
ρU = {ρU
1 , . . . , ρU
u , . . . , ρU
U }. (1.3)
41.3. ESTABLECIMIENTO DEL PROBLEMA
El conjunto, D, está compuesto por D drones de manera que:
D = {d1, . . . , dd, . . . , dD}, (1.4)
donde la posición del dron, d, se determina por:
ρD
d = {xD
d , yD
d , zD
d }, (1.5)
y la matriz de posiciones de todos los UAVs está definida de la siguiente forma:
ρD = {ρD
1 , . . . , ρD
d , . . . , ρD
D}. (1.6)
La red trabaja en una banda de 10 MHz ubicada en 2 GHz. Todos los enlaces radio que forman la
red experimentan ganancias por distancia y lognormal shadowing . Denotamos la ganancia, Gu,d,
como la ganancia entre el UE, u, y el dron, d. En esta ganancia se tienen en cuenta la ganancia de
la antena ( 3.2.1), Ga, la ganancia por distancia ( 3.2.2), Gp, la ganancia outdoor to indoor ( 3.2.4),
Ge, el shadowing ( 3.2.3), Gs, y efectos del multicamino ( 3.2.5), Gff.
Por tanto, la ganancia del enlace se define como:
Gu,d,k = Ga
u,d · Gp
u,d · Ge
u,d · Gs
u,d ·
∣
∣
∣Gff
u,d,k
∣
∣
∣2
, (1.7)
donde k hace referencia al recurso frecuencial utilizado en la comunicación. Cabe destacar que
esta ganancia es dependiente tanto de la posición del dron, ρD
d , como de la posición del UE, ρU
u .
Por lo que se define Gu,d,k(ρD
d , ρU
u ).
A su vez la potencia recibida por el UE, u, proveniente del dron, d, en el recurso frecuencial, k, se
expresa en la siguente ecuación:
P rx
u,d,k = P tx
d,k · Gu,d,k(ρD
d , ρU
u ), (1.8)
donde P tx
d es la potencia transmitida por el dron, d, en el recurso frecuencial, k.
El parámetro que indica la calidad de la señal recibida por el UE, u, del dron, d, en el recurso de
frecuencia, k, se denomina signal to interference plus noise ratio (SINR), γu,d,k, expresado en la
siguiente ecuación:
γu,d,k = P rx
u,d,k
∑D
d′=0
d′̸=d
P rx
u′,d,k + σ2 , (1.9)
donde σ2 es la potencia del ruido.
Una vez definida la SINR, y utilizando el teorema de Shannon-Hartley, la tasa de transmisión de
datos del UE, u, conectado al dron, d, en el recurso de frecuencia, k, se define como:
Ru,d,k = Bk log2(1 + γu,d,k), (1.10)
donde B es el ancho de banda de la señal.
Si se utiliza un scheduler que reparte los recursos disponibles de manera equitativa entre los UEs
de la red, como podría ser el Round-Robin [12], la tasa de transmisión será por tanto la expresada
en la ecuación siguiente.
Ru = B
U log2(1 +  ̄γu,d,k), (1.11)
donde B
U indica la porción del ancho de banda que cada UE utiliza, de media, en la comunicación.
Añadir que se utiliza la SINR media,  ̄γu,d,k, del UE en los recursos de frecuencia.
5CAPÍTULO 1. INTRODUCCIÓN
1.3.2. Objetivo
El objetivo de este trabajo es encontrar, en tiempo real, la posición de los drones que maximice la
tasa de transmisión total justa presentada en la ecuación 1.12, Rfair, que se define como la suma de
los logaritmos de las tasas de transmisión de todos los UEs, y a su vez depende de las posiciones
de los drones, ρD, como de las de los UEs, ρU, como se indicó anteriormente, es decir:
Rfair(ρU, ρD) = ∑
u∈U
log10(Ru,d,k(ρU
u , ρD)). (1.12)
La implementación de una suma de los logaritmos de las tasas de transmisión individuales, Rfair,
es una estrategia que hace el cálculo más justo. Esto se debe a que el logaritmo es una función
cóncava, lo que implica que aumentos adicionales en la tasa de transmisión de un UE con una tasa
ya alta contribuyen menos a Rfair en comparación con aumentos en la tasa de transmisión de un
UE con una tasa baja.
El problema de optimización se formula formalmente entonces como:
maxρD Rfair(ρU, ρD). (1.13)
Este problema se caracteriza por tener una alta dimensionalidad, estocasticidad y no linealidad.
Además, las características temporales del problema, como la necesidad de adaptarse continuamente a las condiciones cambiantes del entorno (por ejemplo, cambios en la posición de los UEs y
en la topología de la red), hacen que los métodos tradicionales de optimización sean inadecuados.
Estos métodos no pueden manejar eficazmente la complejidad y la dinámica del sistema. Por tanto,
hemos optado por utilizar técnicas de RL en esta trabajo, que son capaces de aprender y adaptarse
a través de la interacción con el entorno, para resolver este problema de manera efectiva. El RL
permite encontrar políticas óptimas que maximicen la tasa de transmisión total justa, Rfair(ρU, ρD),
ajustándose dinámicamente a los cambios en las condiciones de la red y proporcionando soluciones
robustas y eficientes.
1.4. Modelado del Entorno
Para abordar y testear con garantías la problemática establecida, se han llevado a cabo los ensayos
en un entorno de simulación capaz de replicar con precisión los eventos más importantes que ocurren en una red. Este entorno es el simulador Giulia, desarrollado en Python, el cual destaca por su
gran modularidad y flexibilidad.
Giulia permite la adaptación específica a nuestro problema y facilita la conexión con diversas
librerías de Python dedicadas al DRL. Esta capacidad es crucial, ya que permite integrar fácilmente
algoritmos avanzados y técnicas de DRL para optimizar la toma de decisiones en la red.
No obstante, antes de poder utilizar Giulia de manera efectiva, fue necesario realizar una optimización del simulador. El DRL es extremadamente sensible al tiempo de simulación, el entrenamiento
de un agente requiere la realización de miles de iteraciones en el entorno simulado para alcanzar
un rendimiento efectivo. En consecuencia la eficiencia en la simulación no solo reduce el tiempo
de cómputo, sino que también mejora la precisión y la estabilidad del modelo entrenado.
61.5. ESTRUCTURA DEL TRABAJO
La optimización del simulador incluyó la mejora de algoritmos internos, la reducción de la complejidad computacional y la implementación de técnicas avanzadas de paralelización. Estos esfuerzos
aseguran que Giulia pueda manejar simulaciones de gran escala y alta fidelidad, permitiendo un
entrenamiento más rápido y preciso de los agentes de DRL.
En resumen, el uso del simulador Giulia optimizado proporciona un entorno robusto y eficiente para
probar y validar nuestras soluciones de DRL, garantizando que se pueda abordar la problemática
de manera efectiva y con resultados confiables.
1.5. Estructura del Trabajo
A continuación, se presenta la estructura de esta tesis, organizada en varios capítulos que detallan
cada aspecto de la investigación realizada.
En el capítulo 2, se presenta una introducción detallada al campo del RL, comenzando con los
conceptos fundamentales y avanzando hasta el estado del arte. Este capítulo proporciona el contexto necesario para comprender la relevancia y aplicación de RL en la resolución de problemas
complejos.
El capítulo 3 se enfoca en la implementación del simulador Giulia. Aquí, se discuten en detalle
las optimizaciones llevadas a cabo para asegurar que el simulador pueda entrenar un agente de
RL de manera eficiente y en tiempos razonables. Este capítulo también cubre los desafíos técnicos
enfrentados y las soluciones implementadas para mejorar el rendimiento del simulador.
En el capítulo 4, se exploran los diferentes algoritmos de RL utilizados en esta investigación. Además, se describen las diversas situaciones y escenarios en los que se realizaron los entrenamientos.
Este capítulo es crucial para entender la metodología empleada y cómo se configuraron los experimentos para evaluar el rendimiento de los agentes de RL.
Los resultados obtenidos de los entrenamientos y simulaciones se presentan en el capítulo 5. Aquí,
se analizan los datos y se discuten las observaciones clave derivadas de los experimentos. Este
análisis proporciona una visión clara del rendimiento de los algoritmos y su eficacia en la resolución
de la problemática planteada.
Finalmente, en el capítulo de 6, se sintetizan los hallazgos más importantes de la investigación,
además de aclarar cual es la futura línea de investigación en el proyecto.
7CAPÍTULO 1. INTRODUCCIÓN
8Capítulo 2
Aprendizaje Reforzado Profundo
2.1. Aprendizaje Reforzado
En esta sección vamos a profundizar en los fundamentos del aprendizaje reforzado, con el objetivo
de poder comprender aquellos algoritmos más complejos como los utilizados en este proyecto.
2.1.1. Proceso de Decisión de Markov
Para adentrarnos en el aprendizaje reforzado es necesario entender lo que es un proceso de decisión
de Markov.
Un proceso de decisión de Markov (MPD, del inglés Markov Decision Process) [13] se define
como un proceso de control estocástico en tiempo discreto. Esto significa que es un proceso que
incorpora elementos de aleatoriedad (estocásticos) y se evalúa en instantes discretos de tiempo.
Además, incluye un agente que puede interactuar con el entorno. En cada instante de tiempo, el
agente observa el estado actual del sistema y selecciona una acción. La elección de esta acción
influye en el próximo estado del sistema, pero debido a la naturaleza estocástica del proceso, esta
transición no es determinista, sino que está descrita por una distribución de probabilidad. Por lo que
partiendo de un mismo estado y tomando la misma acción es posible acabar en diferentes estados.
El objetivo del agente es maximizar una recompensa acumulada a lo largo del tiempo mediante la
toma de decisiones óptimas en cada paso.
Todos los MDP deben respetar la propiedad de Markov presentada en la Ecuación 2.1 la cual establece que el futuro es independiente del pasado una vez conocido el presente. Esto quiere decir que,
una vez conozcamos la situación actual de nuestro entorno, no nos es necesaria información pasada
para poder caracterizar el proceso. Los MDP están definidos por las siguientes características:
El espacio de estados, S y S+. El primer subespacio, S, incluye todos los estados menos los
terminales, el segundo incluye también los terminales.
El espacio de acciones, A.
La probabilidad de transición entre estados, P .
La función de recompensa, r.
9CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
El factor de descuento, que representa la importancia de la recompensa inmediata con respecto a las futuras recompensas, γ.
La trayectoria, τ .
La propiedad de Markov se expresa matemáticamente de la siguiente manera:
P [st+1|st] = P [st+1|s1, s2, . . . , st] (2.1)
Como hemos dicho previamente, esta propiedad indica que la probabilidad de transicionar al estado, st+1, depende únicamente del estado actual, st, no de los pasados, s1, s2, . . . , st−1.
El agente tiene acceso a la información de los estados, de manera total o parcial. A partir de ahora,
para una mejor comprensión, se asumirá que el agente tiene acceso a la información completa de
los estados. Dado el estado actual st ∈ S para cada instante de tiempo t, el agente selecciona una
acción at ∈ A y observa el siguiente estado st+1, el cual se obtiene de acuerdo a la probabilidad
de transición P (st+1|st, at) y recibe una recompensa inmediata rt+1 = r(st, at, st+1).
El objetivo del agente es aprender una política π que le permita seleccionar la mejor acción en
cada estado, maximizando la recompensa esperada acumulada a largo plazo . La política π es una
función que mapea los estados a las acciones, y puede ser determinista o estocástica. La recompensa
esperada se define como:
E
[ ∞∑
t=0
γtr(st, at, st+1) | a ∼ π(· | st), s0
]
(2.2)
Esta ecuación es la dedicada para entornos con horizonte de tiempo infinito, pero también puede
ser que nos encontremos en un problema con horizonte finito, la ecuación para estos casos es:
E
[H−1∑
t=0
γtr(st, at, st+1) | a ∼ π(· | st), s0
]
. (2.3)
2.1.2. Periodicidad y Estados Terminales
En el contexto de los entornos de simulación, podemos diferenciar entre episodios y pasos. Los
pasos se refieren a las interacciones entre el agente y el entorno. Por ejemplo, en un mundo mallado
(comúnmente conocido como gridworld), cada vez que el agente realiza una acción, se considera
un paso. Un ejemplo de gridworld se aprecia en la Figura 2.1.
En este gridworld en específico, el agente comienza en una posición inicial y puede moverse en
cuatro direcciones (arriba, abajo, izquierda, derecha) hasta llegar a un estado terminal. Hay dos
posiciones terminales: [4, 0] y [0, 4]. La posición [4, 0] otorga una recompensa negativa (-1) y la
posición [0, 4] otorga una recompensa positiva (+1). Estos estados terminales finalizan la simulación, lo que significa que una vez el agente alcanza uno de estos estados, el episodio concluye.
Un episodio se define como la secuencia completa de pasos que realiza el agente desde el estado inicial hasta alcanzar uno de los estados terminales. El objetivo del agente en estos episodios
es maximizar la recompensa total acumulada. En el contexto del gridworld específico que hemos
102.1. APRENDIZAJE REFORZADO
Figura 2.1: Estado inicial.
Figura 2.2: Episodio óptimo. Figura 2.3: Episodio subóptimo.
mencionado, el agente debe aprender a llegar al estado terminal que le otorga la recompensa positiva (+1) lo más rápidamente posible, mientras evita el estado terminal que le otorga una recompensa
negativa (-1).
La Figura 2.2 ilustra un episodio óptimo, en el cual el agente navega eficazmente por el gridworld
y logra llegar al estado [0, 4], obteniendo la recompensa positiva (+1). Este episodio demuestra que
el agente ha aprendido una estrategia eficaz para maximizar su recompensa total.
En contraste, la Figura 2.3 muestra un episodio subóptimo. En este caso, el agente termina en el
estado [4, 0], recibiendo una recompensa negativa (-1). Esto indica que el agente no ha seguido una
estrategia efectiva.
Es importante destacar que en algunos entornos de simulación, no hay estados terminales definidos,
11CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
lo que significa que la simulación podría continuar indefinidamente sin concluir.
2.1.3. Procesos de Decisión de Markov Parcialmente Observables
Como hemos mencionado anteriormente, el agente puede tener acceso a la información completa
o parcial de los estados. En el caso de que sea parcialmente nos encontramos con un proceso
de decisión de Markov parcialmente observable (POMDP, del inglés Partially Oservable Markov
Decision Process) [14]. En este caso, el agente no observa toda la información que el entorno
provee, simplemente una parte de ello. En la literatura las observaciones se denotan como Ot ∈
O, y la definición de estas suele confundirse con la definición de los estados. Sin embargo, es
importante tener en cuenta que los estados son todas las variables que definen el entorno, mientras
que las observaciones son las variables que el agente puede observar.
Figura 2.4: Proceso de decisión de Markov (MDP) con observación completa.
Figura 2.5: Proceso de decisión de Markov parcialmente observable (POMDP).
2.1.4. Política
La política, π, es la encargada de seleccionar la acción que toma el agente en el entorno dado un
estado. La política se puede definir como una aplicación π : S → A que asigna a cada estado
s ∈ S una acción a ∈ A. Esta aplicación puede ser:
Determinista : π(s) = a, donde cada estado st se asigna a una acción única at.
Estocástica : π(a | s) = p(a | s), donde p(a | s) representa la probabilidad de seleccionar
la acción at dado el estado s, con p(a | s) ∈ [0, 1].
2.1.5. Probabilidad de Trayectoria
La probabilidad de que un agente, interactuando con un entorno, siga una determinada trayectoria
se puede definir de la siguiente manera:
p(s1, a1, . . . , sT , aT )
︸ ︷︷ ︸
p(τ )
= p(s1)
T∏
t=1
π(at | st)p(st+1 | st, at)
︸ ︷︷ ︸
Cadena de Markov (s,a)
. (2.4)
122.1. APRENDIZAJE REFORZADO
De esta manera, observamos que la probabilidad de que el agente siga la trayectoria τ depende tanto
de la probabilidad de comenzar en el estado inicial , p(s1), como del producto de las probabilidades
de la política, π(at | st), y de las probabilidades de transición, p(st+1 | st, at), a lo largo de todos
los pasos t ∈ T .
2.1.6. Funciones de Valor y Q
El objetivo del agente es encontrar la política, π, que maximice la recompensa acumulada esperada
descontada por:
rt =
∞∑
k=0
γkrt+k+1. (2.5)
El factor de descuento, γ, toma valores en el intervalo [0, 1] y se utiliza para dar mayor importancia
a las recompensas inmediatas en comparación con las futuras [1]. Si el factor de descuento es 0, el
agente se enfoca únicamente en la recompensa inmediata, mientras que si el factor de descuento
es 1, el agente considerará todas las recompensas futuras por igual.
Para simplificar la demostración, en esta sección asumiremos que el factor de descuento está fijado
en 1. No obstante, estas demostraciones podrían realizarse manteniendo el factor de descuento
variable. Por lo tanto, necesitamos buscar una política que maximice:
Eτ ∼p(τ )
[ T∑
t=1
r(st, at)
]
. (2.6)
Esta esperanza se puede descomponer como:
Es1∼p(s1)
[Ea1∼π(a1|s1)
[r(s1, a1) + Es2∼p(s2|s1,a1)
[Ea2∼π(a2|s2) [r(s2, a2) + · · · | s2] | s1, a1
]]] .
(2.7)
De lo que podemos definir:
Q(s1, a1) = r(s1, a1) + Es2∼p(s2|s1,a1)
[Ea2∼π(a2|s2) [r(s2, a2) + . . . | s2] | s1, a1
] . (2.8)
Sustituyendo en la ecuación 2.7:
Eτ ∼p(τ )
[ T∑
t=1
r(st, at)
]
= Es1∼p(s1)
[Ea1∼π(a1|s1) [Q(s1, a1) | s1]] . (2.9)
Por tanto la función Q determina la recompensa acumulada esperada que supondría tomar una
determinada acción en un estado:
Qπ(st, at) = Eπ
[ T∑
t
r(st+1, at+1) | st, at
]
: Recompensa total de tomar at en st. (2.10)
13CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
De igual manera podemos entender otra de las funciones importantes, la función de valor de los
estados. Esta representa la recompensa acumulada que el agente, por estar en ese estado, recibe.
Estos valores dependerán de la actual política, ya que es la que controla la decisión de acciones
como de la probabilidad de transición del propio entorno.
V π(st) = Eπ
[ T∑
t
r(st+1, at+1) | st
]
: Recompensa total de st (2.11)
V π(st) = Eat∼π(at|st) [Qπ(st, at)] = ∑
a∈A
π(st, a)Qπ(st, a). (2.12)
En la siguiente Figura 2.6, se observa un ejemplo para calcular la función de vaor del estado, V π,
en un entorno en el que en el estado st únicamente tenemos tres acciones disponibles a0, a1, a2.
Figura 2.6: Ejemplo cálculo valor V.
Para este ejemplo el cálculo exacto sería:
V π(st) = ∑
a∈A
π(st, a)Qπ(st, a) = 0,2 · 0,5 + 0,4 · 2 + 0,4 · −5 = −1,1.
2.1.6.1. Funciones Q y de Valor con Factor de Descuento
Para hacer una aproximación real a como la mayoría de algoritmos funcionan es necesario tener
en cuenta el factor de descuento.
142.2. MÉTODOS DE APRENDIZAJE REFORZADO
La función Q quedaría de la siguiente manera:
Qπ(s, a) = Eπ {rt | st, at} = Eπ
{ T∑
k=0
γkrt+k+1 | st, at
}
. (2.13)
Y la función de valor de la siguiente manera:
V π(s) = Eπ {rt | st} = Eπ
{ T∑
k=0
γkrt+k+1 | st
}
. (2.14)
2.1.6.2. Funciones Óptimas
Las funciones óptimas se definen como objetivo del aprendizaje reforzado. Son los valores que la
función de valor de estado y Q tendrían si encontrásemos la mejor política posible.
V ∗(s) = maxπV π(s). (2.15)
Q∗(s, a) = maxπQπ(s, a). (2.16)
2.1.7. Ecuaciones de Bellman
La ventaja de las ecuaciones previas es que se pueden escribir de manera recursiva y, por tanto,
recurrir a técnicas de programación dinámica [15] para poder resolver estos entornos y encontrar
la mejor política posible.
Qπ(s, a) = ∑
s′∈S
P (s′ | s, a) [r(s, a, s′) + γV π(s′)] . (2.17)
Partiendo de la ecuación 2.17 y utilizando la definición de 2.12, tenemos:
V π(s) = ∑
a∈A
π(s, a) ∑
s′∈S
P (s′ | s, a) [r(s, a, s′) + γV π(s′)] . (2.18)
De igual manera podemos hacerlo con la función Q. Obteniendo:
Qπ(s, a) = ∑
s′∈S
P (s′ | s, a)
[
r(s, a, s′) + γ ∑
a′∈A
π(s′, a′)Qπ(s′, a′)
]
. (2.19)
2.2. Métodos de Aprendizaje Reforzado
Antes de adentrarnos en los diversos métodos de aprendizaje reforzado, es fundamental establecer
las diferentes clasificaciones que estos métodos pueden tener.
15CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
2.2.1. Clasificación según El Objetivo
Es la clasificación más común entre los diferentes métodos, se pueden identificar tres clases [16]:
Aprendizaje por valor : El objetivo es aprender la función de valor óptima, V ∗(s) o Q∗(s, a).
Esta función de valor permite al agente seleccionar la mejor acción en cada estado al evaluar
las posibles recompensas futuras esperadas.
Aprendizaje por política : El objetivo es aprender la política óptima, π∗(s). A diferencia de los anteriores, en los que se aprende la función de valor aquí directamente tenemos
conocimiento de cual es la mejor acción a decidir.
Actor-crítico : Este método combina los enfoques de aprendizaje por valor y aprendizaje
por política, permitiendo al agente aprender tanto la función de valor como la política óptima
simultáneamente.
2.2.2. Clasificación según El Tipo de Política
Además de los objetivos, también se pueden clasificar los métodos de aprendizaje reforzado según
la información con la que son entrenados:
On-policy : El agente aprende y actualiza la política basándose en la política que está siguiendo actualmente. Este enfoque se centra en mejorar la política en uso mediante la experiencia
adquirida.
Off-policy : El agente aprende la política óptima mientras sigue una política diferente, conocida como política de comportamiento. Este enfoque permite al agente explorar con una
política mientras mejora otra, a menudo utilizando técnicas de aprendizaje más complejas
y exploración extensa. O también puede darse el caso en el que para conseguir aprender la
función valor V (s) se utilicen todas las muestras de las que el agente dispone, dando igual
si se utilizaba otra política.
2.2.3. Clasificación según El Uso de Modelo
Otra forma de clasificar los métodos de aprendizaje reforzado es según el uso de modelos del
entorno:
Métodos basados en modelo : Estos métodos utilizan un modelo explícito del entorno para
tomar decisiones y planificar. El modelo puede predecir las transiciones de estados y las
recompensas futuras, lo que permite al agente simular diferentes escenarios y estrategias
antes de ejecutarlas en el entorno real. De manera que en este algoritmo también se tiene una
cierta estimación de como funciona el entorno sin tener que interactuar con el [17].
Métodos libres de modelo : Estos métodos no utilizan un modelo explícito del entorno. En
lugar de ello, el agente aprende directamente a partir de la experiencia e interactuar con el
entorno, actualizando sus estimaciones de la función de valor o su política basándose en la
retroalimentación recibida por las interacciones directas [18].
162.3. APRENDIZAJE POR VALOR
Por tanto una clasificación de diferentes modelos se puede entender como en la siguiente Figura 2.7. En la cual estudiamos los modelos dependiendo de el tipo de objetivo que siguen, añadiendo
matices sobre la política utilizada.Basados en Valor Basados en Política
Actor-Crítico
Q-Learning
SARSA
Policy Gradients
Policy Iteration
A3C On-policyPPO
TRPO
DDQN
DDPG
Figura 2.7: Clasificación de diferentes modelos.
2.3. Aprendizaje por Valor
Este tipo de técnicas se centran en seleccionar las acciones a tomar basándose en las funciones
V (s) y Q(s, a). De manera que el agente tiene que aprender a aproximar dichas funciones y, posteriormente, decidir cual es la acción que mayor recompensa a largo plazo le va a otorgar.
2.3.1. Q-Learning
El algoritmo Q-Learning [19] se basa en la construcción de una tabla en la que se registran los
resultados de las interacciones del agente con el entorno. La tabla Q almacena los valores de recompensa obtenidos al realizar una acción específica en un estado determinado. Tras múltiples
iteraciones la tabla Q converge hacia una tabla óptima, lo que permite al agente seleccionar las
acciones ideales para maximizar las recompensas acumuladas como se puede ver en la Figura 2.8.
Se caracteriza por ser off-policy y libre de modelo.
En cada iteración, la tabla Q se actualiza mediante:
Q(st, at) ← (1 − α)Q(st, at) + α [rt+1 + γmaxaQ(st+1, a)] , (2.20)
donde tenemos:
α es la tasa de parendizaje.Un parámetro que determina la influencia del nuevo conocimiento
en la actualización de la tabla Q.
rt+1, es la recompensa inmediata, después de realizar la acción actual, at en el estado actual,
st.
γ, es el factor de descuento, el cual determina la importancia de las recompensas futuras.
17CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDOINICIALIZACIÓN TABLA Q
ELEGIR ACCIÓN
REALIZAR LA ACCIÓN
MEDIR RECOMPENSA
ACTUALIZAR LA TABLA...
...
...
...
... ... ... ... ...
Figura 2.8: Algoritmo de aprendizaje Q y tabla Q.
maxaQ(st+1, a), es el valor máximo de la función Q, para el estado siguiente, st+1, evaluado
sobre todas las acciones posibles, a ∈ A.
La intuición detrás de este algoritmo es la siguiente:
1. El agente se encuentra en el estado st.
2. El agente toma la acción at en el estado st.
3. Como resultado de esta acción, el agente recibe una recompensa inmediata rt+1 y transita al
nuevo estado st+1.
4. El valor Q(st, at) se actualiza para reflejar tanto la recompensa inmediata rt+1 como la
recompensa futura estimada.
5. La actualización de Q(st, at) se realiza utilizando la Ecuación 2.20.
6. Esta fórmula de actualización asegura que el valor Q(st, at) se ajusta para reflejar no solo la
recompensa inmediata rt+1, sino también la recompensa futura esperada al seguir la mejor
política posible desde el estado st+1.
7. El proceso se repite para cada paso en el tiempo, ajustando continuamente los valores de Q
para mejorar la política del agente.
Lo que se sintetiza en: se elige una acción, se analiza la recompensa inmediata, y a partir del conocimiento previo se selecciona la acción que va a reportar una mayor recompensa. Una explicación
más extensa se observa en el algoritmo 1.
182.3. APRENDIZAJE POR VALOR
La tabla Q se ajusta iterativamente, combinando recompensas inmediatas y a largo plazo para
aprender la política óptima de una manera implícita.
Algorithm 1 Algoritmo Q-learning
1: Parámetros del algoritmo : tamaño de paso α ∈ (0, 1], 0 < ε < 1
2: Inicializar Q(s, a), para todos s ∈ S+, a ∈ A(s), arbitrariamente excepto que
Q(terminal, ·) = 0
3: repeat
4: Inicializar st
5: repeat
6: Elegir at de st usando una política como ε-greedy
7: Tomar acción at, observar rt, st+1
8: Q(st, at) ← (1 − α)Q(st, at) + α [rt + γmaxaQ(st+1, a)]
9: s ← st+1
10: until s sea terminal
11: until Q converja
La tasa de aprendizaje debe satisfacer las siguientes condiciones:
∞∑
t=0
αt(s, a) = ∞,
∞∑
t=0
α2
t (s, a) < ∞. (2.21)
2.3.1.1. Exploración versus Explotación
En estos algoritmos no siempre es óptimo seleccionar la acción con el mayor valor de Q, ya que,
dependiendo de la inicialización, los valores en la tabla Q pueden ser subóptimos y lejanos del
valor real. Por tanto, se utiliza una técnica llamada ε-greedy [20].
La estrategia ε-greedy consiste en tomar decisiones aleatorias con una pequeña probabilidad ε y
seguir la política óptima con una probabilidad 1 − ε. Es decir, con una probabilidad ε el agente
explora el entorno eligiendo una acción aleatoria. Con una probabilidad 1 − ε, el agente explota su
conocimiento actual seleccionando la acción que maximiza el valor Q.
Matemáticamente, la acción at se selecciona de la siguiente manera:
at =
{
acción aleatoria con probabilidad ε
arg maxaQ(st, a) con probabilidad 1 − ε. (2.22)
La técnica ε-greedy ayuda a equilibrar la exploración y la explotación. La exploración permite
al agente descubrir nuevas acciones que podrían proporcionar mejores recompensas en el futuro,
mientras que la explotación permite al agente utilizar su conocimiento actual para maximizar las
recompensas inmediatas. Básicamente, esta técnica ayuda a descubrir nuevas estrategias que sin
exploración no se podrían obtener. Con el tiempo, el valor de ε puede disminuir gradualmente
para que el agente explore menos y explote más a medida que adquiere más conocimiento sobre el
entorno. Sin embargo, esta técnica solamente se utiliza en la selección de la primera acción. Una vez
seleccionada la primera acción el algoritmo Q-learning elegirá la acción con mayor recompensa.
De esta manera se puede asegurar que estos algoritmos lleguen a converger a una solución más
cercana a la óptima.
19CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
2.3.2. SARSA
El algoritmo SARSA [21], que significa State-Action-Reward-State-Action, sigue la misma intuición que el Q-learning. La diferencia radica en la manera de actualizar la tabla Q. En lugar de
utilizar el valor máximo de Q en el estado siguiente, SARSA vuelve a utilizar la misma política
ε-greedy para seleccionar la siguiente acción. SARSA actualiza la tabla Q utilizando la acción real
tomada en el estado siguiente, en lugar de la mejor acción posible.
Por eso mismo se caracteriza por ser on-policy, ya que la política que se sigue para seleccionar la
siguiente acción es la misma que la que se está actualizando. Por tanto, este algoritmo sigue:
Q(st, at) ← (1 − α)Q(st, at) + α [rt+1 + γQ(st+1, at+1)] . (2.23)
Algorithm 2 Algoritmo SARSA
1: Parámetros del algoritmo : tamaño de paso α ∈ (0, 1], 0 < ε < 1
2: Inicializar Q(s, a), para todos s ∈ S+, a ∈ A(s), arbitrariamente excepto que
Q(terminal, ·) = 0
3: repeat
4: Inicializar st
5: Elegir at de st usando una política como ε-greedy
6: repeat
7: Tomar acción at, observar rt, st
8: Elegir at+1 de st+1 usando una política como ε-greedy
9: Q(st, at) ← (1 − α)Q(st, at) + α[rt + γQ(st+1, at+1)]
10: s ← st+1
11: a ← at+1
12: until st+1 sea terminal
13: until Q converja
2.4. Aprendizaje por Política
Los algoritmos de aprendizaje por política se enfocan en optimizar directamente una política, π,
que puede ser estocástica o determinista. Con el mismo objetivo de maximizar la recompensa
acumulada a largo plazo. A diferencia de los métodos basados en el valor, que primero estiman
una función de valor y luego derivan una política a partir de ella, los métodos de aprendizaje
por política buscan directamente la mejor política sin necesidad de pasar por una función de Q
intermedia. Este enfoque permite manejar de manera más efectiva políticas en espacios de acción
continuos y en problemas donde las políticas deben ser suavemente ajustadas.
2.4.1. Iteración de Política
La iteración de política es uno de los métodos fundamentales en el aprendizaje por política, que
alterna entre dos fases principales: la evaluación de la política actual y la mejora de dicha política.
Este proceso iterativo permite al agente converger hacia una política óptima. Este algoritmo es
similar a los evaluados tabularmente, como el Q-learning o el SARSA.
202.4. APRENDIZAJE POR POLÍTICA
2.4.1.1. Evaluación de la Política
La evaluación de la política implica calcular el valor esperado de la política actual, π. Esto se realiza
mediante un proceso iterativo que ajusta los valores de los estados basándose en las recompensas
y las transiciones esperadas. El algoritmo se observa en 3. El cual se detiene cuando las diferencias
entre las iteraciones sucesivas son menores que un umbral, θ.
Algorithm 3 Evaluación iterativa de la política, para estimar V ≈ V π
1: Entrada : π, la política a evaluar
2: Parámetro del algoritmo : un pequeño umbral θ > 0 que determina la precisión de la estimación
3: Inicializar V (s), para todos s ∈ S+, arbitrariamente excepto que V (terminal) = 0
4: repeat
5: ∆ ← 0
6: for cada s ∈ S do
7: v ← V (s)
8: V (s) ← ∑
a π(a|s) ∑
s′,r p(s′, r|s, a) [r + γV (s′)]
9: ∆ ← max(∆, |v − V (s)|)
10: end for
11: until ∆ < θ
2.4.1.2. Mejora de la Política
Una vez que la política actual ha sido evaluada, el siguiente paso es mejorarla. La mejora se realiza
ajustando su valor para que seleccione acciones que maximicen el valor esperado en cada estado,
utilizando la función de valor V (s) obtenida en la fase de evaluación. Este proceso de alternar entre
evaluación y mejora se repite hasta que la política converge a una política óptima. El algoritmo 4,
muestra como se actualiza está política.
Este proceso asegura que el agente iterativamente mejora su política hasta que alcanza la óptima,
donde ningún cambio adicional puede proporcionar una recompensa mayor.
2.4.2. Política de Gradientes
Estos algoritmos, en comparación a los de iteración de política y los basados en valor, no se centran en ningún tipo de estimación de la función de valor [22]. Computan una política π, a través
de la que interactuar con el entorno. En este caso la política estará determinada por una serie de
parámetros,θ ∈ Rd′
, a través de los cuales aproximaremos nuestra política. De manera que la política ahora quedará como π(a | s, θ) = P r(a | s, θ) para cada acción a ∈ A tomada en el intervalo
de tiempo t, en el estado s ∈ S y distribución de parámetros θ.
Al tener unos parámetros, θ, es necesario optimizarlos y para ello se utilizan técnicas como el
cálculo del gradiente, el cual veremos más adelante en la subsección 2.5.4.3. El objetivo de esto es aumentar la función de rendimiento escalar J(θ). El cual definimos como la recompensa
acumulada esperada. Los pesos se actualizan en base a:
θt+1 = θt + α∇θJ(θ). (2.24)
21CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
Algorithm 4 Iteración de Política (usando evaluación iterativa de la política) para estimar π ≈ π∗
1: Inicialización
2: V (s) ∈ R y π(s) ∈ A(s) arbitrariamente para todos s ∈ S
3:
4: Evaluación de la Política
5: Mejora de la Política
6: policy-stable ← true
7: for cada s ∈ S do
8: old-action ← π(s)
9: π(s) ← arg maxa
∑
s′,r p(s′, r|s, a) [r + γV (s′)]
10: if old-action̸ = π(s) then
11: policy-stable ← false
12: end if
13: end for
14: if policy-stable then
15: detener y retornar V ≈ v∗ y π ≈ π∗
16: else
17: ir a 2
18: end if
Para que estos teoremas se puedan utilizar es necesario que la política, π(a | s, θ), sea diferenciable
con respecto a los parámetros, θ,para todos s ∈ S, a ∈ A y θ ∈ Rd′
. Además, para asegurar
la explotación en este tipo de métodos es necesario que esta nunca se convierta en una política
determinista. Ya que aprovecha la naturaleza de las distribuciones estocásticas haciendo que cada
vez que está en el estado st elija una de las acciones en π(a | s, θ). Sin embargo, si la naturaleza del
problema requiere que la política sea determinista, se pueden elegir acciones mediante ε-greedy,
cuyo parámetro ε puede ir decayendo a través de las iteraciones. De esta manera, si la política
óptima es una determinista, las actualizaciones de las acciones suboptimas serán infinitamente
más pequeñas que las óptimas.
2.4.2.1. El Teorema de La Política de Gradiente
Dentro de este algoritmo, y gracias a las propiedades de la parametrización, las probabilidades
de las acciones cambian de manera suave. Esto contrasta con otras técnicas que pueden producir
cambios bruscos en la probabilidad de las acciones con una mínima variación en la estimación de
los valores V . Esto se debe principalmente a que se pueden utilizar técnicas como el ascenso de
gradiente 2.24. Para realizar modificaciones en la actuación de la política de manera más gradual.
Esta técnica presenta ciertas complicaciones para determinar cuál es la modificación de los parámetros que hace que la recompensa, J(θ), aumente. Además la modificación de los parametros 2.24
impacta directamente en cómo el agente actúa en el entorno y, por tanto, modifica el valor de los
estados. Esto ocurre porque la política está parametrizada por θ, y cualquier cambio en estos parámetros altera la probabilidad de seleccionar ciertas acciones en ciertos estados, lo que a su vez
afecta las transiciones y recompensas en el entorno. Por ejemplo, si en una primera iteración en 2.1
mi agente decide desplazarse hacia abajo, el valor de los estados va a ser negativo ya que no conoce otra solución, y al evaluar la política tendrán valor negativo. Sin embargo, si en la siguiente
222.4. APRENDIZAJE POR POLÍTICA
iteración este agente decide ir hacia la derecha, el valor de los estados será positivo. Lo cual hace
que sea inestable.
Esta interacción compleja entre los parámetros y el entorno puede ser manejada efectivamente
mediante el uso de técnicas de gradiente. En particular, el teorema de la política de gradiente nos
proporciona una forma de calcular el gradiente de la recompensa esperada con respecto a los parámetros de la política. Consiguiendo evitar el problema de las modificaciones repentinas en la
función de valor.
Demostración de Algoritmos de Política de Gradiente
∇θvπθ (s) = ∇θ
[∑
a
πθ(a|s)Qπθ (s, a)
]
, para todo s ∈ S
= ∑
a
[∇θπθ(a|s)Qπθ (s, a) + πθ(a|s)∇θQπθ (s, a)]
= ∑
a

∇θπθ(a|s)Qπθ (s, a) + πθ(a|s)∇θ
∑
s′,r
p(s′, r|s, a)(r + vπθ (s′))


= ∑
a
[
∇θπθ(a|s)Qπθ (s, a) + πθ(a|s) ∑
s′
p(s′|s, a)∇θvπθ (s′)
]
= ∑
a
[
∇θπθ(a|s)Qπθ (s, a) + πθ(a|s) ∑
s′
p(s′|s, a)
∑
a′
[
∇θπθ(a′|s′)Qπθ (s′, a′) + πθ(a′|s′) ∑
s′′
p(s′′|s′, a′)∇θvπθ (s′′)
]]
= ∑
x∈S
∞∑
k=0
Pr(s → x, k, πθ) ∑
a
∇θπθ(a|x)Qπθ (x, a),
∇θJ(θ) = ∇θvπθ (s0)
= ∑
s
( ∞∑
k=0
Pr(s0 → s, k, πθ)
) ∑
a
∇θπθ(a|s)Qπθ (s, a)
= ∑
s
η(s) ∑
a
∇θπθ(a|s)Qπθ (s, a),
= ∑
s′
η(s′)
(∑
s′
η(s′)
∑
s′ η(s′)
) ∑
a
∇θπθ(a|s)Qπθ (s, a)
= ∑
s′
η(s′) ∑
s
μ(s) ∑
a
∇θπθ(a|s)Qπθ (s, a)
∝ ∑
s
μ(s) ∑
a
∇θπθ(a|s)Qπθ (s, a).
Como hemos nombrado antes, al modificar los parámetros de la aproximación cambia la dinámica
en la que se interactúa con el entorno. Esto no supone un problema a la hora de calcular cómo se
23CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
modifica la recompensa que se obtiene, ya que si se modifica la política, πθ, de manera directa se
puede obtener cómo la recompensa es modificada. Pero no sucede lo mismo con la función valor,
no sabemos directamente como un cambio en los parámetros puede afectar a su valor.
Por tanto, en la primera parte de la prueba se intenta llegar a una solución para el gradiente en la
que solo se computa el gradiente con respecto a la política multiplicando al valor Q de cada tupla
estado, acción. Donde Pr(s → x, k, πθ) representa la probabilidad de acabar en el estado x después
de k iteraciones.
La manera de medir la mejor política es la función valor en el estado inicial s0, de forma que
cuanto mayor sea dicho valor mejor será la política πθ que estamos usando. Volviendo al ejemplo
del gridworld, si el valor de mi estado estado inicial es positivo indica que la política utilizada es
mejor que la política en el caso en el que ese valor es negativo.
En la segunda parte de la demostración se pone este cálculo del gradiente en función de η(s) y
μ(s), lo que representan el número esperado de visitas al estado s por episodio y la probabilidad
de estar en el estado s respectivamente. De manera que μ(s) ≥ 0 y ∑
s μ(s) = 1. La relación que
tienen ambas es:
μ(s) = η(s)
∑
s′ η(s′) . (2.25)
Esto se hace a que si es posible estimar en media la probabilidad de que el agente se encuentre
en ese estado, no pasa así con las probabilidades de transición entre estados que requieren un
conocimiento completo del entorno.
2.4.2.2. Algoritmo REINFORCE
Para lograr el objetivo de aprender mediante el gradiente 2.24, utilizamos [23]:
∇J(θ) ∝ ∑
s
μ(s) ∑
a
Qπ(s, a)∇π(a | s, θ)
= Eπ
[∑
a
Qπ(st, a)∇π(a | st, θ)
]
.
(2.26)
El teorema de la política de gradiente establece una relación de proporcionalidad necesaria para que
el algoritmo funcione correctamente. Para el ascenso de gradiente, necesitamos obtener muestras
cuya esperanza sea proporcional al gradiente real del rendimiento en función de los parámetros,
J(θ), de esta manera se puede cumplir 2.4.2.1. La parte derecha del teorema de la política de gradiente es una suma sobre los estados, ponderada por la frecuencia con la que ocurren los estados
bajo la política π. Sin embargo, para hacer posible utilizar esta ecuación sin necesidad de conocer
parámetros como la probabilidad de estar en un estado, μ(s), se utilizan las propiedades del muestreo. La cual indica que si se sigue la política π, en media, las interacciones con el entorno seguirán
aquella distribución que cumple el valor real de μ(s). Por tanto, en lugar de utilizar un sumatorio
sobre todos los estados, interactuamos con el entorno y calculamos la esperanza de esto.
De la misma manera que en 2.26 con st lo hacemos para at, siguiendo la política π, las acciones que
se muestreen van a seguir la misma distribución que si hacemos media sobre todos las acciones.
242.4. APRENDIZAJE POR POLÍTICA
∇J(θ) = Eπ
[∑
a
π(a | st, θ)Qπ(st, a) ∇π(a | st, θ)
π(a | st, θ)
]
= Eπ
[
Qπ(st, at) ∇π(at | st, θ)
π(at | st, θ)
]
= Eπ
[
Rt
∇π(at | st, θ)
π(at | st, θ)
]
.
(2.27)
Además, por definición, tenemos que Qπ(st, at) = Eπ [rt | st, at].
Por tanto, la expresión final de 2.24, termina con la expresión que se encuentra dentro de los paréntesis.
θt+1 = θt + αRt
∇π(at | st, θ)
π(at | st, θ) . (2.28)
Esta expresión es intuitiva por dos razones principales. Primero, hace que el parámetro se mueva
principalmente en las direcciones que favorecen las acciones que generan la mayor recompensa
total esperada, al estar multiplicando el gradiente por estas. Segundo, la ponderación inversa a la
probabilidad de la acción asegura que las acciones seleccionadas con mayor frecuencia no tengan una ventaja desproporcionada, evitando que dominen la política incluso si no proporcionan la
mayor recompensa.
En la implementación del algoritmo 2.4.2.2 se usa la siguiente transformación ∇ ln(x) = ∇x
x . De
manera que la ecuación 2.28 se puede reescribir de la siguiente forma:
θt+1 = θt + αRt∇ ln π(at | st, θ). (2.29)
Algorithm 5 REINFORCE:
1: Entrada: una parametrización diferenciable de la política π(a | s, θ)
2: Parámetro del algoritmo: tamaño de paso α > 0
3: Inicializar el parámetro de la política θ ∈ Rd (por ejemplo, a 0)
4: Bucle infinito (para cada episodio):
5: Generar un episodio s0, a0, r1, . . . , sT −1, aT −1, rT , siguiendo π(· | ·, θ)
6: for cada paso del episodio t = 0, 1, . . . , T − 1 do
7: R ← ∑T
k=t+1 γk−t−1rk
8: θ ← θ + αγtR∇ ln π(at | st, θ)
9: end for
2.4.3. El Problema de La Expansión Exponencial del Espacio de Estados
Consideremos un ejemplo sencillo de un gridworld. Si definimos un entorno de tamaño 4x4, tenemos un total de 16 estados. Con 4 acciones posibles en cada estado, el número total de valores
para la función Q sería 16 × 4 = 64. Sin embargo, al incrementar el tamaño del gridworld a 8x8,
el número de estados aumenta a 64, y el número total de valores para la función Q se incrementa
a 64 × 4 = 256.
25CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
A medida que trabajamos en entornos más complejos, como imágenes o videojuegos, el número
de estados posibles crece exponencialmente. Por ejemplo, en el caso de una imagen de 100x100
píxeles en un entorno binario, el número de posibles configuraciones de estados sería 210000, un
número extremadamente grande y difícil de manejar.
Este crecimiento exponencial del espacio de estados, conocido como la explosión del espacio de
estados, representa un desafío significativo en el aprendizaje por refuerzo. Manejar y aprender en
un espacio de estados tan vasto requiere técnicas avanzadas para la generalización y la eficiencia
computacional.
Por lo tanto, se utilizan técnicas que puedan reducir la dimensionalidad de estados tan grande e
implementarlas dentro de los algoritmos, como las redes neuronales artificiales. Estas técnicas permiten representar de manera compacta y eficiente el espacio de estados, facilitando el aprendizaje
y la toma de decisiones en entornos complejos.
2.5. Redes Neuronales Artificiales
Las redes neuronales artificiales son un tipo de modelo de aprendizaje automático inspirado en la
estructura y funcionamiento del cerebro humano. Están compuestas por múltiples capas de neuronas interconectadas, que procesan y transforman la información a medida que se propaga a través
de la red. Las redes neuronales son capaces de aprender patrones y relaciones complejos en los
datos, lo que las convierte en una herramienta poderosa para la clasificación, la regresión y la toma
de decisiones en entornos complejos.
2.5.1. Perceptrón
El perceptrón es la unidad básica de una red neuronal, introducida en [24], que simula el comportamiento de una neurona biológica. En la Figura 2.9, observamos que el perceptrón consiste en un
conjunto de entradas representadas como un vector x = [x1, x2, . . . , xN ] que se multiplica por un
vector de pesos w = [w1, w2, . . . , wN ]. El producto interno de estos vectores, más un sesgo b, se
pasa a través de una función de activación φ(·), la cual determina la salida del perceptrón.
El modelo matemático de este perceptrón se define como:
y = φ (⟨w, x⟩ + b) . (2.30)
2.5.2. Funciones de Activación
Dentro de las redes neuronales, las funciones de activación juegan un papel crucial al introducir no
linealidades en el modelo [25]. Esto permite a las redes neuronales aprender y modelar relaciones
complejas. Una característica importante de muchas funciones de activación es su diferenciabilidad, que facilita la aplicación de métodos de optimización como el descenso de gradiente 2.51.
Función Escalón: La función escalón es una función de activación simple que produce una
salida binaria. Si la entrada es mayor que un umbral, la salida es 1; de lo contrario, la salida
262.5. REDES NEURONALES ARTIFICIALES.
.
.
Figura 2.9: Esquema de un perceptrón.
es 0. La función escalón es una función no diferenciable, lo cual arrastra consigo una serie
de problemas en la propagación del gradiente.
f (x) =
{
1 si x ≥ 0
0 si x < 0.
Función Sigmoide: La función sigmoide es una función de activación suave que produce
una salida en el rango de 0 a 1. La función sigmoide es diferenciable, lo que la hace útil para
el entrenamiento de redes neuronales utilizando métodos basados en gradientes.
f (x) = 1
1 + e−x .
Función ReLU: La función ReLU (Rectified Linear Unit) es una función de activación no
lineal que produce una salida de 0, si la entrada es negativa, y la entrada misma, si esta
es positiva. Es diferenciable en casi todo su dominio menos en 0, lo que facilita el uso de
métodos de optimización basados en gradientes.
f (x) =
{
x si x ≥ 0
0 si x < 0.
2.5.3. Perceptrón Multicapa
Cuando nos enfrentamos a problemas complejos en los que es necesario aprender a resolver tareas de clasificación y regresión no lineales, el perceptrón simple no es suficiente. Añadir varias
capas de perceptrones, junto con la posibilidad de seleccionar distintos tipos de funciones de activación, permite al modelo aprender relaciones más complejas [26]. En la Figura 2.10, se muestra
un esquema de un perceptrón multicapa.
27CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDOSalida
2ª Capa
Oculta
1ª Capa
Oculta
Entradas
Figura 2.10: Esquema de un perceptrón multicapa.
Dentro de este perceptrón multicapa se pueden observar tres tipos de capas:
Capa de entrada: Esta capa recibe el vector de entrada, x, y lo propaga a través de la red.
Capas ocultas: Estas capas intermedias procesan la información y aprenden representaciones complejas de los datos. Reciben la salida de las capas anteriores denominada como activación, a. Gracias a las características no lineales de la función de activación, la red puede
procesar relaciones más abstractas de los datos.
Capa de salida: Esta capa produce la salida final de la red, ˆy, que puede ser una clasificación,
una regresión, o cualquier otro tipo de tarea.
En la Figura 2.10, se observa el esquema de un perceptrón multicapa completamente conectado,
pero también puede darse el caso que no todas las neuronas estén conectadas entre sí, o que haya
conexiones entre neuronas de capas no consecutivas.
2.5.4. Proceso de Entrenamiento
Como se mencionó previamente, las redes neuronales han demostrado ser un aproximador de funciones excepcional, capaz de adaptarse a cualquier tipo de dato y aprender relaciones complejas
entre ellos, ya sean lineales o no. Ejemplos claros de su funcionamiento son los grandes modelos
de lenguaje, o Large Language Models, los cuales pueden aprender las relaciones entre palabras
y llegar a imitar el comportamiento humano en cuanto a la generación de lenguaje [27]. Otros
ejemplos, más relacionados con el campo del aprendizaje reforzado, son modelos como AlphaGo,
desarrollado por Google, que logró vencer al campeón mundial del famoso juego asiático Go [28].
282.5. REDES NEURONALES ARTIFICIALES
Para que las redes neuronales alcancen este nivel de desempeño excepcional, es necesario llevar a
cabo un proceso de entrenamiento previo, el cual consta de tres etapas principales.
2.5.4.1. Feedforward
El proceso de propagación hacia adelante, o feedforward, se puede comprender como la aplicación
lineal de la Ecuación 2.30 en cada capa de la red. La salida de una capa se convierte en la entrada
de la siguiente, y así sucesivamente hasta llegar a la capa de salida. Este proceso se puede expresar
matemáticamente de la siguiente manera:
a(l) = φ
(
W(l)a(l−1) + b(l))
, (2.31)
donde a(l) es la salida (o activación) de la capa l, W(l) es la matriz de pesos de la capa l, b(l) es el
vector de sesgos de la capa l, y φ(·) es la función de activación de la capa l.
Cada columna de la matriz de pesos, W(l), corresponde con las conexiones que tiene con la siguiente capa, y cada fila, con el numero de neuronas de la capa anterior. En el ejemplo de la figura 2.10,
tendríamos tres matrices de pesos, la que conecta la capa de entrada con la primera capa oculta, la
que conecta la primera capa oculta con la segunda capa oculta y la que conecta la segunda capa
oculta con la capa de salida. Además, a cada capa ha de añadirsele un vector de sesgos, b. Estas
tendrían la siguiente forma:
W(1) =



w(1)
11 w(1)
12 w(1)
13 w(1)
14
w(1)
21 w(1)
22 w(1)
23 w(1)
24
w(1)
31 w(1)
32 w(1)
33 w(1)
34


 , b(1) =





b(1)
1
b(1)
2
b(1)
3
b(1)
4




 , (2.32)
W(2) =





w(2)
11 w(2)
12 w(2)
13
w(2)
21 w(2)
22 w(2)
23
w(2)
31 w(2)
32 w(2)
33
w(2)
41 w(2)
42 w(2)
43




 , b(2) =



b(2)
1
b(2)
2
b(2)
3


 , (2.33)
W(3) =



w(3)
11
w(3)
21
w(3)
31


 , b(3) =
(
b(3)
1
)
. (2.34)
De manera que un completo feedforward se puede representar por las operaciones en la Ecuacion 2.35, aprovechando la propiedad de la composición de funciones:
^y = φ(3) (
W(3)φ(2) (
W(2)φ(1) (
W(1)x + b(1))
+ b(2))
+ b(3))
. (2.35)
2.5.4.2. Cálculo de Pérdida
Una vez se ha propagado la entrada por toda la red, y usando un ejemplo de clasificación o regresión, debemos comparar la salida de la red con el valor real. La manera de hacerlo es mediante
29CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
una función de pérdida, que mide la diferencia entre la salida predicha, ˆy, y el valor real, y. La
elección de la función de pérdida depende del tipo de problema que se esté abordando. Algunas de
las funciones de pérdida más comunes son:
Error Cuadrático Medio (MSE): La función de pérdida MSE es una medida común para
problemas de regresión. Calcula el promedio de las diferencias al cuadrado entre las predicciones y los valores reales.
MSE = 1
n
n∑
i=1
(yi − ˆyi)2, (2.36)
donde n es el número de muestras, yi es el valor real de la muestra i y ˆyi es el valor predicho
por la red en la muestra i. Esta función de pérdida penaliza fuertemente los errores grandes,
haciendo que el modelo se ajuste para reducir estos errores.
Error Absoluto Medio (MAE): Utilizada para problemas de regresión, la función MAE
calcula el promedio de las diferencias absolutas entre las predicciones y los valores reales.
MAE = 1
n
n∑
i=1
|yi − ˆyi|. (2.37)
La MAE es más robusta a valores atípicos en comparación con el MSE, ya que no penaliza
los errores grandes de manera tan severa.
Entropía Cruzada: La entropía cruzada es una función de pérdida común para problemas
de clasificación. Mide la discrepancia entre la distribución de probabilidad predicha por el
modelo y la distribución real de las etiquetas.
CE = − ∑
i
yi log(ˆyi), (2.38)
donde yi es la etiqueta verdadera (generalmente un valor binario 0 o 1) y ˆyi es la probabilidad
predicha por la red de que la etiqueta sea 1. La entropía cruzada penaliza más las predicciones
incorrectas que están seguras (valores de probabilidad predicha cercanos a 0 o 1) que las
predicciones incorrectas que están inseguras (valores de probabilidad predicha cercanos a
0.5).
2.5.4.3. Cálculo de Gradiente y Backpropagation
Una vez calculado el error que la red ha cometido, es necesario actualizar los parámetros para
minimizar dicho error. Este proceso se realiza mediante el algoritmo de retropropagación, o backpropagation [26]. Está busqueda de parámetros se representa como:
θ∗ = argminθL(θ), (2.39)
donde θ representa los parámetros de la red y L(θ) es la función de pérdida. El objetivo es encontrar
los valores de θ que minimicen L(θ).
Antes de actualizar los pesos para reducir esta función de pérdidas, primero debemos entender
cómo cada parámetro (tanto pesos, w, como sesgos, b) afecta a la función de pérdida. Para ello, se
calcula el gradiente de la función de pérdida con respecto a los parámetros de la red.
302.5. REDES NEURONALES ARTIFICIALES
Gradiente
El gradiente es una herramienta fundamental en cálculo, ya que nos indica la dirección y la magnitud del cambio más pronunciado de una función en un punto dado. Por ejemplo, para una función
sencilla como f (x) = x2, el gradiente en un punto x es simplemente 2x.
En el caso de funciones más complejas, como las que encontramos en redes neuronales, necesitamos calcular la derivada parcial de la función para cada variable y punto de la red.
De manera matemática, el gradiente se define como:
∇f (x) =
( ∂f (x)
∂x1
, . . . , ∂f (x)
∂xn
)
(2.40)
De manera que cada componente del vector del gradiente nos indica la pendiente de la función en
el mismo punto.
Sin embargo, cuando tenemos funciones muy complicadas, como es el caso de una red neuronal,
es necesario el mecanismo de la regla de la cadena para calcular el gradiente.
Regla de La Cadena
La regla de la cadena es una herramienta fundamental en el cálculo diferencial que se utiliza para
encontrar la derivada de una función compuesta. Si tenemos una función compuesta, es decir, una
función que resulta de aplicar una función a otra, la regla de la cadena nos dice que la derivada de
la función compuesta es el producto de las derivadas de las funciones individuales.
Por ejemplo, si tenemos una función y que depende de una variable u, y a su vez u depende de
otra variable x, la regla de la cadena nos dice que la derivada de y con respecto a x es el producto
de la derivada de y con respecto a u y la derivada de u con respecto a x. Matemáticamente, esto se
expresa como:
dy
dx = dy
du · du
dx (2.41)
Esto significa que para encontrar la tasa de cambio de y con respecto a x, primero encontramos
cómo cambia y con respecto a u y luego cómo cambia u con respecto a x, y multiplicamos esos
dos resultados.
En la Figura 2.11, vemos el grafo de computación para la función L = g(a, b, c, f ) = (a·b+c)f , el
cual vamos a utilizar como un ejemplo sencillo para entender como funciona la regla de la cadena
en el backpropagation (El ejemplo no hace referencia directa a las operaciones utilizadas en una
red neuronal).
Para saber cuanto influye cada hoja del grafo, o entrada de la función, en la salida final es necesario
calcular el gradiente. El cual, en este caso, se puede calcular de manera bastante sencilla por la
naturaleza de la función, como se enseña a continuación:
L = (a · b + c) · f (2.42)
31CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
Figura 2.11: Grafo función g.
∂L
∂a = f · b = 6. (2.43)
Este gradiente nos indica que si modificamos el valor de a en una unidad, el resultado final de la
función cambiará en 6, lo cual tiene lógica ya que la variable a está multiplicada primeramente por
−3 y luego por −2.
Sin embargo, este proceso no siempre es tan sencillo de calcular, ya que en las redes neuronales el
número de parámetros puede ascender al millón, por tanto la manera de hacerlo es usando la regla de
la cadena. Y gracias a la visualización mediante grafos se hace mucho más sencillo comprenderla.
Comenzando en la salida L, su derivada parcial consigo misma es:
∂L
∂L = 1. (2.44)
A continuación vamos hacia atrás (de ahí backpropagation) y calculamos la derivada parcial de L
con respecto a f y d, que es:
∂L
∂d = ∂(d · f )
∂d = f, (2.45)
∂L
∂f = ∂(d · f )
∂f = d. (2.46)
De esto se concluye que, si modificamos el parámetro f en una unidad, el resultado final de la
función cambiará en proporción a d, y viceversa. Esto revela una propiedad importante: cuando dos
variables se combinan en un nodo de multiplicación, la derivada parcial de la salida con respecto
a cada una de las variables es el producto del valor de la otra variable y el gradiente proveniente
del nodo anterior.
Ahora continuamos hacia atrás, y calculamos la derivada parcial de L con respecto de e y c, que
son:
∂L
∂e = ∂L
∂d · ∂d
∂e = f ·
( ∂(e + c)
∂e
)
= f, (2.47)
322.5. REDES NEURONALES ARTIFICIALES
∂L
∂c = ∂L
∂d · ∂d
∂c = f ·
( ∂(e + c)
∂c
)
= f. (2.48)
Si dos variables se juntan en un nodo de suma, la derivada parcial de la salida con respecto a
las variables es uno, y por tanto, el gradiente de cada variable es el gradiente que viene del nodo
anterior, en este caso f = −2.
Seguimos haciendo estos cálculos y al final obtenemos el grafo presentado en la Figura 2.12.
Figura 2.12: Grafo computación con gradientes.
Si nos fijamos en las hojas de nuestro grafo, que coinciden con las variables de la función, tendremos una idea de lo que cada una de ellas afecta al resultado.
Este es un ejemplo sencillo que explica la intuición detrás de la backpropagation que se utiliza en
las redes neuronales. Sin embargo, recordar que grafo de computación de dichas redes suele muchísimo más complejo y su backpropagation resulta inabarcable para realizarlo de manera manual,
teniendo en cuenta que pueden incluir funciones no lineales en las neuronas, lo cual incrementa la
complejidad de las derivadas, ya que nos alejamos de simples multiplicaciones y sumas.
Backpropagation en Redes Neuronales
El gradiente de la función de pérdida de una red neuronal se puede expresar de la siguiente forma:
∇L =
[ ∂L
∂W (1)
∂L
∂b(1) · · · ∂L
∂W (L)
∂L
∂b(L)
]T
. (2.49)
Utilizando la regla de la cadena, Ecuación 2.31, obtenemos las siguientes expresiones para los
gradientes con respecto a los pesos y los sesgos de la última capa:
∂L
∂W (L) = ∂L
∂a(L) · ∂a(L)
∂z(L) · ∂z(L)
∂W (L)
∂L
∂b(L) = ∂L
∂a(L) · ∂a(L)
∂z(L) · ∂z(L)
∂b(L) ,
(2.50)
donde:
33CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
a(L) es la salida de la última capa de la red.
z(L) es la entrada a la última capa de la red (pre-activación).
L es la función de pérdida.
Estas ecuaciones descomponen el gradiente en tres componentes: el cambio de la función de pérdida con respecto a la salida de la última capa, el cambio de la salida de la última capa con respecto
a la entrada de la última capa, y el cambio de la entrada de la última capa con respecto al peso o
sesgo correspondiente.
Este calculo se va haciendo de manera iterativa y retropropagando por la red hasta conseguir calcular los gradientes de todos los parámetros de nuestra red.
Actualización de Los Pesos
Una vez conocemos los gradientes de los parámetros, lo siguiente a realizar es modificar estos
parámetros de manera que el error total que ha cometido con respecto a los datos de entrenamiento
se reduzca. Para ello, se utiliza un algoritmo de optimización, como el descenso de gradiente, que
actualiza los pesos en la dirección opuesta al gradiente, como se ve en la siguiente ecuación:
θ ← θ − α∇L. (2.51)
La razón por la que se actualizan los pesos en el descenso del gradiente es para minimizar la función
de pérdida. El gradiente indica la dirección en la que debemos movernos para aumentar la función
de pérdida, por lo que restar el gradiente a los pesos nos asegura que nos movemos en la dirección
correcta para alcanzar este objetivo.
Pongamos un ejemplo, si el gradiente de un peso es positivo, significa que la función de pérdida
aumentará si aumentamos el peso. Por lo tanto, para reducir la función de pérdida, debemos disminuir el peso, lo que se logra restando el gradiente al peso. Si el gradiente es negativo, significa
que la función de pérdida aumentará si disminuimos el peso, por lo que debemos aumentar el peso
para reducir la función de pérdida.
El parámetro α se conoce como la tasa de aprendizaje y controla la magnitud de la actualización
de los pesos. Un valor pequeño de α puede hacer que el entrenamiento sea más lento, pero puede
ayudar a evitar oscilaciones y divergencias. Por otro lado, un valor grande de α puede hacer que
el entrenamiento sea más rápido, pero puede hacer que el modelo sea inestable y diverja. También
existe el riesgo de que el modelo no converja a un mínimo global, sino a un mínimo local, por tanto
es importante encontrar un equilibrio en el valor de α.
2.6. Estado del Arte Aprendizaje Reforzado Profundo
En los últimos años, ha habido una tendencia creciente a incorporar técnicas de aprendizaje profundo dentro de las ya existentes de aprendizaje reforzado. Esto se debe principalmente al gran
número de estados de los problemas a tratar y a la gran capacidad de las redes neuronales para
representar funciones complejas y generalizar datos.
342.6. ESTADO DEL ARTE APRENDIZAJE REFORZADO PROFUNDO
Uno de los estudios más importantes que allanaron el camino para este nuevo campo fue el presentado en [29]. En esta investigación, se introdujeron redes neuronales convolucionales (CNNs)
para aproximar la función de valor de las tuplas estado-acción, Q. El artículo muestra cómo el
agente, equipado con capacidad de visión gracias a las propiedades de las CNNs, y usando dicha
CNN, es capaz de superar la destreza humana en la mayoría de los videojuegos de Atari 2600. Esto
representaba un desafío significativo debido al problema de la explosión de estados presentado en
la sección 2.4.3.
En [30], además de utilizar la red convolucional del estudio original de DQN, se introduce otra red
neuronal con el objetivo de reducir la sobreestimación de las recompensas que realiza el algoritmo
inicial. Esta segunda red se emplea en la etapa de toma de decisiones, eligiendo la acción con la
mayor recompensa a largo plazo. Sin embargo, debemos de tener en cuneta que no se toma el valor
directo que esta red proporciona. En su lugar, se estima la recompensa del estado elegido utilizando
la red neuronal original.
Otra mejora significativa fue el desarrollo de las Dueling-DQN [31]. En esta Dueling-DQN, se
implementa un nuevo término, la ventaja. Este término, se implementa dentro de la función valor
de las tuplas estado-acción, Q. De manera que tenemos Q(s, a) = V (s) + A(s, a). Por tanto
se entiende la ventaja como la recompensa esperada a largo plazo que te aporta tomar la acción a
parte del valor base de estar en el estado, V (s). Esta técnica ha demostrado converger hacia mejores
políticas gracias a esta descomposición. La idea es que no es necesario tomar acciones en todos
los estados; a veces, no realizar ninguna es la mejor opción, y el término de la ventaja ayudaba a
identificar estos casos.
La manera en la que los algoritmos similares a la DQN eran entrenados, era con un muestreo
uniforme de las interacciones con el entorno. Lo que se estaba haciendo era crear una base de
datos de las interacciones con ese entorno, guardando la recompensa que estas tenían, y entrenar la
red neuronal que aproxima la función de valor de las tuplas estado-acción, Q(s, a), con esa base de
datos. Pero la manera en la que se elegían las muestras para entrenar era un muestreo uniforme. Sin
embargo, en [32], se demuestra como un muestreo en función de la frecuencia en la que suceden
las muestras hace que los algoritmos converjan a mejores aproximaciones.
Otra innovación importante en el campo de las DQN aborda el problema del espacio de acciones
discreto. En Deep Deterministic Policy Gradient (DDPG) [33], se propone una versión de DQN con
un enfoque actor-crítico, donde el actor puede tomar acciones en un espacio continuo, y el crítico
evalúa el valor de cada tupla estado-acción. Esta aproximación permite a los agentes aprender
tareas complejas como el balanceo del cartpole swing-up y la conducción autónoma. En un enfoque
similar, el Asynchronous Advantage Actor-Critic (A3C) [34] logra reducir el costo computacional
utilizando múltiples agentes entrenando en paralelo.
En los últimos años, los trabajos de Trust Region Policy Optimization (TRPO) [35] y Proximal
Policy Optimization (PPO) [36] representaron un cambio de paradigma en el campo del DRL. En
estos algoritmos, en lugar de implementar una red neuronal que aproxime la función del valor de
los estados, V (s), o de las tuplas estado-acción, Q(s, a), la red neuronal es usada para aproximar
la política, π, teniendo como base el algoritmo de los Policy Gradients 2.4.2 llamado REINFORCE 2.4.2.2.
En TRPO, se introduce un nuevo tipo de pérdida, explicada en [35], pero una idea básica es modificar la pérdida de la ecuación 2.26, de manera que se consiga una mayor estabilidad, para ello se
implementa una técnica que mide las distancias entre las diferentes distribuciones de probabilidad
35CAPÍTULO 2. APRENDIZAJE REFORZADO PROFUNDO
de la política anterior y la nueva. Este enfoque demostró un rendimiento robusto tanto en espacios
de acciones continuos como discretos, siendo efectivo en tareas como juegos de Atari y control
continuo en robótica simulada (natación, salto, etc. ). A pesar de ser un método on-policy que puede sufrir colapsos de rendimiento durante el entrenamiento, TRPO garantiza adaptarse rápido a las
nuevas tareas con un ajuste mínimo de hiperparámetros.
PPO, por otro lado, mejora el cálculo de las restricciones de la función de pérdida. En TRPO, para
calcular que la distancia de las distribuciones entre las diferentes políticas es necesario aproximaciones de segundo orden, lo cual es bastante costoso, mientras que en PPO se toma una aproximación de primer orden. PPO ha demostrado un rendimiento superior en tareas de locomoción robótica simulada y juegos de Atari, ofreciendo un equilibrio favorable entre complejidad de muestra,
simplicidad y tiempo de entrenamiento.
No solo existen demostraciones de como estos algoritmos funcionan bien en tareas como juegos
Atari o de control básicas como cart pole. Sino que también se demuestran una destreza increíble
en juegos como el Go.
En resumen, la combinación de técnicas de aprendizaje profundo y refuerzo ha revolucionado el
campo del aprendizaje automático, permitiendo a los agentes superar retos complejos en diversos
dominios. Los avances en DQN y sus variantes, como Double DQN y Dueling DQN, han mejorado significativamente la estabilidad y precisión en la toma de decisiones. Innovaciones como
el muestreo prioritario, DDPG y A3C han ampliado las capacidades de los algoritmos a espacios
de acción continuos y escenarios de alta demanda computacional. Por último, enfoques recientes
como TRPO y PPO han optimizado la eficiencia y robustez del entrenamiento, consolidando su
aplicación en robótica y juegos de alta complejidad.
36Capítulo 3
Giulia
3.1. Naturaleza del Simulador
Para desarrollar un agente de aprendizaje reforzado capaz de ubicar dinámicamente el UAV, es
fundamental contar con un entorno capaz de simular las redes de comunicación. Con este propósito,
se ha utilizado un simulador denominado Giulia, desarrollado en Python, que simula con precisión
la complejidad de los entornos reales.
Existen diferentes tipos de simuladores: a nivel de enlace y a nivel de sistema. Los simuladores a
nivel de enlace se enfocan en la conexión punto a punto, proporcionando un análisis detallado, bit
a bit, de las pérdidas por efectos de canal, como la propagación, la interferencia y el multicamino.
Por otro lado, los simuladores a nivel de sistema se centran en la simulación de la red en su totalidad, con la incorporación de múltiples UEs y estaciones base. Estos simuladores utilizan modelos
estandarizados que representan las características de la red, incluyendo modelos de despliegue de
red y de UEs, operación de las distintas capas de protocolos y el canal, que capturan los efectos
del slow y el fast fading.
La simulación a nivel de sistema es crucial en la evaluación del rendimiento de redes celulares
complejas. Este enfoque permite modelar los elementos y operaciones de una red celular a través de software, proporcionando una solución más precisa y económica en comparación con las
implementaciones reales.
El modelado de la ganancia de antena y la pérdida por distancia es fundamental para evaluar el
rendimiento de la red. Las ganancias de antena, medidas en decibelios isotrópicos (dBi), y la pérdida por distancia, modelada mediante enfoques deterministas y estadísticos como los modelos
de Okumura-Hata y del 3rd Generation Partnership Project (3GPP), son elementos clave en esta
evaluación. Además, el shadow fading y el desvanecimiento multi-trayectoria afectan significativamente la calidad de la señal recibida. Estos fenómenos se modelan como variables log-normales
y mediante Power Delay Profiles (PDP), respectivamente.
Para evaluar la calidad y la fuerza de la señal recibida, se emplean modelos de SINR. Para evaluar la
calidad de servicio, se utilizan modelos de tasa de transmisión, basados en el teorema de ShannonHartleyAsimismo, los modelos de tráfico de UE y los modelos de movilidad, ayudan a evaluar el rendimiento de la red bajo diferentes condiciones de demanda y movimiento de los UEs.
37CAPÍTULO 3. GIULIA
En resumen, el simulador Giulia incorpora una amplia variedad de modelos para representar de
manera precisa las condiciones de operación de una red celular, lo cual es esencial para el desarrollo
y la optimización de tecnologías de redes celulares avanzadas.
3.2. Cálculo de La Tasa de Transmisión
Para poder calcular la tasa de transmisión de la que dispone cada UE, es necesario previamente calcular todas las ganancias relacionadas con el canal entre el UE y la estación base. En primer lugar,
se calcula la ganancia de antena, Ga, que depende de la orientación de la antena. A continuación, se
calcula la pérdida por distancia, Gp, que depende de la distancia entre el UE y la estación base. La
ganancia de penetración outdoor to indoor, Ge, es debida a los obstáculos que tiene que atravesar
la señal. Además, se tienen en cuenta el shadow fading, Gs, y el desvanecimiento multi-camino
Gff, que afectan a la calidad de la señal recibida y dependen en gran medida de la presencia de
obstáculos en el entorno. Una vez calculadas todas estas ganancias, se puede determinar la tasa de
transmisión de cada UE en función de la calidad de la señal recibida.
3.2.1. Ganancia de Antena
La antena típica en un entorno de simulación esta conformada por un array vertical de antenas. El
cual tiene los siguientes diagramas de radiación vertical 3.2 y horizontal 3.1 respectivamente:
Ga
H(φ)[dB] = − mín
[
12
( φ
φ3dB
)2
, κ
]
, (3.1)
y
Ga
V(θ)[dB] = 20 log10
∣
∣
∣
∣
sin (K (θ − θetilt))
K (θ − θetilt)
∣
∣
∣
∣ , (3.2)
donde:
φ y θ son los ángulos de llegada, medidos en grados, en los planos horizontal y vertical
respecto a la dirección del haz principal.
Ga
H(φ) y Ga
V(θ) son las atenuaciones, medidas en dB, introducidas por los patrones de antena
horizontal y vertical con respecto a la ganancia máxima de la antena en la dirección de φ y
θ, respectivamente.
φ3dB y θ3dB son los anchos del haz principal, medidos en grados, de los patrones de antena
horizontal y vertical dentro de los cuales se transmite la mitad de la potencia máxima.
κ es la relación delante-detrás, medida en dB, en el plano horizontal.
θetilt es el ángulo, medido en grados, entre la dirección del haz principal y el horizonte en el
plano vertical (también conocido como la inclinación eléctrica de la antena).
K = 164
θ3dB .
383.2. CÁLCULO DE LA TASA DE TRANSMISIÓN
Según [37], para optimizar el tilt θetilt, este se puede calcular con respecto a la altura efectiva de
la antena transmisora, hT,eff, y la distancia entre la estación base y el límite de la celda, dc, como
sigue (ver Figura 3.1):
θetilt = arctan
( hT,eff
dc
)
+ z · θ3dB, (3.3)
donde z fue determinado empíricamente para un despliegue celular regular, y se estableció en 0.7
para lograr un buen equilibrio entre la potencia recibida y la interferencia entre celdas, donde hT,eff
y dc están expresados en metros.
hT,eff
dc
θetilt
θ3dB
hR
d
Figura 3.1: Inclinación de una estación base [38].
Para considerar el efecto conjunto de los patrones de antena horizontal y vertical y crear un patrón
de antena 3-D, se utiliza el siguiente modelo:
Ga(φ, θ)[dB] = Ga
M[dBi] + Ga
H(φ)[dB] + Ga
V(θ)[dB], (3.4)
donde Ga
M es la ganancia máxima de la antena en dBi. La Tabla 3.1 presenta valores típicos de este
modelo de antena.
Parámetro Valor
3 sectores
Ga
M 15.5 dBi
φ3dB 65 deg
θ3dB 11.5 deg
κ 30 dB
Tabla 3.1: Parámetros típicos de una antena sectorial.
Los patrones de la antena, tanto vertical como horizontal, se observan en la Figura 3.2.
3.2.2. Modelado de La Pérdida por Distancia
Un importante factor en la ganancia total del canal entre un transmisor y un receptor es la pérdida
por distancia. Esta pérdida se debe a la atenuación de la señal a medida que se propaga a través
del espacio. La pérdida por distancia se puede modelar de diferentes maneras, dependiendo de las
características del entorno y de la frecuencia de operación. En general, la pérdida por distancia se
puede modelar como una función logarítmica de la distancia entre el transmisor y el receptor. En
situación de visión directa, la pérdida por distancia se puede modelar como:
Gp,f s[dB] = −20 log10(d) − 20 log10(fc) + 147,55, (3.5)
39CAPÍTULO 3. GIULIAhorizontal
vertical
 ́30 ̋
 ́60 ̋
 ́90 ̋
 ́120 ̋
 ́150 ̋
 ̆180 ̋
150 ̋
120 ̋
90 ̋
60 ̋
30 ̋
0 ̋
0 dB
 ́10 dB
 ́20 dB
Figura 3.2: Diagrama de radiación de la antena [38].
donde d es la distancia entre el transmisor y el receptor en metros, y fc es la frecuencia de operación
en Hz. Sin embargo, esta pérdida por distancia no se mantiene para todos los entornos debido a que
asume condiciones ideales, como la ausencia de obstáculos y la propagación en línea de visión.
Por lo tanto, es necesario considerar otros factores, como la presencia de obstáculos, vegetación y
edificios, que pueden afectar la propagación de la señal y aumentar la pérdida por distancia.
Para aumentar la precisión del cálculo de la pérdida por distancia a escenarios más realistas, modelados deterministas o estadísticos han de ser utilizados [39]. Los modelos deterministas están
basados en trazado de rayos, y por tanto resultan muy precisos. Pero tienen un contrapunto, además de requerir un alto coste computacional, necesitan una descripción detallada del entorno, lo
que puede ser complicado de obtener. Por otro lado, los modelos estadísticos son más sencillos de
implementar y están basados en medias de pérdida por distancia en entornos típicos (rural, urbano,
etc.).
Un modelo estadístico ampliamente utilizado es el modelo de Okumura-Hata [40], el cual fue construido a partir de mediciones recolectadas en la ciudad de Tokio y mejorado mediante mediciones
en otras ciudades. Sin embargo, el modelo de Okumura-Hata tiene algunas limitaciones intrínsecas. Por ejemplo, no considera el perfil del terreno entre el transmisor y el receptor, ya que asume
que los transmisores están ubicados en colinas, y solo proporciona soporte hasta 1.9 GHz.
Para superar tales limitaciones y adaptarse a todo tipo de condiciones, se han desarrollado una
gran cantidad de modelos estadísticos basados en diferentes campañas de medición. En nuestro
caso hemos utilizado el modelo 3GPP TR 36.814 caso 1. Cuya función de pérdida por distancia
está modelada en la siguiente ecuación:
Gp,3GP P T R36,814[dB] = −128,1 − 37,6 log10 (R) , (3.6)
donde R es la distancia entre el transmisor y el receptor en kilómetros.
Se pueden observar las diferencias entre el modelo ideal y el modelo 3GPP TR 36.814 en la Figura 3.3. El modelo 3GPP TR 36.814 presenta una mayor pérdida por distancia en comparación
con el modelo ideal. Esto se debe a que el modelo 3GPP TR 36.814 tiene en cuenta la presencia
de obstáculos y la propagación no ideal de la señal.
403.2. CÁLCULO DE LA TASA DE TRANSMISIÓN0 2000 4000 6000 8000 10000
Distancia (metros)
160
140
120
100
80
60
40
20
Pérdida por Distancia (dB)
Comparación de Modelos de Pérdida por Distancia
Pérdida por Distancia en Espacio Libre (1 GHz)
Pérdida por Distancia según 3GPP TR 36.814
Figura 3.3: Comparación entre el modelo ideal y el modelo 3GPP TR 36.814 de pérdida por
distancia.
3.2.3. Modelado de Shadowing
El shadowing modela las fluctuaciones aleatorias de la potencia de señal recibida en el lado del
receptor, causadas por las obstrucciones de objetos que una señal de radio experimenta en su trayectoria de propagación. Las ubicaciones, tamaños y propiedades dieléctricas de los objetos obstructores, así como las superficies reflectantes y los obstáculos dispersores, generalmente son desconocidos. Debido a tales incertidumbres, se utilizan modelos estadísticos para modelar el shadowing.
Un modelo ampliamente utilizado es el modelo de shadowing log-normal [41], que ha demostrado
ser capaz de modelar las atenuaciones del shadowing con buena precisión en entornos exteriores
e interiores.
En consecuencia, el shadowing en la trayectoria entre un transmisor y un receptor se puede modelar a priori utilizando una variable aleatoria log-normal, Gs ∼ N (μs, σ∈
s ), donde μs y σs son
la media y la desviación estándar de la variable aleatoria log-normal en dB, respectivamente. Sin
embargo, el modelado del shadowing cuando se consideran múltiples transmisores (por ejemplo,
estaciones base) y receptores (por ejemplo, UEs) es más complicado debido a las propiedades de
auto-correlación espacial y correlación cruzada del shadowing.
La auto-correlación espacial se debe a que un UE en movimiento puede experimentar atenuaciones
de shadow fading similares de una estación base dada, mientras que estas atenuaciones pueden
diferir significativamente en posiciones alejadas.
La correlación cruzada, por otro lado, se debe a que múltiples enlaces de diferentes estaciones base
a un mismo UE pueden observar un shadow fading altamente correlado, dado que el entorno del
UE es común. Por ejemplo, si dos estaciones base están muy cerca una de la otra, los dos caminos
entre las estaciones base y un UE pueden atravesar un entorno similar, y por tanto el shadow fading
será similar en los enlaces.
41CAPÍTULO 3. GIULIA
3.2.4. Ganancia de Penetración
La ganancia de penetración modela la atenuación que sufre una señal de radio al atravesar un
obstáculo, como edificios o paredes. Esta pérdida depende de la frecuencia de la señal y de las propiedades del material (permisividad, permeabilidad y conductividad). Para evaluar el rendimiento
de un receptor dentro de un edificio esta perdida es fundamental. Los modelos estadísticos de pérdida por distancia solo capturan el efecto promedio de los obstáculos, y asumen que el receptor
esta fuera de edificios.
Una forma de modelar estos efectos es usar herramientas deterministas como el trazado de rayos,
Sin embargo, estas herramientas son computacionalmente complejas y pueden requerir mucho
tiempo de simulación.
Como compromiso, se puede usar un modelo que asocie una ganancia de penetración, Ge, a cada
UE dentro de un edificio y agregarlo a la pérdida por distancia. La ganancia de penetración total se
calcula sumando las pérdidas individuales de cada pared entre el transmisor y el receptor. Si ambos,
transmisor y receptor, están en interiores pero en edificios diferentes, la ganancia de penetración
es 2 × Ge; de lo contrario, es 0 dB. Según [42], la ganancia de penetración promedio es Ge = −20
dB.
3.2.5. Modelado del Desvanecimiento Multicamino
Los objetos que interfieren en la trayectoria de propagación desde el transmisor al receptor también
pueden producir copias reflejadas, difractadas y dispersas de la señal, resultando en componentes
de múltiples trayectorias (MPC, del inglés Multi Path Component). Los MPC pueden llegar al
receptor atenuados en potencia, retrasados en el tiempo y desplazados en frecuencia (y/o fase) con
respecto al primer y más fuerte MPC, sumándose así de manera constructiva o destructiva. Como
consecuencia, la potencia recibida en el receptor puede variar significativamente sobre distancias
muy pequeñas del orden de unas pocas longitudes de onda [41].
Dado que diferentes MPC viajan por diferentes caminos de distintas longitudes, un único impulso,
enviado desde un transmisor, y que sufre de múltiples trayectorias, resultará en múltiples copias
de dicho impulso único, siendo recibidas en el receptor en diferentes momentos. Como resultado,
la respuesta al impulso del canal de múltiples trayectorias se puede modelar usando una línea de
retardo con taps modelados de la siguiente forma:
h(τ, t) =
ν−1∑
i=0
hi(t) · δ(τ − τi), (3.7)
donde ν es el número de taps o MPC resolubles, y hi(t) y τi son la ganancia compleja del canal y
el retardo del tap i, respectivamente.
Los PDPs se utilizan a menudo para modelar estas líneas de retardo con taps. Un perfil de retardo
de potencia se caracteriza por el número de taps, ν, el retardo temporal relativo al primer tap, la
potencia promedio relativa al tap más fuerte y el espectro Doppler de cada tap. Los PDPs más
frecuentemente utilizados son los especificados por ITU 3.2,
Es importante señalar que las pérdidas de las muestras con retardo suelen ser relativamente pequeñas, pero ocasionalmente y dependiendo del escenario, pueden ser significativos. Estos retardos se
pueden observar en la Tabla 3.2.
423.2. CÁLCULO DE LA TASA DE TRANSMISIÓN
Retardo (ns) Potencia Relativa (dB) Retardo (ns) Potencia Relativa (dB)
Peatón (≤3 km/h)
Tap Canal A (40 %) {Canal B} (55 %)
1 0 0 0 0
2 110 -9.7 200 -0.9
3 190 -19.2 800 -4.9
4 410 -22.8 1200 -8.0
5 - - 2300 -7.8
6 - - 3700 -23.9
Tabla 3.2: PDPs para UEs peatones (Pedestrian, ≤3 km/h).
Sin embargo, los cálculos de todas estos taps pueden ser realmente costosos para los ordenadores
actuales, ya que hay que calcularlos por cada señal se envía en el sistema. Por tanto, se usan matrices precalculadas que no tienen ningún significado geográfico, simplemente un comportamiento
general de como estas reflexiones multicamino se comportan. En la Figura 3.4, se puede ver un
ejemplo de dichos modelos precalculados.frequency (RBs)
time (ms)
12.5 25 37.5 50 62.5 75 87.5 100
−50
−40
−30
−20
−10
0
10
2.5
5
7.5
10
12.5
15
17.5
20
22.5
25
Figura 3.4: Mapa desvanecimiento multicamino [38].
3.2.6. Modelado de Potencia de Señal Recibida
Habiendo definido la ganancia de la antena, Ga, la pérdida por distancia, Gp, el desvanecimiento
por shadowing, Gs, la ganancia de penetración, Ge, y el desvanecimiento por múltiples trayectorias,
Gff en las secciones anteriores, la potencia de la señal recibida, P rx
t,r,k, de la señal transmitida por
un transmisor, Tt, en un receptor, Rr, en un recurso de frecuencia, Kk, se puede calcular como:
P rx
t,r,k = P tx
t,k · Ga
t,r · Gp
t,r · Ge
t,r · Gs
t,r ·
∣
∣
∣Gff
t,r,k
∣
∣
∣2
, (3.8)
donde:
P tx
t,k es la potencia de transmisión aplicada por el transmisor Tt en el recurso de frecuencia
Kk,
Ga
t,r es la ganancia de la antena desde el transmisor Tt hasta el receptor Rr,
43CAPÍTULO 3. GIULIA
Gp
t,r es la pérdida por distancia desde el transmisor Tt hasta el receptor Rr,
Ge
t,r es la ganancia de penetración desde el transmisor Tt hasta el receptor Rr,
Gs
t,r es el desvanecimiento por shadowing desde el transmisor Tt hasta el receptor Rr y
∣
∣
∣Gff
t,r,k
∣
∣
∣2
es el desvanecimiento por múltiples trayectorias desde el transmisor Tt hasta el
receptor Rr en el recurso de frecuencia Kk
(nótese que Gff
t,r,k se definió como una ganancia compleja). Todas estas variables están expresadas
en unidades lineales.
3.2.7. Modelado de La Calidad de Señal
Habiendo definido la potencia de la señal recibida , P rx
t,r,k, de la señal transmitida por un transmisor,
Tt, a un receptor, Rr, y un recurso de frecuencia, Kk. Se define la SINR, γt,r,k, de esta señal ,en un
caso de entrada única y salida única, se puede modelar como ya se introdujo en 1.9, sin embargo
aquí se utiliza notación de transmisor, tx, y receptor, rx:
γt,r,k = P rx
t,r,k
∑T
t′=0
t′̸=t
P rx
t′,r,k + σ2 . (3.9)
Para ilustrar, la Figura 3.5 muestra los mapas de SINR para una disposición de macroceldas hexagonales simulada en la ciudad de Dublín, Irlanda, donde los mapas se han calculado como se
indica en la Ecuación 3.9.SINR [dB]
y[m]
x[m]
 ́500 0 500  ́10
 ́5
0
5
10
15
20
25
30
 ́600
 ́400
 ́200
0
200
400
600
Figura 3.5: SINR ciudad de Dublín [38].
443.3. CREACIÓN DE UN ENTORNO
3.2.8. Calculo de La Tasa de Transmisión
El teorema de Shannon-Hartley [43] indica que la capacidad máxima de un canal de comunicación
se puede calcular según:
Cr,k,t = B log2(1 + γt,r,k), (3.10)
donde, si utilizamos la capacidad máxima del canal, tenemos que la tasa de transmisión en la
comunicación entre el transmisor y receptor es Rr,k,t = Cr,k,t.
3.3. Creación de Un Entorno
Para trabajar de manera efectiva con Giulia, es fundamental asegurar que todas las simulaciones
se realicen en un entorno uniforme. Esto garantiza que, independientemente del sistema utilizado
ya sea un ordenador local, un clúster de ordenadores, o si otra persona accede al simulador, se
mantengan las mismas versiones de todos los componentes. Esto es crítico en Giulia debido a la
naturaleza estocástica de muchos de sus cálculos.
La estocasticidad en programación es un aspecto delicado, ya que las versiones de los paquetes
utilizados pueden influir en las semillas que generan los números aleatorios. Por lo tanto, es vital
mantener las mismas versiones de los paquetes, así como los drivers instalados en el sistema.
Para resolver esta problemática, se ha optado por utilizar Docker.
Docker es una herramienta poderosa que permite la creación de contenedores que encapsulan todas
las dependencias necesarias, asegurando un entorno replicable y consistente. Para configurar este
entorno utilizando Docker, se debe crear un archivo Dockerfile que especifique las versiones
exactas de los paquetes, librerías y drivers necesarios.
El Dockerfile actúa como una receta detallada para construir la imagen del contenedor. Una
vez creado este archivo, se procede a construir la imagen, un proceso que genera una versión
fija del entorno de trabajo. Esta imagen puede ser almacenada y reutilizada tantas veces como
sea necesario. La principal ventaja de esto es que una vez que se ha creado la imagen, se pueden
generar múltiples contenedores a partir de ella. Si se realiza un cambio en un contenedor que resulta
en un fallo del código, simplemente se elimina ese contenedor y se crea uno nuevo desde la imagen
original, garantizando que el entorno inicial se mantenga intacto.
El uso de Docker para gestionar el entorno de trabajo en proyectos de simulación, como los realizados con Giulia, ofrece múltiples ventajas:
Consistencia del Entorno: Docker asegura que todos los desarrolladores y usuarios del
proyecto trabajen con el mismo entorno, evitando problemas derivados de las diferencias en
configuraciones locales.
Reproducibilidad: Gracias a la naturaleza de las imágenes Docker, se puede garantizar que
las simulaciones y resultados sean reproducibles en cualquier momento, incluso en diferentes
máquinas.
Portabilidad: Los contenedores Docker pueden ejecutarse en cualquier lugar donde Docker
esté instalado, ya sea en una computadora personal, un servidor o en la nube. Esto facilita la
transición entre diferentes entornos de desarrollo, prueba y producción.
45CAPÍTULO 3. GIULIA
Aislamiento: Cada contenedor opera de manera independiente, aislado del sistema anfitrión
y de otros contenedores. Esto minimiza conflictos de dependencias y asegura un entorno
limpio para cada instancia del proyecto.
Facilidad de Mantenimiento: Actualizar o modificar el entorno es tan sencillo como editar
el Dockerfile y reconstruir la imagen. Esto permite una gestión centralizada y controlada
de las dependencias y configuraciones del proyecto.
En resumen, utilizar Docker para gestionar el entorno de trabajo con Giulia proporciona una solución robusta y eficiente para mantener la consistencia, portabilidad y reproducibilidad de los
resultados de las simulaciones.
3.4. Optimizaciones sobre Giulia
Dentro del RL, la rapidez de nuestro entorno es crítica a la hora de entrenar un agente. Para que
estos agentes converjan y obtengan buenos resultados, es necesario realizar del orden de miles
de iteraciones. El simulador, Giulia, debe ser por tanto extremadamente eficiente. Sin embargo,
la versión inicial de Giulia no habia sido optimizada y permitía mejoras, 100 iteraciones podían
consumir 185.18 segundos.
Para poder comparar, a pesar de que Giulia tiene implementados un gran número de modelos estándar de comunicaciones inalámbricas del 3GPP, utilizaremos el modelo ITU-R M.2135-1 [44].
Este modelo es usado como referencia ya que no es el modelo más complejo, pero tampoco el que
menos cálculos requiere, permitiendo evaluar cómo se comportan las optimizaciones en un modelo
de complejidad media. En concreto, este modelo tiene 570 UEs y 57 estaciones base de tecnología
4G.
Además, para las comparaciones utilizaremos el tiempo en el que tardan en hacerse 100 ejecuciones. Una simple ejecución implica que el modelo realice los cálculos explicados en la sección 3.2,
con toda la complejidad que ello conlleva, incluyendo una gran cantidad de UEs, estaciones base,
matrices de números complejos aleatorios de gran tamaño, etc.
Las optimizaciones realizadas siguen tres principales vertientes:
Plotting ineficiente: El código original incluía una gran cantidad de plotting, lo cual ralentizaba la ejecución. Se eliminó todo el plotting innecesario para poder comparar la velocidad
de ejecución de los modelos, ya que este plotting solo es necesario cuando se visualizan los
resultados uno a uno, no cuando se realizan múltiples ejecuciones.
Funciones vectorizadas: En Python, es más eficiente utilizar funciones vectorizadas en lugar de aplicar funciones a cada elemento individualmente. Es decir, en lugar de recorrer todos
los elementos de un vector en un bucle y aplicar una función a cada uno, es más eficiente
aplicar la función directamente al vector. Esto se debe a que las funciones vectorizadas están
implementadas en C, lo que las hace mucho más rápidas que un bucle en Python. Aprovechamos también el broadcasting y el indexing [45].
Utilización de GPU: Para la implementación en GPU se utilizó la librería de Python, Pytorch [46]. Pytorch es una librería de código abierto desarrollada por Meta que facilita la
463.4. OPTIMIZACIONES SOBRE GIULIA
realización de cálculos en paralelo en la GPU. La GPU es una unidad de procesamiento que
se utiliza para acelerar los cálculos, especialmente en tareas que requieren una gran cantidad
de cálculos matriciales.
Gracias a todas estas optimizaciones, se logró reducir el tiempo de ejecución por cada 100 iteraciones de 185.18 segundos a 22.41 segundos, lo que representa una reducción del 87.9 % en el tiempo
de ejecución.
3.4.1. Plotting Ineficiente
Como se explicó previamente, el plotting es una operación que consume mucho tiempo en entornos de programación. Si se realizan 100 iteraciones, después de cada iteración es necesario detener
los cálculos, recopilar los resultados y mostrarlos en pantalla, lo cual consume tiempo, especialmente cuando se necesitan iteraciones lo más rápidas posibles. Por tanto, una primera estrategia
de optimización fue eliminar los plots innecesarios, dejando solo el resultado final para comparar
la velocidad de ejecución de los modelos.
Solo con esta modificación el tiempo de ejecución se redujo de 185.18 segundos a 108.017 segundos en el modelo que estamos comparando. En la Tabla 3.3, se puede observar cual es la eficiencia
de nuestro modelo, ITU-R M.2135-1, así como de otros implementados en Giulia, tras dicha modificación.
Modelo Tiempo Total
3GPP TR 38.901 UMa lsc 719.801
ITU-R M.2135 UMa Umi colocated multilayer 246.922
ITU-R M.2135 UMa multilayer 214.340
3GPP TR 36.777 UMi AV 129.000
3GPP TR 36.777 UMa AV 122.614
3GPP TR 38.901 UMi lsc 118.566
ITU-R M.2135 UMa 108.017
ITU-R M.2135 UMi 96.860
3GPP TR 36.814 Case-1 38.608
3GPP TR 36.814 Case-1.omni 22.520
3GPP TR 38.901 UMa lsc single bs 9.936
3GPP TR 36.814 Case-1 single bs 5.585
Tabla 3.3: Resultados después de plotting ineficiente.
3.4.2. Funciones Vectorizadas
Un claro ejemplo de cómo los bucles de Python pueden crecer en complejidad y llegar a no ser
eficientes se muestra en la Figura 3.6.
En este cálculo se están determinando las distancias entre los UEs y las estaciones base, así como
los ángulos que forman, para calcular posteriormente la ganancia de la antena según el diagrama
de radiación, por tanto se recorren cada una de las posiciones de la matriz.
47CAPÍTULO 3. GIULIA
index_3d_min = np.argmin(distance_3d_partial_results, axis=0)
distance_3d_results = np.array([[distance_3d_partial_results[index_3d_min[i,j],i,j] for
j in range(0, np.size(index_3d_min, 1))] for i in range(0, np.size(index_3d_min,
0))])
↪→
↪→
index_2d_min = np.argmin(distance_2d_partial_results, axis=0)
distance_2d_results = np.array([[distance_2d_partial_results[index_2d_min[i,j],i,j] for
j in range(0, np.size(index_2d_min, 1))] for i in range(0, np.size(index_2d_min,
0))])
↪→
↪→
azimuth_results = np.array([[azimuth_partial_results[index_3d_min[i,j],i,j] for j in
range(0, np.size(index_3d_min, 1))] for i in range(0, np.size(index_3d_min, 0))])↪→
zenith_results = np.array([[zenith_partial_results[index_3d_min[i,j],i,j] for j in
range(0, np.size(index_3d_min, 1))] for i in range(0, np.size(index_3d_min, 0))])↪→
Figura 3.6: Código ejemplo bucles for en list comprehension.
Para mejorar esta implementación se utilizó un estilo de funciones vectorizadas y el indexamiento
directo de vectores, resultando en el código visto en la Figura 3.7.
Las funciones vectorizadas permiten realizar operaciones sobre arrays enteros de manera eficiente, utilizando optimizaciones de bajo nivel que aprovechan el hardware subyacente. Por ejemplo,
NumPy proporciona funciones que realizan operaciones como suma, multiplicación y funciones
matemáticas avanzadas directamente sobre arrays.
El indexamiento directo de vectores permite acceder y manipular elementos específicos de un
array mediante el uso de índices, que pueden ser enteros, slices o booleanos. Esto resulta en una
mayor eficiencia en comparación con los bucles for tradicionales. En un bucle for, el intérprete
de Python tiene que gestionar la iteración y acceder a cada elemento uno por uno, lo cual puede
ser lento debido a la sobrecarga de la gestión del bucle y la interpretación de cada instrucción.
En cambio, el indexamiento directo aprovecha la implementación optimizada en C de NumPy,
permitiendo operaciones masivas sobre arrays sin la necesidad de bucles explícitos, reduciendo así
la sobrecarga y acelerando el procesamiento.
index_3d_min = np.argmin(distance_3d_partial_results, axis=0)
index_2d_min = np.argmin(distance_2d_partial_results, axis=0)
# Creating arrays of indices for gathering the results efficiently
i_indices, j_indices = np.indices(index_3d_min.shape)
# Efficiently gather the 3D distance results
distance_3d_results = distance_3d_partial_results[index_3d_min, i_indices, j_indices]
# Efficiently gather the 2D distance results using the index_2d_min
distance_2d_results = distance_2d_partial_results[index_2d_min, i_indices, j_indices]
# Efficiently gather the azimuth and zenith results
azimuth_results = azimuth_partial_results[index_3d_min, i_indices, j_indices]
zenith_results = zenith_partial_results[index_3d_min, i_indices, j_indices]
Figura 3.7: Mejora por indexamiento directo de vectores.
483.4. OPTIMIZACIONES SOBRE GIULIA
Esto permitió una implementación más directa del código sin tener que iterar a través de todas las
posiciones de las matrices, siendo mucho más eficiente.
Estos cambios también se aplicaron a la forma en que la posición de los UEs se implementa en el
simulador, estandarizada en [44]. El código inicialmente utilizado se puede ver en la Figura 3.8.
Es un conjunto de operaciones denso en el que hay que manejar bien las dimensiones. Por cada iteración del bucle se están haciendo multiplicaciones de matrices. En las que además, dependiendo
del modelo de entorno que se esté utilizando las matrices varían de tamaño. Así que se aprovechó
el comando de Numpy llamado einsum [47], que permite hacer una gran cantidad de operaciones siguiendo la notación de Einstein para operaciones matriciales [48]. Estas modificaciones se
pueden ver en la Figura 3.9.
De esta manera se consigue evitar los bucles for, y realizar un aplicación directa de funciones
vectorizadas, además de la gran utilidad que tiene el comando Einsum, con el cual hemos realizado
multiplicaciones de matrices con tamaños inconsistentes.
3.4.3. Implementación GPU
Como se mencionó anteriormente, la GPU es fundamental para acelerar las operaciones matriciales. En el simulador, se maneja una gran cantidad de matrices debido al elevado número de UEs y
estaciones base, así como las conexiones entre ellos.
En la Figura 3.10, se muestra cómo la variable aleatoria que forma parte de la computación del
fast fading, que refleja pequeños cambios en la señal debido a fenómenos como el multicamino,
se calculaba utilizando Numpy, el cual realiza todos sus cálculos a través de la CPU.
Este proceso se maneja de manera más eficiente con la librería Torch, que permite especificar un
device donde se realizarán los cálculos. En nuestro caso, utilizar la GPU es lo más conveniente;
sin embargo, para los equipos sin GPU, esta asignación se realiza automáticamente mediante el
código en la Figura 3.11.
Después de incluir estas líneas en el main del proyecto, se implementó el cálculo de fast fading
con Torch, como se puede observar en la Figura 3.12.
Implementaciones similares se realizaron a lo largo del código en funciones que manejaban matrices muy grandes y que se utilizaban en gran parte del proyecto, como la optimización mostrada en
la Figura 3.13.
La realización de estos cambios supuso una complejidad adicional más allá de la simplemente
observada en las líneas de código. Al tratarse de un proyecto complejo, fue necesario cambiar
la estructura del simulador y adaptar estos nuevos cambios sin tener que realizar un refactoring
completo. Es decir, todas las partes del código donde se utilizaban estas funciones tuvieron que
ser modificadas para adaptarse a la librería Torch. Además, se llevó a cabo un análisis detallado
para determinar si los cuellos de botella del simulador podían ser solucionados mediante una simple
implementación en GPU o si había otros factores subyacentes. Este fue el caso del código mostrado
en la Figura 3.14.
Esta función, aunque está claramente estructurada y no parece presentar problemas de eficiencia a
primera vista, genera un cuello de botella debido a cómo los operadores lógicos de Numpy operan
sobre vectores y matrices.
49CAPÍTULO 3. GIULIA
antenna_element_GCS_position_m =
np.zeros((np.size(antenna_element_LCS_position_m,0),3), dtype=np.single)↪→
# Process node by node as they have different rotations
values, counts = np.unique(antenna_to_node_mapping, return_counts=True)
for node_index in values:
# Get azimuth rotation
alpha_rad = np.radians(-alpha_bearing_deg[node_index])
# Get zenith rotaton
beta_rad = np.radians(-beta_downtilt_deg[node_index]+90)
# Get slant rotation - Usually MIMO panels do not have a slant rotation
gamma_rad = np.radians(-gamma_slant_deg[node_index])
# Precompute some products
coscos = np.cos(alpha_rad)*np.cos(beta_rad)
cossin = np.cos(alpha_rad)*np.sin(beta_rad)
sincos = np.sin(alpha_rad)*np.cos(beta_rad)
sinsin = np.sin(alpha_rad)*np.sin(beta_rad)
# Compute rotation matrix - See equation 7.1-5 of TR 38.901 V17.0.0
R_inv = np.array([[ coscos
, sincos ,
-1*np.sin(beta_rad)],
↪→
↪→
[ cossin*np.sin(gamma_rad) -
np.sin(alpha_rad)*np.cos(gamma_rad),
sinsin*np.sin(gamma_rad) +
np.cos(alpha_rad)*np.cos(gamma_rad),
np.cos(beta_rad)*np.sin(gamma_rad) ],
↪→
↪→
↪→
↪→
[ cossin*np.cos(gamma_rad) +
np.sin(alpha_rad)*np.sin(gamma_rad),
sinsin*np.cos(gamma_rad) -
np.cos(alpha_rad)*np.sin(gamma_rad),
np.cos(beta_rad)*np.cos(gamma_rad) ]])
↪→
↪→
↪→
↪→
# Get antennas of the node
mask_antennas_of_selected_cells = antenna_to_node_mapping == node_index
antenna_element_LCS_position_m_filtered =
antenna_element_LCS_position_m[mask_antennas_of_selected_cells]↪→
# Compute LCS to GCS rotation for all antennas of the node
antenna_element_GCS_position_m_filtered = np.array([ np.matmul(R_inv,
np.transpose((antenna_element_LCS_position_m_filtered[i,:]))) for i in
range(0,np.size(antenna_element_LCS_position_m_filtered,0)) ])
↪→
↪→
antenna_element_GCS_position_m[mask_antennas_of_selected_cells] =
antenna_element_GCS_position_m_filtered↪→
#Store calculations in array
node_positions_mat_m = np.repeat(node_positions_m, counts, axis=0)
antenna_element_GCS_position_m += node_positions_mat_m
return antenna_element_GCS_position_m
Figura 3.8: Código transformación de posición LCS a GCS en bucle for.
503.4. OPTIMIZACIONES SOBRE GIULIA
antenna_element_GCS_position_m =
np.zeros((np.size(antenna_element_LCS_position_m,0),3), dtype=np.single)↪→
# Count the number of antennas per node
_ , counts = np.unique(antenna_to_node_mapping, return_counts=True)
# Get azimuth rotation
alpha_rad_vector = np.radians(-alpha_bearing_deg[antenna_to_node_mapping])
# Get zenith rotaton
beta_rad_vector = np.radians(-beta_downtilt_deg[antenna_to_node_mapping]+90)
# Get slant rotation - Usually MIMO panels do not have a slant rotation
gamma_rad_vector = np.radians(-gamma_slant_deg[antenna_to_node_mapping])
coscos_vector = np.cos(alpha_rad_vector)*np.cos(beta_rad_vector)
cossin_vector = np.cos(alpha_rad_vector)*np.sin(beta_rad_vector)
sincos_vector = np.sin(alpha_rad_vector)*np.cos(beta_rad_vector)
sinsin_vector = np.sin(alpha_rad_vector)*np.sin(beta_rad_vector)
# Compute rotation matrix - See equation 7.1-5 of TR 38.901 V17.0.0
R_inv_vector = np.array([[ coscos_vector
, sincos_vector ,
-1*np.sin(beta_rad_vector)],
↪→
↪→
[ cossin_vector*np.sin(gamma_rad_vector) -
np.sin(alpha_rad_vector)*np.cos(gamma_rad_vector),
sinsin_vector*np.sin(gamma_rad_vector) +
np.cos(alpha_rad_vector)*np.cos(gamma_rad_vector),
np.cos(beta_rad_vector)*np.sin(gamma_rad_vector) ],
↪→
↪→
↪→
↪→
[ cossin_vector*np.cos(gamma_rad_vector) +
np.sin(alpha_rad_vector)*np.sin(gamma_rad_vector),
sinsin_vector*np.cos(gamma_rad_vector) -
np.cos(alpha_rad_vector)*np.sin(gamma_rad_vector),
np.cos(beta_rad_vector)*np.cos(gamma_rad_vector) ]])
↪→
↪→
↪→
↪→
# Compute LCS to GCS rotation for all antennas of the nodes
antenna_element_GCS_position_m = np.einsum('ijk, kj -> ki', R_inv_vector,
antenna_element_LCS_position_m)↪→
Figura 3.9: Código transformación de posición LCS a GCS sin bucle for.
def Rayleigh_fading(self, available_PRBs, number_of_RX_antennas, number_of_TX_antennas):
rng = np.random.RandomState(self.seed+0)
return (1/np.sqrt(2)) * (rng.standard_normal((available_PRBs, number_of_RX_antennas,
number_of_TX_antennas))).astype(np.single)↪→
Figura 3.10: Código fast fading de Numpy.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
Figura 3.11: Código asignación automática dispositivo.
51CAPÍTULO 3. GIULIA
def Rayleigh_fading_torch(self, available_PRBs, number_of_RX_antennas,
number_of_TX_antennas):↪→
rng = torch.Generator(device=self.device)
a = (1/torch.sqrt(torch.tensor(2, device=self.device))) *
(torch.cuda.FloatTensor(available_PRBs, number_of_RX_antennas,
number_of_TX_antennas, device = self.device).normal_(generator=rng))
↪→
↪→
#convert to cpu and numpy
return a
Figura 3.12: Código fast fading de torch.
def mW_to_dBm_torch(value_mW):
result = 10 * torch.log10(value_mW)
return result
Figura 3.13: Código mW a dB de torch.
if isinstance(beam_activity_per_ue, np.ndarray):
mask = np.logical_and(beams_in_same_carrier_than_server,
PRB_ue_beam_interference_activity)↪→
sum_rss_mW = np.sum(tools.dBm_to_mW(np.where(mask, rsrp_prb_ue_to_cell_dBm,
np.NINF)),axis=2)↪→
else:
sum_rss_mW = np.sum(tools.dBm_to_mW(np.where(beams_in_same_carrier_than_server,
rsrp_prb_ue_to_cell_dBm, np.NINF)),axis=2)↪→
Figura 3.14: Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR.
if isinstance(beam_activity_per_ue, np.ndarray):
mask = torch.logical_and(torch.tensor(beams_in_same_carrier_than_server,
device=device), torch.tensor(PRB_ue_beam_interference_activity, device=device))↪→
selected_values_dBm_2 = torch.where(mask, torch.tensor(rsrp_prb_ue_to_cell_dBm,
device=device, dtype=torch.float64), torch.tensor(np.NINF, device=device))↪→
rsrp_prb_ue_to_cell_mW_2 = tools.dBm_to_mW_torch(selected_values_dBm_2)
sum_rss_mW = torch.sum(rsrp_prb_ue_to_cell_mW_2, axis=2)
sum_rss_mW= sum_rss_mW.cpu().numpy()
else:
selected_values_dBm = torch.where(torch.tensor(beams_in_same_carrier_than_server,
device=device), torch.tensor(rsrp_prb_ue_to_cell_dBm, device=device), np.NINF)↪→
rsrp_prb_ue_to_cell_mW = tools.dBm_to_mW_torch(selected_values_dBm)
sum_rss_mW = torch.sum(rsrp_prb_ue_to_cell_mW, axis=2)
sum_rss_mW = sum_rss_mW.cpu().numpy()
Figura 3.15: Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR usando
torch.
523.4. OPTIMIZACIONES SOBRE GIULIA
Para llegar a esta versión final que se observa en la Figura 3.15, se evaluó si cada línea de código
era más eficiente al realizarla en Python o en Torch. La versión final, que incluye la conversión de
Torch a Numpy al final, resultó ser la más eficiente.
3.4.4. Comparación de Eficiencia del Simulador
Una vez implementadas las optimizaciones mediante la vectorización de funciones y el uso de
GPU, se procedió a comparar la eficiencia del simulador. Para una comparación realista de lo
que suponen estos cambios de GPU y vectorización, se utilizó como referencia el tiempo total
consumido por modelo medido después de la eliminación del plotting innecesario, presentada en
la Tabla 3.3. Tras aplicar todas las modificaciones, el tiempo total consumido por modelo se observa
en la Tabla 3.4.
Modelo Tiempo Total
3GPP TR 38.901 UMa lsc 63.226
ITU-R M.2135 UMa Umi colocated multilayer 39.023
ITU-R M.2135 UMa multilayer 32.528
3GPP TR 36.777 UMi AV 54.778
3GPP TR 36.777 UMa AV 51.053
3GPP TR 38.901 UMi lsc 47.532
ITU-R M.2135 UMa 24.732
ITU-R M.2135 UMi 27.092
3GPP TR 36.814 Case-1 16.666
3GPP TR 36.814 Case-1.omni 14.396
3GPP TR 38.901 UMa lsc single bs 7.456
3GPP TR 36.814 Case-1 single bs 4.689
Tabla 3.4: Resultados tras vectorización y uso de GPU.
Para evaluar cuantitativamente las mejoras, se calculó el porcentaje de tiempo ahorrado para cada
modelo, reflejando la eficiencia ganada en comparación con el tiempo de ejecución original. La
Tabla 3.5 muestra estos porcentajes de ahorro.
Los resultados indican una notable mejora en los tiempos de ejecución para la mayoría de los
modelos. En particular, los modelos 3GPP TR 38.901 UMa lsc e ITU-R M.2135 UMa multilayer
muestran ahorros superiores al 90 % y 84 % respectivamente.
Es importante destacar que las mejoras no son tan evidentes en los modelos con tiempos de ejecución bajos. Estos modelos no aprovechan tanto la GPU debido a la menor complejidad de las
multiplicaciones matriciales o a que los bucles for no son lo suficientemente grandes como para que la implementación en C de Numpy demuestre su eficiencia. Estos resultados subrayan la
importancia de realizar un análisis detallado para identificar las áreas donde las optimizaciones
tendrán el mayor impacto.
En conclusión, las optimizaciones aplicadas han resultado en una mejora significativa en la eficiencia del simulador para la mayoría de los modelos, demostrando la efectividad de la vectorización
y el uso de GPU en la reducción del tiempo de ejecución.
53CAPÍTULO 3. GIULIA
Modelo Porcentaje de Tiempo Ahorrado
3GPP TR 38.901 lsc UMa fr1 Umi C band plus fr3 58.17 %
3GPP TR 38.901 UMa lsc 91.21 %
ITU-R M.2135 UMa Umi colocated multilayer 84.20 %
ITU-R M.2135 UMa multilayer 84.82 %
3GPP TR 36.777 UMi AV 57.53 %
3GPP TR 36.777 UMa AV 58.36 %
3GPP TR 38.901 UMi lsc 59.90 %
ITU-R M.2135 UMa 77.11 %
ITU-R M.2135 UMi 72.02 %
3GPP TR 36.814 Case-1 56.82 %
3GPP TR 36.814 Case-1.omni 36.05 %
3GPP TR 38.901 UMa lsc single bs 24.95 %
3GPP TR 36.814 Case-1 single bs 16.06 %
Tabla 3.5: Porcentaje de tiempo ahorrado tras optimización.
54Capítulo 4
Metodología
4.1. Diseño del Problema de RL
Para hacer una primera aproximación al problema, se decidió utilizar un entorno con baja complejidad, con el objetivo de poder observar si utilizar DRL es una solución viable.
Por tanto, definimos el número de UEs, U , igual a 10, y los inicializamos en un área cuadrada de
10m2. Estos representan por ejemplo un equipo de rescate moviéndose todos juntos en un escenario
de emergencia. Por otra parte, el número de drones, D, utilizados es 1.
4.1.1. Espacio de Estados
Como se explicó en la sección 2.1.3, el espacio de estados no es lo mismo que el espacio de observaciones. En nuestro problema, y al disponer y entender completamente el entorno, podríamos
definir los estados como la posición de los UEs, la distancia que tiene el agente a ellos y donde
se encuentra este. Pero esto no es lo que estamos buscando, ya que nuestro objetivo es que este
agente aprenda de la forma más parecida a lo necesario en un entorno real. Por tanto, este tipo de
información no nos conviene. El espacio final de observaciones viene definido por:
Pos = [ρD
1,t, ρD
1,t−m, ρD
1,t−M ] (4.1)
 ̄γu = [ ̄γu,t,  ̄γu,t−m,  ̄γu,t−M ] (4.2)
μθ = [μθ,t, μθ,t−m, μθ,t−M ] (4.3)
σθ = [σθ,t, σθ,t−m, σθ,t−M ] (4.4)
OBS = [Pos,  ̄γu, μθ, σθ] (4.5)
donde Pos es el vector de posiciones de los drones,  ̄γu representa el vector de la relación señal a
interferencia más ruido, μθ y σθ son la media y la desviación estándar del ángulo con el que se
recibe la señal, medidas tomadas con un punto de referencia desde el este, como se observa en
la Figura 4.1, y OBS es el vector de observaciones que está conformado por Pos,  ̄γu, μθ y σθ.
Además, disponemos de una memoria de tamaño, M , que almacena las últimas M observaciones.
El objetivo de utilizar estas observaciones es que el dron use la información que puede inferir a
través de su estación base, para tener una idea de la posición de los UEs sin directamente dársela
como estado.
55CAPÍTULO 4. METODOLOGÍAN
S
EO
Figura 4.1: Ejemplo cálculo ángulos.
4.1.2. Espacio de Acciones
Dependiendo del modelo utilizado, que se explican en la sección 4.2, el espacio de acciones será
discreto o continuo.
En el caso de un espacio de acciones discreto, el agente dispone de un conjunto finito de estas,
definido por el conjunto A = {↑, ↓, →, ←, ·}, donde cada acción corresponde a un movimiento en
una de las siguientes direcciones: norte (↑), sur (↓), este (→), oeste (←), o permanecer en la misma
posición (·). Además, la magnitud del movimiento es constante, denotada por d, es decir, el agente
se desplaza una distancia fija de d metros en una sola dirección.
En el caso de un espacio de acciones continuo el agente tiene dos variables a elegir, la dirección y
la magnitud del movimiento. La dirección se define por el ángulo θd y la magnitud por la distancia
d. Por lo tanto, el espacio de acciones es A = {(θ, d)}, donde θ ∈ [−180, 180) y d ∈ [0, dmáx].
Siendo dmáx, la distancia máxima que puede recorrer el dron en una acción. El ángulo, θd, está
definido con respecto al este.
4.1.3. Recompensa
La elección de que recompensa elegir es de vital importancia dentro del DRL, ya que es lo que
va a determinar el comportamiento del agente. En este caso, hemos utilizado como recompensa la
ecuación 1.12. Sin embargo, con esto no era suficiente, ya que al estar los UEs tan concentrados,
hacía que las posiciones cercanas a este clúster fueran ya de por si rentables para el agente, por
tanto, no le importaba estar en la mejor posición, sino que iba saltando entre diferentes posiciones
cercanas. Para solucionar esto, primero se realizó una normalización de la recompensa, y después
se ajustó la recompensa para los valores más altos utilizando la siguiente ecuación:
reward = 2
( reward − 4
8,104 − 4
)
− 1 (4.6)
Si la recompensa resultante es mayor a 1.47, entonces se asigna un valor fijo de 10 a la recompensa
para incentivar fuertemente estas posiciones óptimas. Los valores de la normalización, la recompensa, y la posterior recompensa fija se han obtenido mediante prueba y error, y se ha comprobado
que son los que mejor resultado dan.
564.2. ALGORITMOS DE RL PARA RESOLVER EL PROBLEMA
4.2. Algoritmos de RL para Resolver El Problema
En esta sección vamos a explicar los fundamentos teóricos de los algoritmos implementados a lo
largo de nuestro proyecto.
4.2.1. Deep Q Networks
Estas redes trajeron consigo una revolución al campo del aprendizaje reforzado, ya que fue el
primer trabajo de investigación que consiguió demostrar la capacidad de las redes neuronales en
entornos complejos como los videojuegos de la Atari 2600 [29].
4.2.1.1. Naturaleza
En este trabajo, la red implementada se trataba de una red neuronal convolucional, que se encargaba
de aproximar la función de valor de las tuplas estado-acción, Q(s, a), que es la función que nos
indica cuánto de buena es una acción en un estado concreto. Un concepto muy similar a lo que
sucede en el algoritmo de Q-learning 1 , introducido en la sección 2.3.1, en este caso implementado
con redes neuronales.
Esta red neuronal convolucional se puede intercambiar por cualquier otro aproximador de funciones, como una red neuronal densa, lo que permite adaptarlo a cualquier tipo de tarea.
El algoritmo en el que se basa el DQN es en el de Q-learning, pero con unas ciertas diferencias a
la hora de implementarlo.
4.2.1.2. Función de Pérdida
Lo primero es la manera de implementar la pérdida, ya que ahora no será una tabla en la que se
almacenen los valores Q(s, a), sino que será una red neuronal la que se encargue de aproximar
estos valores. De manera que la pérdida se calcula como:
Li (θi) = Es,a∼ρ(·)
[
(yi − Q (s, a; θi))2]
, (4.7)
donde yi = Es′∼ε [r + γ máxa′ Q(s′, a′; θi−1)|s, a] es el objetivo para la iteración i, y ρ(s, a) es
una distribución de probabilidad sobre las secuencias, s y las acciones, a, a la que nos referimos
como la distribución de comportamiento.
Los parámetros de la iteración anterior, θi−1, se mantienen fijos al optimizar la función de pérdida,
Li(θi), la cual no deja de ser una pérdida cuadrática como la presentada en la ecuación 2.36 entre
el valor predicho por la red y el valor que debería tener.
4.2.1.3. Red Objetivo
En la función de pérdida presentada en la ecuación 4.7, no solo se consigue optimizar la red de forma que en cada iteración consigue predecir mejor los valores de la recompensa, sino que además
se introduce el concepto de una red objetivo o copia. Esta red sirve principalmente para mantener
57CAPÍTULO 4. METODOLOGÍA
la estabilidad en el entrenamiento. De manera que θi es la red donde se van haciendo las actualizaciones en cada iteración y θi−1 es la red objetivo que se va actualizando cada más iteraciones.
4.2.1.4. Gradiente
El gradiente de dicha pérdida se quedaría como:
∇θi Li (θi) = Es,a∼ρ(·);s′∼E
[(
r + γ máx
a′ Q (s′, a′; θi−1
) − Q (s, a; θi)
)
∇θi Q (s, a; θi)
]
.
(4.8)
4.2.1.5. Deadly Triad
Teóricamente este algoritmo no está asegurado a converger, ya que cumple la deadly triad [49],
al mezclar aproximadores de funciones no lineales, con un algoritmo de optimización basado en
diferencias temporales y ser off-policy. En la práctica se ha demostrado que con ciertas técnicas,
como el uso de un replay buffer y target networks, el algoritmo converge y es capaz de aprender a
resolver tareas complejas.
4.2.1.6. Replay Buffer
Se trata de una técnica que se utiliza para mejorar la eficiencia del entrenamiento. Consiste en
almacenar las transiciones que se van produciendo en el entorno en un buffer. De este buffer se
muestrean las transiciones de manera uniforme para poder entrenar al agente. Este buffer tiene una
capacidad finita, por lo que cuando se llena, se van eliminando las transiciones más antiguas.
4.2.1.7. Exploración versus Explotación
El algoritmo DQN utiliza una estrategia, ε-greedy expresado en la ecuación 2.22, para balancear
la exploración y la explotación, donde un mayor ε hace que el agente tienda a elegir una acción
aleatoria antes que la acción que le produce más recompensa. Inicialmente, ε es alto para fomentar
la exploración, y disminuye gradualmente para favorecer la explotación de las políticas aprendidas.
El algoritmo de la DQN se puede observar en el bloque 6.
De manera intuitiva, ahora mismo la representación de los valores, Q, para cada tupla de estadoacción, pasaría de tener una visualización como en la Figura 2.8, a como la vista en la Figura 4.2.
En esta figura se quiere dar a entender que, gracias al uso de las redes neuronales, ya no es necesario
construir una tabla explícita, sino que se puede estimar de manera precisa cuál es la recompensa
esperada de cada tupla estado-acción. La capacidad que tienen las redes neuronales de extraer
características permite comprender las tuplas de manera conjunta y realizar agrupaciones de datos
complejos, como se observa en la imagen.
584.2. ALGORITMOS DE RL PARA RESOLVER EL PROBLEMA
Algorithm 6 DQN
1: Inicializar la memoria de reproducción D con capacidad N
2: Inicializar la función de valor de acción Q con pesos aleatorios
3: for episodio = 1, M do
4: Inicializar la secuencia s1 = {x1}.
5: for t = 1, T do
6: Con probabilidad ε seleccionar una acción aleatoria at
7: de lo contrario seleccionar at = máxa Q∗(st, a; θ)
8: Ejecutar la acción at en el emulador y observar la recompensa rt e imagen xt+1
9: Establecer st+1 = st, at, xt+1
10: Almacenar la transición (st, at, rt, st+1) en D
11: Muestrear un minibatch aleatorio de transiciones (sj , aj , rj , sj+1) de D
12: Establecer yj =
{
rj si sj+1 es terminal
rj + γ máxa′ Q(sj+1, a′; θ) si sj+1 no es terminal
13: Realizar un paso de descenso de gradiente en (yj − Q(sj , aj ; θ))2 según la Ecuación 4.8
14: end for
15: end for
Figura 4.2: Visualización tabla Q DQN.
4.2.2. Trust Region Policy Optimization y Proximal Policy Optimization
Estos dos algoritmos difieren mucho de los que había en DRL, ya que en lugar de aproximar
funciones de valor, y a raíz de esto, elegir la acción más probable, se aproxima directamente la
política.
4.2.2.1. Naturaleza
Estos algoritmos están basados en los Policy Gradients y la ecuación base 2.26 que estos presentan, por tanto son on-policy, y model-free. En la literatura no encontramos un consenso sobre si
estos métodos son actor-crítico o basados en política. Ambos heredan la estructura del algoritmo
REINFORCE presentado en la sección 2.4.2.2, sin embargo añaden una red neuronal de la función
59CAPÍTULO 4. METODOLOGÍA
de valor para hacer la optimización. Por tanto, los consideraremos métodos actor-crítico.
En lugar de usar directamente la función, Rt, se emplea la ventaja, ˆAt, definida en la siguiente
ecuación: ˆAt = Rt − V (st), (4.9)
donde V (st) es la función de valor del estado st. Esta ventaja indica cuánto mejor es una acción
en comparación con la media de las acciones posibles en un estado concreto. La función, V (st),
puede estimarse directamente con una red neuronal o indirectamente mediante métodos iterativos
de la función de valor. Es esto lo que le da a TRPO y PPO un punto de vista de actor-crítico: estimar
la política, π, y la función de valor, V (s).
4.2.2.2. Beneficios
Buscan mejorar varios problemas asociados con los Policy Gradients. Uno de los principales desafíos es la difícil elección de parámetros, como la tasa de aprendizaje. Además, los cambios significativos en la política pueden causar divergencias en el entrenamiento, lo que impide que el agente
aprenda adecuadamente. PPO y TRPO mitigan este problema limitando la magnitud del cambio
en la política, asegurando actualizaciones más graduales y estables.
Otro problema es la eficiencia en términos de muestras. Al ser métodos on-policy, TRPO y PPO no
pueden reutilizar las muestras de entrenamiento, lo que incurre una mayor complejidad comparado
con métodos off-policy. Necesitan generar nuevas muestras para cada actualización de la política,
haciendo el proceso de entrenamiento más costoso en términos de uso de datos.
4.2.2.3. Trust Region Policy Optimization
El TRPO, basa la función a optimizar en [35], definiendo el objetivo de la siguiente forma:
max
θ
ˆEt
[ πθ (at | st)
πθold (at | st) ˆAt
]
subject to ˆEt [KL [πθold (· | st) , πθ (· | st)]] ≤ δ.
(4.10)
La intuición detrás de esta ecuación es que, la nueva distribución de pesos, θ, no se aleje demasiado
de la distribución anterior, θold, de manera que se garantiza que las actualizaciones de la política
sean suaves y no provoquen divergencias. Con la función objetivo 4.10, se busca que la política
de los nuevos pesos maximice las acciones que tienen ventaja positiva, y que la divergencia de
Kullback-Leibler entre las dos políticas no sea mayor que un valor, δ. La divergencia de KullbackLeibler es una medida de la diferencia entre dos distribuciones de probabilidad. Además, dentro
se implementa el importance sampling, representado como la fracción entre la política actual, πθ,
y la anterior, πθold , técnica utilizada para poder estimar la ventaja de la nueva política utilizando
las muestras de una política anterior. El problema que tiene TRPO es que es necesario calcular el
operador de Hessiano, lo cual es computacionalmente costoso. Por tanto, se propone una versión
simplificada de este algoritmo, que es el PPO.
604.2. ALGORITMOS DE RL PARA RESOLVER EL PROBLEMA
4.2.2.4. Proximal Policy Optimiaztion
En lugar de darle importancia a la diferencia entre distribuciones con la divergencia KullbackLeibler, partimos de la siguiente:
LCP I (θ) = ˆEt
[ πθ (at | st)
πθold (at | st) ˆAt
]
= ˆEt
[
rt(θ) ˆAt
]
, (4.11)
donde definimos rt(θ) como el ratio de las políticas, que es la fracción de probabilidad de la política
actual y la anterior. Por tanto, se nos queda la siguiente función de pérdida:
LCLIP (θ) = ˆEt
[
mín
(
rt(θ) ˆAt, clip (rt(θ), 1 − ε, 1 + ε) ˆAt
)]
, (4.12)
donde ε es un hiperparámetro que controla la magnitud del cambio en la política. Si el ratio de las
políticas es mayor que 1 + ε, se recorta a 1 + ε, y si es menor que 1 − ε, se recorta a 1 − ε. De esta
manera, se garantiza que la política no cambie demasiado en cada iteración, evitando divergencias
en el entrenamiento.
4.2.2.5. Generalized Advantage Estimation
La Estimación de la Ventaja Generalizada (GAE) [50] es una técnica utilizada en PPO para evaluar
mejor qué tan buena es una acción en un estado. En lugar de considerar solo las recompensas
inmediatas, GAE utiliza un factor de descuento, λ, para combinar recompensas futuras y actuales
de manera más sofisticada. A diferencia del descuento exponencial tradicional utilizado por el
factor, γ, GAE aplica un promedio ponderado exponencialmente. Esto suaviza las fluctuaciones, y
proporciona una estimación más precisa y estable de la ventaja, ayudando a PPO a aprender mejores
políticas de manera más eficiente. Al ajustar el factor, λ, podemos controlar cuánto queremos
confiar en las recompensas a largo plazo frente a las inmediatas, logrando un equilibrio que mejora
la calidad del aprendizaje del agente.
El algoritmo de PPO se puede observar en el bloque 7.
Algorithm 7 PPO, Estilo Actor-Crítico
1: for episodio = 1, 2, ... do
2: for actor = 1, 2, ..., N do
3: Ejecutar la política πθold en el entorno durante T pasos de tiempo
4: Calcular las estimaciones de ventaja ˆA1, . . . , ˆAT
5: end for
6: Optimizar el objetivo LCP I con respecto a θ, con K épocas y tamaño de minibatch M ≤
N T
7: θold ← θ
8: end for
Un ejemplo para una intuición de como funciona este algoritmo se observa en la Figura 4.3, donde
podemos ver un ejemplo en el que un agente debe subir una montaña, y se observa como sigue tres
principales caminos en diferentes episodios.
Se puede observar como la política, π, va mejorando a lo largo de los episodios, pero sin realizar
cambios bruscos. La flecha roja indica una primera inicialización del agente donde la solución no
61CAPÍTULO 4. METODOLOGÍA
es la óptima. La flecha amarilla es un siguiente episodio donde el agente ha aprendido a subir un
poco la montaña, y así obtener mejores recompensas. La flecha verde, y última, es a la que converge
el algoritmo, siendo así la política óptima.
De esta figura podemos extraer como el PPO, en lugar de poder aprender directamente aquella
política verde, es más conservador y va escalando poco a poco la montaña. Esto, que a priori no
parece eficiente, es necesario ya que cambios drásticos en la distribución de pesos de la política
pueden llevar al agente a no converger.
Figura 4.3: Ejemplo PPO.
4.3. Implementación
Para la implementación de los algoritmos, hemos utilizado la librería de aprendizaje por refuerzo
pytorchrl [51], una herramienta de código abierto que incluye los algoritmos de aprendizaje por refuerzo más comunes. Sin embargo, debido a su simplicidad, el algoritmo DQN se ha implementado
desde cero. En contraposición, hemos decidido implementar PPO utilizando una librería externa
debido a su mayor complejidad y necesidad de precisión. Además, esta elección facilita la futura
incorporación de otros algoritmos.
En la Figura 4.4 se muestra cómo hemos integrado Giulia en la librería torchrl. Los elementos en
verde representan las partes relacionadas con Giulia, mientras que los elementos en rojo corresponden a la librería torchrl. Esta librería permite diseñar un entorno con el cual el agente interactúa, y
hemos integrado Giulia en esta parte del código.
Para la comunicación entre todos los módulos, utilizamos tensordict, una librería desarrollada por el
mismo autor de pytorchrl. Tensordict permite la comunicación mediante diccionarios de tensores,
ofreciendo una gran versatilidad para aplicar funciones a los tensores y trabajar con ellos en la
GPU.
Además, hemos diseñado un diagrama de flujo que se aplica a ambos casos de uso. En la Figura 4.5,
se presenta el flujo de trabajo que seguimos para entrenar con el algoritmo PPO utilizando la librería
624.3. IMPLEMENTACIÓNTORCHRL
ENTORNO ALGORITMO
INICIALIZACIÓN
RESET
PASO
AGENTE
ACCIÓN
PÉRDIDA
TENSORDICT
Figura 4.4: Sistema de entorno y agente.
pytorchrl.Observaciones
Despliegue UEs
Despliegue UAV
Acciones
Agente
NO SI
CÁLCULO VENTAJA
ENTRENAMIENTO
DE RED NEURONAL
EVALUACIÓN SOBRE
ENTORNO
RESETEO DE ENTORNO
Movimiento UAV
Movimiento UEs
¿Paso final?
SI NO
¿Episodio
Final?
Guardar Modelos
Figura 4.5: Diagrama de flujo del entrenamiento PPO con torchrl.
63CAPÍTULO 4. METODOLOGÍA
4.3.1. Detalles de Implementación
En esta sección, explicaremos algunos detalles avanzados sobre la implementación del algoritmo
PPO.
4.3.1.1. Selección de Acciones
Debido a la naturaleza del algoritmo PPO, es necesario definir un método más complejo para seleccionar acciones en un espacio continuo. La red neuronal genera una distribución de probabilidad
como salida, y se selecciona un valor de esta distribución.
La distribución de probabilidad generada es una distribución normal, caracterizada por una media
y una desviación estándar. Por lo tanto, la red neuronal debe devolver la media y la desviación
estándar para cada dimensión de la acción. En este caso, dado que las acciones son el ángulo y la
velocidad, la red neuronal devolverá cuatro valores.
Durante el entrenamiento, para fomentar la exploración, se seleccionan acciones muestreadas de la
distribución normal basada en la media y la desviación estándar. En cambio, durante la inferencia,
se selecciona la acción con la mayor probabilidad, que corresponde a la media de la distribución.
4.3.1.2. Entropía para La Selección de Acciones
Para promover la exploración de estados y acciones en PPO, se introduce un nuevo hiperparámetro llamado entropy coefficient. Este coeficiente incrementa la desviación estándar al decidir las
acciones, haciendo que el agente, durante el entrenamiento, tienda a tomar acciones más diversas
y alejadas de la media de la distribución.
4.4. Casos de Uso
En esta sección presentamos los diferentes casos de uso en los que los agentes van a ser entrenados.
Se distingue entre movimiento de UEs e inicialización de los agentes.
4.4.1. Movimiento UEs
En esta subsección explicamos los diferentes tipos de movimientos que van a seguir los UEs en
nuestro entorno. El entorno en el que se pueden desplazar está delimitado en un rectángulo de 300m
x 300m.
4.4.1.1. UEs Estáticos
Como mencionamos anteriormente, inicializamos 10 UEs distribuidos uniformemente en un área
de 10m x 10m. Esta región de inicialización se extiende desde x = 0 hasta x = 10 y desde y = 0
hasta y = 10. En este escenario, los UEs permanecen en sus posiciones iniciales y no se mueven.
644.4. CASOS DE USO
4.4.1.2. UEs Dinámicos
Una vez realizado el estudio sobre los UEs estáticos, es interesante avanzar un poco más en el
objetivo del trabajo, estos patrones se acercan más a los que se pueden observar en la realidad. Se
plantean dos nuevos movimientos para los UEs:
Movimiento lineal: En este caso el clúster de UES comienza centrado en (0,0), avanza hacia
el punto (300,0). Cambia de sentido hacia la izquierda hasta llegar a (-300,0) y, por último,
vuelve a la posición (0,0).
Movimiento circular: En este caso los UEs siguen un movimiento circular con el centro en
(0,0) y un radio de 200m. En este caso, el clúster de UEs está inicializado en (0,-200),
El agente debe ser capaz de seguir el movimiento lineal y el circular.
4.4.2. Inicialización Agentes
Dentro de los escenarios mostrados previamente, tenemos que seleccionar tambien donde inicializamos el agente. Esta decisión va a ser de vital importancia a la hora de determinar si el agente
aprende o no.
4.4.2.1. Inicialización Estática
Durante el entrenamiento y la evaluación el agente se mantiene en la misma posición. Este tipo de
inicialización se utiliza como una primera aproximación al problema, para comprender si es capaz
de resolver una tarea más sencilla.
4.4.2.2. Inicialización Aleatoria
Con el objetivo de saber si estos algoritmos son capaces de generalizar. Durante el entrenamiento
se inicializa el agente en posiciones aleatorias de dentro del rectángulo sobre el que trabajamos.
Se entrena durante los episodios que sea necesario y, después, se realiza una evaluación. Esta
evaluación por norma general será fija, es decir, a pesar de estar entrenando en posiciones aleatorias
se comprueba el rendimiento del agente siempre desde la misma posición o conjunto de posiciones.
Por ejemplo, realizamos 100 episodios con esta inicialización aleatoria. Posteriormente ponemos al
modelo en modo evaluación, con el objetivo de no contaminar los datos de entrenamiento, y vemos
si sabe llegar al clúster de UEs si lo ponemos en la posición (0,100). Si este episodio es uno de los
primeros que necesita el modelo para aprender, lo común es que el agente no sepa llegar a los UEs.
Una vez se vayan avanzando en los episodios deberíamos observar como en estas evaluaciones
el agente si es capaz de llegar. De esta manera se comprueban si el agente aprende a medida que
avanzan los episodios.
65CAPÍTULO 4. METODOLOGÍA
4.5. Parámetros e Hiperparámetros
En esta sección vamos a introducir los parámetros utilizados para el simulador y los hiperparámetros con los que hemos implementado los algoritmos. Los parámetros son esenciales para definir
las condiciones del entorno de simulación, mientras que los hiperparámetros son cruciales para el
ajuste y el rendimiento de los algoritmos de aprendizaje reforzado.
4.5.1. Parámetros
Los parámetros del simulador se definen en la tabla 4.1. Estos incluyen el número de UEs y drones,
el modelo de referencia, el tipo de entorno, y la duración de la simulación.
Parámetro Valor
Número de UEs 10
Número de drones 1
Modelo 3GPP TR 36.814
Tipo de Entorno Rectangular
Longitud Temporal Simulación 128
Tabla 4.1: Parámetros del simulador.
4.5.2. Hiperparámetros
Debido a las diferencias de los algoritmos, para un correcto funcionamiento, es necesario implementar diferentes hiperparámetros en cada uno de ellos. A continuación, se detallan los hiperparámetros utilizados para los algoritmos DQN y PPO en las tablas 4.2 y 4.3, respectivamente. Los
hiperparametros óptimos han sido seleccionados a partir de [29] y [36] para DQN y PPO respectivamente.Además de una validación empírica basada en un grid-search de múltiples valore.
4.5.2.1. DQN
La tabla 4.2 muestra los hiperparámetros utilizados para el algoritmo DQN. Estos incluyen detalles
sobre la memoria, las capas ocultas, las neuronas por capa, la función de activación, la tasa de
aprendizaje, y otros parámetros específicos de la técnica ε-greedy.
4.5.2.2. PPO
La tabla 4.3 muestra los hiperparámetros utilizados para el algoritmo PPO. incluyen detalles sobre
la memoria, las capas ocultas, las neuronas por capa, la función de activación, la tasa de aprendizaje,
el tamaño de batch y minibatch, las épocas de entrenamiento, y otros parámetros específicos del
método PPO como ε de recorte y entropía.
664.5. PARÁMETROS E HIPERPARÁMETROS
Hiperparámetro Valor
Tamaño de Memoria 4
Capas Ocultas 2
Número de neuronas por capa 256
Función de Activación ReLU
Tasa de Aprendizaje 1e-4
Tamaño de Batch 128
Tasa de ε-greedy inicial 0.9
Tasa de ε-greedy final 0.05
Decaimiento por episodio de tasa de ε-greedy 8,5e−5
Factor de descuento, γ 0.99
Tabla 4.2: Hiperparámetros del algoritmo DQN.
Hiperparámetro Valor
Tamaño de Memoria 4
Capas Ocultas 3
Número de neuronas por capa 128
Función de Activación ReLU
Tasa de Aprendizaje 1e-4
Tamaño de Batch 128
Tamaño de minibatch 64
Épocas de entrenamiento 15
Factor de descuento, γ 0.99
ε de recorte 0.2
Entropía de ε 0.02
Factor GAE, λ 0.95
Tabla 4.3: Hiperparámetros del algoritmo PPO.
67CAPÍTULO 4. METODOLOGÍA
68Capítulo 5
Resultados
5.1. User Equipments Estáticos
En esta sección vamos a comparar los resultados obtenidos con DQN y PPO, en el caso en el que
los UEs no se desplazan. Esto se hace con el objetivo de hacer un cribado para decidir que modelo
nos interesa para los siguientes casos de uso.
5.1.1. Inicialización Estática
En la primera fila de imágenes de la Figura 5.1 se muestran los resultados de entrenamiento en
el entorno explicado previamente, pero con el agente siempre inicializado en (-100,0), tanto en el
entrenamiento como en la evaluación. Como hemos introducido en la sección 4.4.2, esto se hace
con el objetivo de saber si nuestro agente puede o no resolver este problema.100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 0
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 1000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 2000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 3000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 0
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 1000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 2000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 3000
Posición Inicial del Dron
Trayectoria del Dron
UEs
Figura 5.1: Comparación de trayectorias PPO y DQN.
A simple vista se observa que ambos modelos, DQN y PPO, convergen de una manera rápida, a los
69CAPÍTULO 5. RESULTADOS
1000 episodios ya tienen un conocimiento de donde se encuentran los UEs. El modelo PPO muestra
una mayor precisión a la hora de mantenerse estático encima de los usuarios, principalmente por
su naturaleza basada en un espacio de acciones continuo. En DQN, si el agente toma alguna acción
incorrecta, se estará desplazando 10 metros. Sin embargo, si en PPO se toma una decisión incorrecta
es más probable que la acción tomada sea menor a esos 10 metros. Las gráficas de recompensa para
ambos algoritmos se presentan en la Figura 5.2.
5.1.2. Inicialización Aleatoria
Además, también llevamos a cabo esta comparación en el caso de inicialización aleatoria, donde el
aprendizaje es más complejo. Como se observa en la Figura 5.2, no hay una gran diferencia en los
resultados entre entrenar con aleatoriedad o no, lo que indica que el agente aprende y sabe llegar
al centro desde cualquier posición. Por tanto, es capaz con los parámetros de entrada del modelo,
que no incluyen las posiciones de los usuarios, de tener una representación de donde se encuentran
los usuarios.0 500 1000 1500 2000 2500 3000
Episodios
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Recompensas
Recompensas media por episodio de DQN
Recompensas DQN Random
Recompensas DQN No Random
(a) Recompensa media por episodio DQN.0 500 1000 1500 2000 2500 3000
Episodios
2
0
2
4
6
8
10
Recompensas
Recompensas medias por episodio de PPO
Recompensas PPO No Random
Recompensas PPO Random (b) Recompensa media por episodio PPO.
Figura 5.2: Comparación de recompensas entre DQN (fila de arriba) y PPO (fila de abajo).
Un aspecto de particular importancia son los valores de recompensas que se aprecian para los
modelos DQN y PPO. En el caso de DQN la recompensa media máxima tiene un valor de 1.4,
mientras que en PPO el máximo de recompensa tiene un valor de aproximadamente 9. Volvemos
a lo mencionado previamente, esto se debe a la naturaleza del propio DQN, al estar incializándose
en (-100,0), solo tiene una resolución de movimiento, d. Por tanto no puede llegar a converger al
punto donde se consigue la mayor recompensa, en este caso esa posición es la cercana a (5,5).
5.2. UEs en Movimiento
Como se observa en la sección anterior, el algoritmo DQN tiene un carácter más rígido, principalmente heredado por la discretización a la hora de seleccionar las acciones, por lo que para una
mayor adaptación a los patrones de movimiento del dron a los de los UEs he decidido utilizar PPO
en casuísticas más complejas.
705.2. UES EN MOVIMIENTO
5.2.1. Movimiento Lineal
En la Figura 5.3, se puede observar el ejemplo actual. En este caso, los UEs se desplazan de derecha a izquierda inicializados en la posición (0,0), llegando al extremo derecho (300,0) luego al
izquierdo (-300,0) y volviendo al centro (0,0), como se explicó en la subsección 4.4.1.1. Además,
los UEs están inicializados en un clúster de 10x10.
5.2.1.1. Inicialización Estática
Como se ha realizado en el ejemplo anterior, primero se realizan experimentos con el agente inicializado en la misma posición en entrenamiento y en evaluación. En la Figura 5.3, se puede observar
el caso en el que el agente se inicializa en (-100,0). La flecha azul indica el movimiento que tiene
el conjunto de los usuarios.
El agente muestra un claro aprendizaje conforme avanzan los episodios, siendo capaz de posicionarse exactamente encima de los UEs. Con 2000 episodios, el dron todavía no es capaz de entender
que los UEs se mueven de derecha a izquierda. La situación cambia con 4000 episodios donde el
dron sigue la misma trayectoria que los UEs.
En la Figura 5.4, se muestran las gráficas correspondientes con la distancia media a los UEs (medida
únicamente con respecto a los ejes x e y, ya que el z no tiene movimiento), además de la recompensa
media por episodio que tiene el agente. Este resultado nos ayuda a entender que el dron llega a
converger a una solución independientemente de la posición de inicialización. De esta forma el
dron es capaz de encontrar a los UEs y seguirles desde cualquier posición inicial.
Se observa como la distancia media tiene una variación muy grande hasta que llega al episodio
4000, en el cual ya converge a una política buena. Si miramos la gráfica de la recompensa media
obtenida en el episodio 4000, todavía tenemos una política que puede mejorar. Esto se debe a
que la posición óptima está extremadamente cerca de los UEs, por tanto el agente puede seguir
optimizando las decisiones a tomar para que así se encuentre en todo momento exactamente encima
de ellos.
5.2.1.2. Inicialización Aleatoria
Al igual que se realizó en la experimentación de los UEs estáticos para comprobar el aprendizaje
del agente. En este caso también se han implementado simulaciones para determinar la capacidad
del dron para encontrar soluciones óptimas iniciándolo desde posiciones aleatorias. Estos resultados se observan en la Figura 5.5. Analizado dichos resultados, se puede observar que gracias a la
inicialización aleatoria durante el entrenamiento, el agente aprende a resolver todas las posiciones
de la misma manera. Esto es un avance, ya que no necesitamos un entrenamiento especifico para cada posición de salida del dron. En consecuencia, independientemente de la inicialización, el
dron aprende el movimiento de los usuarios alcanzando en todo momento la máxima recompensa. Este hallazgo difiere respecto a los resultados obtenidos con inicialización estática, donde la
dependencia de dicha inicialización es excesivamente elevada en lo concerniente a la recompensa
media obtenida. Por tanto, concluimos que el agente sabe aproximarse y seguir a los usuarios desde
cualquier punto del escenario.
71CAPÍTULO 5. RESULTADOS
(a) Episodio 0. (b) Episodio 2000.
(c) Episodio 4000. (d) Episodio 6000.
Figura 5.3: Visualización de episodios con posición de inicialización (0, -100).
5.2.2. Movimiento Circular
Después de obtener los resultados del movimiento lineal, se decide realizar el circular. Al suponer
este un mayor reto para los UEs, debido a que necesitan realizar acciones más dispares que en el
ejemplo anterior. Es decir, tienen que ir cambiando la trayectoria poco a poco para poder seguir el
movmiento circular. Sin embargo, cuando es lineal puede mantener la acción de seguir recto hasta
que llega al extremo.
5.2.2.1. Inicialización Estática
En la Figura 5.6, se muestra como el agente es capaz de seguir a los UEs cuando estos tienen un
movimiento circular. Indicar que el agente siempre parte de la posición inicial (0,0). Se observa
725.2. UES EN MOVIMIENTO0 1000 2000 3000 4000 5000 6000
Episodio
0
50
100
150
200
250
300
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (0, 100)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (-100, 0)
Inicialización en (100, 0)
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
0
2
4
6
8
Recompensa Media en Evaluación
Comparación de Recompensa Media en Evaluación
Inicialización en (0, 100)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (-100, 0)
Inicialización en (100, 0)
(b) Comparación de Recompensa Media en
Evaluación.
Figura 5.4: Comparación de métricas en diferentes inicializaciones durante entrenamiento
aleatorio.0 1000 2000 3000 4000 5000 6000
Episodio
0
50
100
150
200
250
300
350
400
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
2
0
2
4
6
8
Recompensa Media en Evaluación
Comparación de Recompensa Media en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
(b) Comparación de Recompensa Media en
Evaluación.
Figura 5.5: Comparación de métricas en diferentes inicializaciones
como al episodio 2000 ya es capaz de seguir a los UEs, el resto de episodios hasta el final, como
pasaba anteriormente, los utiliza para ir refinando las acciones que tiene que decidir.
Una representación más detallada de la distancia y la recompensa media se encuentra en la Figura 5.7, donde se observa lo comentado previamente. En el episodio 2000 el agente ya ha aprendido
a realizar un seguimiento de los usuarios, pero como sucedía en el caso de movimento lineal, esta
política todavía se puede mejorar para acercarse a la óptima.
5.2.2.2. Inicialización Aleatoria
A continuación pasamos a hacer el experimento en el que el agente se inicializa de manera aleatoria. Sin embargo, para evaluar, comparamos que pasaría si posicionamos al agente en diferentes
posiciones. En la Figura 5.7, se observan dichos valores además de compararse con el de la inicialización estática.
73CAPÍTULO 5. RESULTADOS
(a) Episodio 0. (b) Episodio 2000.
(c) Episodio 4000. (d) Episodio 6000.
Figura 5.6: Visualización de episodios con movimiento circular con inicialización en (0,0).
Tras observar los resultados, se concluye que para poder realizar un buen bucle de entrenamiento,
y que el agente pueda aprender a llegar desde todas las posiciones, con la misma rapidez con la
que lo hace en inicialización estática, se debería entrenar durante más episodios.
5.3. Experimentos de Ablación
El ajuste de los parámetros es una es una ardua tarea que requiere una gran cantidad de experimentos. Los parámetros implementados en las tablas presentadas anteriormente, Tablas 4.2 y 4.3, son
los definitivos que obtuvimos después de realizar múltiples entrenamientos. Esta sección explica
cómo afectan los diferentes parámetros a la convergencia del algoritmo PPO.
745.3. EXPERIMENTOS DE ABLACIÓN0 1000 2000 3000 4000 5000 6000
Episodio
100
200
300
400
500
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
Inicialización en (0,0) no random
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
2
0
2
4
6
Recompensa Media en Evaluación
Comparación de Recompensa Media en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
Inicialización en (0,0) no random
(b) Comparación de Recompensa Media en
Evaluación.
Figura 5.7: Comparación de métricas en diferentes inicializaciones, incluyendo inicialización
en (0,0) no random.
5.3.1. Número de Capas y Neuronas
La configuración óptima, presentada en la Tabla 4.3, consiste en 3 capas ocultas con 128 neuronas
por capa. Se ha observado que, con 2 capas ocultas y 64 neuronas por capa, el agente no puede
seguir a los UEs debido a su limitada capacidad de aprendizaje. Por otro lado, con 4 capas ocultas
y 256 neuronas por capa, aunque el agente puede seguir a los UEs, el tiempo de convergencia es
mayor.
5.3.2. Tasa de Aprendizaje y Número de episodios
La tasa de aprendizaje es un parámetro crucial en el entrenamiento de una red neuronal. Con una
tasa de aprendizaje de 1e-3, los cambios en la política son demasiado bruscos, impidiendo que
el agente siga a los UEs. Contrariamente, con una tasa de aprendizaje de 1e-5, los cambios son
demasiado pequeños y el agente tampoco puede seguir a los UEs. Una buena tasa de aprendizaje
es 3e-4. Con respecto al número de episodios tenemos que un mayor número de episodios permite
al agente resolver mejor el problema. Sin embargo, un número excesivo de episodios incrementa
considerablemente el tiempo de entrenamiento. Se ha comprobado que, con 6000 episodios en el
caso de los UEs móviles, el agente puede seguir a los UEs en casi todos los casos.
5.3.3. Entropía de Epsilon
Este hiperparámetro controla la entropía de la política y por ende, la exploración del agente. Un
valor de 0.02 logra un buen equilibrio entre exploración y explotación. Valores como 0.1 o 0.001,
hacen que el agente no pueda seguir a los UEs debido a una exploración excesiva o insuficiente,
respectivamente.
75CAPÍTULO 5. RESULTADOS
76Capítulo 6
Conclusiones
Esta trabajo ha abordado un problema relevante en el campo de las telecomunicaciones: el servicio
en situaciones de emergencia y accidentes, como los desastres naturales. Utilizando un dron capaz
de adaptarse de manera dinámica y eficiente a los movimientos cambiantes de los usuarios y a las
condiciones del entorno.
Mediante un enfoque matemático, se ha definido el problema de optimización necesario para conseguir el objetivo: asegurar que los UEs tienen las mejores prestaciones. Es importante señalar que
en la definición se han utilizando parámetros de entrada medibles por la estación base montada en
el mismo. A diferencia de la literatura existente, donde se utilizan parámetros no medibles como la
localización exacta de los usuarios. este trabajo se basa en datos realistas y accesibles (SINR, ángulo de llegada de la señal y posición actual del dron), pero también altamente variables y difíciles
de predecir, justificando la elección del RL para la solución propuesta.
En esta investigación se ha descrito el estado del arte del RL, y se ha demostrado la potencia del
DRL cuando se aplica al ámbito de las telecomunicaciones. En concreto, se ha desarrollado un
agente capaz de aprender diferentes patrones de movimiento de los UEs, utilizando el algoritmo
PPO, considerado uno de los más avanzados en la actualidad.
En más detalle, en este trabajo, se ha definido rigurosamente el problema de DRL-PPO, describiendo estados, acciones y la función de recompensa. Se ha trabajado intensamente en la adaptación
del simulador Giulia, utilizado como entorno para aumentar su velocidad de ejecución y permitir
su integración con la librería de torchrl. Se han comparado los resultados de PPO con la DQN,
y se ha demostrado que PPO converge hacia mejores soluciones debido a su capacidad de tomar
acciones en el espacio continuo. Se ha demostrado también cómo el algoritmo no solo es capaz de
converger cuando se entrena y se evalúa en el mismo escenario y con la misma inicialización, sino
que también es capaz de funcionar entrenando con posiciones iniciales del dron aleatorias.
Los resultados de este trabajo han demostrando cómo el dron puede proporcionar un servicio excelente a UEs en una posición fija, representando, por ejemplo, a un grupo de rescate en una zona
montañosa sin cobertura. Asimismo, se ha demostrado la capacidad de los agentes para funcionar
en situaciones con UEs en movimiento, como en un terremoto, donde las comunicaciones para los
bomberos son esenciales y el dron debe seguirlos para ofrecer la mejor cobertura posible.
Estos escenarios, aunque puedan parecer alejados de la realidad, reflejan la necesidad de comunicaciones perfectas en emergencias para que las fuerzas de seguridad actúen de manera eficiente. En
77CAPÍTULO 6. CONCLUSIONES
resumen, este trabajo ha demostrado que el uso de drones optimizados mediante algoritmos avanzados de DRL puede mejorar significativamente las telecomunicaciones en situaciones críticas,
aportando una solución innovadora y realista a los desafíos actuales en el campo.
Como futuras líneas de investigación en este trabajo, podríamos considerar el aumento de la variedad de movimientos sobre los que entrenamos a los agentes. Con el objetivo de constatar que
estos algoritmos convergen independientemente del patrón de movimiento de los UEs. Además, se
podrían realizar entrenamientos mezclando diferentes tipos de movimiento, comprobando así que
también son capaces de seguir a UEs en escenarios mucho más complejos. Otro enfoque sería estudiar la generalización de estos algoritmos, por ejemplo, entrenando a los agentes en un caso en el
que los UEs se desplazan horizontalmente y comprobar su desempeño si se cambia el movimiento
a vertical. Para aumentar el nivel de complejidad de la red y asemejarnos más a entornos reales, se
podría incrementar el número de clústers de usuarios y drones.
0Este texto es completamente transparente: 6G, UES, RL, DRL, ML, UAV, CNN, RNN, GAN,
MAC, SINR, DQN, DDPG, TDMA, BS, MDP, POMDP, SARSA, ReLU, A3C, TRPO, PPO, 3GPP,
PDP, GAE, MPC, ITU.
78Bibliografía
[1] Richard S Sutton y Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
[2] Harrison Kurunathan et al. “Machine learning-aided operations and communications of unmanned aerial vehicles: A contemporary survey”. En: IEEE Communications Surveys &
Tutorials (2023).
[3] Sarah M Hamylton et al. “Evaluating techniques for mapping island vegetation from unmanned aerial vehicle (UAV) images: Pixel classification, visual interpretation and machine
learning approaches”. En: International Journal of Applied Earth Observation and Geoinformation 89 (2020), pág. 102085.
[4] Qianqian Zhang, Aidin Ferdowsi y Walid Saad. “Distributed generative adversarial networks for mmWave channel modeling in wireless UAV networks”. En: ICC 2021-IEEE
International Conference on Communications. IEEE. 2021, págs. 1-6.
[5] Dinh Thai Hoang et al. Deep Reinforcement Learning for Wireless Communications and
Networking: Theory, Applications and Implementation. John Wiley & Sons, 2023.
[6] Faris B Mismar, Brian L Evans y Ahmed Alkhateeb. “Deep reinforcement learning for
5G networks: Joint beamforming, power control, and interference coordination”. En: IEEE
Transactions on Communications 68.3 (2019), págs. 1581-1592.
[7] Faroq Al-Tam, Noélia Correia y Jonathan Rodriguez. “Learn to schedule (LEASCH): A
deep reinforcement learning approach for radio resource scheduling in the 5G MAC layer”.
En: IEEE Access 8 (2020), págs. 108088-108101.
[8] Giorgio Stampa et al. “A deep-reinforcement learning approach for software-defined networking routing optimization”. En: arXiv preprint arXiv:1709.07080 (2017).
[9] Ying Liu, Junjie Yan y Xiaohui Zhao. “Deep reinforcement learning based latency minimization for mobile edge computing with virtualization in maritime UAV communication
network”. En: IEEE Transactions on Vehicular Technology 71.4 (2022), págs. 4225-4236.
[10] Chi Harold Liu et al. “Energy-efficient UAV control for effective and fair communication
coverage: A deep reinforcement learning approach”. En: IEEE Journal on Selected Areas
in Communications 36.9 (2018), págs. 2059-2070.
[11] Ruijin Ding, Feifei Gao y Xuemin Sherman Shen. “3D UAV trajectory design and frequency band allocation for energy-efficient and fair communication: A deep reinforcement
learning approach”. En: IEEE Transactions on Wireless Communications 19.12 (2020),
págs. 7796-7809.
79CAPÍTULO 6. CONCLUSIONES
[12] Rasmus V Rasmussen y Michael A Trick. “Round robin scheduling–a survey”. En: European Journal of Operational Research 188.3 (2008), págs. 617-636.
[13] Ronald A Howard. “Dynamic programming and markov processes.” En: (1960).
[14] Matthijs TJ Spaan. “Partially observable Markov decision processes”. En: Reinforcement
learning: State-of-the-art. Springer, 2012, págs. 387-414.
[15] Richard Bellman. “Dynamic programming”. En: science 153.3731 (1966), págs. 34-37.
[16] Xu Wang et al. “Deep reinforcement learning: A survey”. En: IEEE Transactions on Neural
Networks and Learning Systems (2022).
[17] Thomas M Moerland et al. “Model-based reinforcement learning: A survey”. En: Foundations and Trends® in Machine Learning 16.1 (2023), págs. 1-118.
[18] Sinan Çalışır y Meltem Kurt Pehlivanoğlu. “Model-free reinforcement learning algorithms:
A survey”. En: 2019 27th signal processing and communications applications conference
(SIU). IEEE. 2019, págs. 1-4.
[19] Christopher JCH Watkins y Peter Dayan. “Q-learning”. En: Machine learning 8 (1992),
págs. 279-292.
[20] Melanie Coggan. “Exploration and exploitation in reinforcement learning”. En: Research
supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University (2004).
[21] Richard S Sutton. “Learning to predict by the methods of temporal differences”. En: Machine learning 3 (1988), págs. 9-44.
[22] Richard S Sutton et al. “Policy gradient methods for reinforcement learning with function
approximation”. En: Advances in neural information processing systems 12 (1999).
[23] Ronald J Williams. “Simple statistical gradient-following algorithms for connectionist reinforcement learning”. En: Machine learning 8 (1992), págs. 229-256.
[24] Frank Rosenblatt. “The perceptron: a probabilistic model for information storage and organization in the brain.” En: Psychological review 65.6 (1958), pág. 386.
[25] Andrea Apicella et al. “A survey on modern trainable activation functions”. En: Neural
Networks 138 (2021), págs. 14-32.
[26] David E Rumelhart, Geoffrey E Hinton y Ronald J Williams. “Learning Internal Representations by Error Propagation, Parallel Distributed Processing, Explorations in the Microstructure of Cognition, ed. DE Rumelhart and J. McClelland. Vol. 1. 1986”. En: Biometrika
71 (1986), págs. 599-607.
[27] Wayne Xin Zhao et al. “A survey of large language models”. En: arXiv preprint arXiv:2303.18223
(2023).
[28] David Silver et al. “Mastering the game of Go with deep neural networks and tree search”.
En: nature 529.7587 (2016), págs. 484-489.
[29] Volodymyr Mnih et al. “Playing atari with deep reinforcement learning”. En: arXiv preprint
arXiv:1312.5602 (2013).
[30] Hado Van Hasselt, Arthur Guez y David Silver. “Deep reinforcement learning with double
q-learning”. En: Proceedings of the AAAI conference on artificial intelligence. Vol. 30. 1.
2016.
[31] Ziyu Wang et al. “Dueling network architectures for deep reinforcement learning”. En: International conference on machine learning. PMLR. 2016, págs. 1995-2003.
80[32] Tom Schaul et al. “Prioritized experience replay”. En: arXiv preprint arXiv:1511.05952
(2015).
[33] Timothy P Lillicrap et al. “Continuous control with deep reinforcement learning”. En: arXiv
preprint arXiv:1509.02971 (2015).
[34] Volodymyr Mnih et al. “Asynchronous methods for deep reinforcement learning”. En: International conference on machine learning. PMLR. 2016, págs. 1928-1937.
[35] John Schulman et al. “Trust region policy optimization”. En: International conference on
machine learning. PMLR. 2015, págs. 1889-1897.
[36] John Schulman et al. “Proximal policy optimization algorithms”. En: arXiv preprint arXiv:1707.06347 (2017).
[37] Georg Fischer, Florian Pivit y Werner Wiesbeck. “EISL, the Pendant to EIRP: A Measure
for the Receive Performance of Base Stations at the Air Interface”. En: Proc. European
Microwave Week. Milan, Italy, 2002, págs. 1-4.
[38] Holger Claussen et al. Small cell networks: deployment, management, and optimization.
John Wiley & Sons, 2017.
[39] Xiaoli Chu et al. Heterogeneous cellular networks: theory, simulation and deployment.
Cambridge University Press, 2013.
[40] John S Seybold. Introduction to RF propagation. John wiley & sons, 2005.
[41] A. Goldsmith. Wireless Communications. Cambridge University Press, 2005.
[42] 3GPP TR 36.814. Evolved Universal Terrestrial Radio Access (E-UTRA); Further Advancements for E-UTRA Physical Layer Aspects. Inf. téc. v 9.0.0.
[43] Claude Elwood Shannon. “A mathematical theory of communication”. En: The Bell system
technical journal 27.3 (1948), págs. 379-423.
[44] M Series. “Guidelines for evaluation of radio interface technologies for IMT-Advanced”.
En: Report ITU 638.31 (2009).
[45] NumPy Developers. Code Explanations. https : / / numpy . org / doc / stable / dev /
internals.code-explanations.html. Accessed: 2024-06-17.
[46] PyTorch Developers. PyTorch Documentation. https://pytorch.org/docs/stable/
index.html. Accessed: 2024-06-17.
[47] NumPy Developers. Numpy Einsum. https://numpy.org/doc/stable/reference/
generated/numpy.einsum.html. Accessed: 2024-06-17.
[48] Alan H Barr. “The Einstein summation notation”. En: An Introduction to Physically Based
Modeling (Course Notes 19), pages E 1 (1991), pág. 57.
[49] Hado Van Hasselt et al. “Deep reinforcement learning and the deadly triad”. En: arXiv preprint arXiv:1812.02648 (2018).
[50] John Schulman et al. “High-dimensional continuous control using generalized advantage
estimation”. En: arXiv preprint arXiv:1506.02438 (2015).
[51] Pytorch. Pytorchrl. https://pytorch.org/rl/stable/index.html. Accessed: 202406-17.
81




scholar link https://scholar.google.com/citations?user=U-WrIfQAAAAJ&hl=es&oi=ao


