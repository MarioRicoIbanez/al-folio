First, use this as the photo https://media.licdn.com/dms/image/v2/D4D03AQFJAK0pNXCm8Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1683541363898?e=1757548800&v=beta&t=_HCR45v1bahS_5truXJHSX_s-QTsddxIDSAr7sEUZN8

Download and upload it or even copy the link if it doesnt work. 


And then use all this context to carefully create the page with my full experience detailed




CV 
% AUTHOR: Sina Atalay
% LICENSE: https://github.com/sinaatalay/rendercv/blob/main/LICENSE

\documentclass[10pt, letterpaper]{article}

% Packages:
\usepackage[
        ignoreheadfoot, % set margins without considering header and footer
        top=2 cm, % seperation between body and page edge from the top
        bottom=2 cm, % seperation between body and page edge from the bottom
        left=2 cm, % seperation between body and page edge from the left
        right=2 cm, % seperation between body and page edge from the right
        footskip=1.0 cm, % seperation between body and footer
        % showframe % for debugging 
    ]{geometry} % for adjusting page geometry
\usepackage[explicit]{titlesec} % for customizing section titles
\usepackage{tabularx} % for making tables with fixed width columns
\usepackage{array} % tabularx requires this
\usepackage[dvipsnames]{xcolor} % for coloring text
\definecolor{primaryColor}{RGB}{0, 79, 144} % define primary color
\usepackage{enumitem} % for customizing lists
\usepackage{fontawesome5} % for using icons
\usepackage{amsmath} % for math
\usepackage[
    pdftitle={Mario Rico's CV},
    pdfauthor={Mario Rico},
    colorlinks=true,
    urlcolor=primaryColor
]{hyperref} % for links, metadata and bookmarks
\usepackage[pscoord]{eso-pic} % for floating text on the page
\usepackage{calc} % for calculating lengths
\usepackage{bookmark} % for bookmarks
\usepackage{lastpage} % for getting the total number of pages
\usepackage[default, type1]{sourcesanspro} % for using source sans 3 font
\usepackage{ifthen}

% Some settings:
\pagestyle{empty} % no header or footer
\setcounter{secnumdepth}{0} % no section numbering
\setlength{\parindent}{0pt} % no indentation
\setlength{\topskip}{0pt} % no top skip
\makeatletter
\let\ps@customFooterStyle\ps@plain % Copy the plain style to customFooterStyle
\patchcmd{\ps@customFooterStyle}{\thepage}{
\color{gray}\textit{\small Mario Rico - Page \thepage{} of \pageref*{LastPage}}}{}{} % replace number by desired string
\makeatother
\pagestyle{customFooterStyle}

\titleformat{\section}{
        % make the font size of the section title large and color it with the primary color
        \Large\color{primaryColor}
    }{
    }{
    }{
        % print bold title, give 0.15 cm space and draw a line of 0.8 pt thickness
        % from the end of the title to the end of the body
        \textbf{#1}\hspace{0.15cm}\titlerule[0.8pt]\hspace{-0.1cm}
    }[] % section title formatting

\titlespacing{\section}{
        % left space:
        0pt
    }{
        % top space:
        0.3 cm
    }{
        % bottom space:
        0.2 cm
    } % section title spacing

\newcolumntype{L}[1]{
    >{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}
} % left-aligned fixed width column type
\newcolumntype{R}[1]{
    >{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}
} % right-aligned fixed width column type
\newcolumntype{K}[1]{
    >{\let\newline\\\arraybackslash\hspace{0pt}}X
} % justified flexible width column type
\setlength\tabcolsep{-1.5pt} % no space between columns
\newenvironment{highlights}{
        \begin{itemize}[
                topsep=0pt,
                parsep=0.10 cm,
                partopsep=0pt,
                itemsep=0pt,
                after=\vspace{-1\baselineskip},
                leftmargin=0.4 cm + 3pt
            ]
    }{
        \end{itemize}
    } % new environment for highlights

\newenvironment{header}{
        \setlength{\topsep}{0pt}\par\kern\topsep\centering\color{primaryColor}\linespread{1.5}
    }{
        \par\kern\topsep
    } % new environment for the header

\newcommand{\placelastupdatedtext}{% \placetextbox{<horizontal pos>}{<vertical pos>}{<stuff>}
  \AddToShipoutPictureFG*{% Add <stuff> to current page foreground
    \put(
        \LenToUnit{\paperwidth-2 cm-0.2 cm+0.05cm},
        \LenToUnit{\paperheight-1.0 cm}
    ){\vtop{{\null}\makebox[0pt][c]{
        \small\color{gray}\textit{}\hspace{\widthof{Last updated in February 2024}}
    }}}%
  }%
}%

% save the original href command in a new command:
\let\hrefWithoutArrow\href
 % new command for external links:
\renewcommand{\href}[2]{\hrefWithoutArrow{#1}{\mbox{\ifthenelse{\equal{#2}{}}{ }{#2 }\raisebox{.15ex}{\footnotesize \faExternalLink*}}}}

\let\originalTabularx\tabularx
\let\originalEndTabularx\endtabularx

\renewenvironment{tabularx}{\bgroup\centering\originalTabularx}{\originalEndTabularx\par\egroup}

% For TextEntrys (see https://tex.stackexchange.com/a/600/287984):
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1\topsep=0pt\itemsep=0pt\parsep=0pt\parskip=0pt\labelwidth=0pt\itemindent=0pt\labelsep=0pt}\item[]}
\let\endchangemargin=\endlist 

% Ensure that generate pdf is machine readable/ATS parsable
\pdfgentounicode=1

\begin{document}
    \placelastupdatedtext
    \begin{header}
        \fontsize{30 pt}{30 pt}
        \textbf{Mario Rico Ibáñez}

        \vspace{0.3 cm}

        \normalsize
        \mbox{\hrefWithoutArrow{tel:+410766880725}{{\footnotesize\faPhone*}\hspace*{0.13cm}+41 076 688 07 25}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{mailto:mario.ricoibanez@epfl.ch}{{\small\faEnvelope[regular]}\hspace*{0.13cm}mario.ricoibanez@epfl.ch}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{https://www.linkedin.com/in/mario-rico-ibáñez-6b5888225/}{{\small\faLinkedinIn}\hspace*{0.13cm}}}
        \hspace*{0.5 cm}
        \mbox{\hrefWithoutArrow{https://github.com/MarioRicoIbanez}{{\small\faGithub}\hspace*{0.13cm}}}
        \hspace*{0.5 cm}
        \mbox{\faMapMarker \hspace*{0.13cm} Lausanne, Switzerland}
        \hspace*{0.5 cm}
    \end{header}

    \vspace{0.3 cm}

\section{Education}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{MSc}
    &
    \textbf{Computer Science}, École Polytechnique Fédérale de Lausanne
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item \textbf{Coursework}: Machine Learning, Modern Natural Language Processing, Reinforcement Learning, Mobile Networks, Intelligent agents, Algorithms, Distributed Algorithms
        \item \textbf{Projects:} Adversarial Attacks on Image-to-image Generative Models
        \item \textbf{GPA:5.36/6} .
    \end{highlights}
    &
    2024 to Current
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{BSc}
    &
    \textbf{Telecommunications Engineering}, Universitat Politècnica de València
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item \textbf{Specialization}: \textbf{Telecommunication Systems}
        \item \textbf{GPA}: \textbf{4}. 22/40 subjects with honours. \textbf{Top 3} out of 200+ students
        \item Participant in the \textbf{High Performance Group} with \textbf{English teaching}.

        \item \textbf{Coursework:} Algebra, Calculus, Probability and Random Signals, Digital Signal Processing, Digital and Analog Electronics, Information Theory, Spatial Communications, Mobile and Wireless Communications, Digital Communications, Telematics, Antennas, Electromagnetic Waves, Microwaves
        \item \textbf{Bachelor's thesis}: Deep Reinforcement Learning for UAV Base Station Dynamic Positioning
    \end{highlights}
    &
    2020 to 2024
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{Dip}
    &
    \textbf{Artificial Intelligence by SAMSUNG INNOVATION}, Universitat Politècnica de València
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Diploma equivalent to 35 ECTS
        \item \textbf{Coursework:} Maths for Data Science, Probability and Statistics, Python for Data Science, Machine Learning, Deep Learning, Natural Language Processing
        \item \textbf{Final Project}: Emotion Detection through audio and text \href{https://github.com/laurarimi/IA-project/tree/main}{Repo}
        \end{highlights}
    &
    Sept. 2022 to Dec. 2022
\end{tabularx}

\begin{tabularx}{\textwidth-0.4cm-0.13cm}{L{0.85cm} K{0.2cm} R{4.1cm}}
    \textbf{BSc}
    &
    \textbf{Mathematics}, Universidad Nacional de Educación a Distancia
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Currently enrolled in a long-term program
        \item \textbf{Coursework}: Discrete Maths, Formal Languages and Basic Geometry
    \end{highlights}
    &
    2023 to 2030
\end{tabularx}

    \section{Experience}
\begin{tabularx}{\textwidth-0.4cm-0.13cm}{K{0.2cm} R{4.1cm}}
    \textbf{Computer Vision \& Behaviour Analysis Lab}, Machine Learning Developer
    
    \vspace{0.10cm}
    
    \begin{highlights}
        \item Developed SOTA text models for \textbf{Multilabel Emotion Recognition} within a psychology context
        \item Conducted research on Deep Learning for Emotion Recognition, comparing \textbf{Encoder-only} Transformers (like BERT and RoBERTa) with \textbf{Decoder-only} Transformers (like Llama-2, Mistral, Qwen)
        \item Explored the \textbf{explainability} of \textbf{Large Language Models} in the context of Emotion Recognition


        
    \end{highlights}
    
    &  March 2023 to July 2024

\end{tabularx}
\vspace{0.1cm}
\begin{tabularx}{\textwidth-0.4cm-0.13cm}{K{0.2cm} R{4.1cm}}
     \textbf{Instituto de Telecomunicaciones y Aplicaciones Multimedia}, Machine Learning Developer
    
    

    
   \begin{highlights}
\item Developing 5G/6G multi-carrier, massive MIMO, \textbf{GPU-based} system-level \textbf{wireless communication simulator} (\textbf{PyTorch})
\item Utilizing \textbf{Deep Reinforcement Learning} strategies to optimize the placement of Base Stations
\item Investigating the \textbf{applicability} of Deep Reinforcement Learning techniques in telecommunications and endeavoring to implement algorithms across varied scenarios, including singular 4G layer, multi-layer environments, and several distributions of users.
\end{highlights}

& January 2024 to Current

\\
Supervised by Prof. Valery Naranjo Ornedo and Dr. David López Pérez

\end{tabularx}

\section{Publications}

Rico Ibáñez, Mario; del Amor, Rocío; Naranjo, Valery. Mejorando el Análisis de Emociones en texto mediante Llama 2. \textit{In: XLI Congreso Anual de la Sociedad Española de Ingeniería Biomédica}. Valencia: Universitat Politècnica de València, 2023. Pp. 670-673. ISBN: 978-84-17853-76-1 \href{https://repositorio.upct.es/handle/10317/13756}{Link}

\vspace{0.1cm}

Rico Ibáñez, Mario; Akhtarshenas, Azim; López-Pérez, David; Geraci, Giovanni. Optimizing Aerial Base Station Positioning Using Deep Reinforcement Learning in UAV Networks. \textit{International Conference on Communications}.


\section{Projects}

\begin{tabularx}{\textwidth-0.4 cm-0.13cm}{K{0.2 cm} R{4.1 cm}}

\textbf{Adversarial Attacks on Image-to-Image Generative Models}

\vspace{0.10 cm}

\begin{highlights}
    \item Investigated the robustness of \textbf{Image-to-Image generative models} against \textbf{adversarial attacks}, such as \textbf{Automatic Projected Gradient Descent}, analyzing vulnerabilities and defenses, such as \textbf{RobustCLIP}
    \item Developed adversarial strategies targeting models based on \textbf{Diffusion Models }, specifically assessing the stability of embeddings
    \item Conducted extensive experiments evaluating the performance impacts of adversarial perturbations across datasets
    \item Supervised by Prof. Volkan Cevher and Elias Abad Rocamora 
\end{highlights}

\end{tabularx}

\vspace{0.2 cm}

\begin{tabularx}{\textwidth-0.4 cm-0.13cm}{K{0.2 cm} R{4.1 cm}}

\textbf{Impact of Whole-Slide Image Resolution on Pathology Models}

\vspace{0.10 cm}

\begin{highlights}
    \item Systematically analyzed how changes in \textbf{Whole-Slide Image (WSI)} resolution affect the performance of histopathological foundation models
    \item Implemented a preprocessing pipeline to generate and validate tiles across varying magnification levels (5x, 10x, 20x, 40x), ensuring data integrity and consistency
    \item Utilized \textbf{Multiple-Instance Learning (MIL)} techniques, including mean pooling and gated attention mechanisms, to aggregate embeddings from the UNI model
    \item Conducted ablation studies on three breast histology datasets (BACH, BRACS, BreakHis) demonstrating the importance of magnification levels and model architecture choices
\end{highlights}

\end{tabularx}

\section{Additional Experience and Awards}

    \begingroup\leftskip=0.2 cm
    \advance\csname @rightskip\endcsname 0.2 cm
    \advance\rightskip 0.2 cm

    \textbf{IAESTE Faculty Delegate:} Served a full academic year as a delegate for the IAESTE organization, focusing on securing international job placements for students. Actively engaged with both students and employers, facilitating successful job matches and supporting students throughout their application processes \par\endgroup

    \vspace{0.2 cm}
    \begingroup\leftskip=0.2 cm
    \advance\csname @rightskip\endcsname 0.2 cm
    \advance\rightskip 0.2 cm

    \textbf{Honors at Upper Secondary School:} Achieved Honors in the last year\par\endgroup

    
    \section{Skills and Interests}

        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm

        \textbf{Programming:} Python, Pytorch, Tensorflow, Keras, Bash, Assembly, SystemVerilog, Matlab, Java(Beginner), Git, LaTeX, Docker \par\endgroup

        \vspace{0.2 cm}
        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm

        \textbf{Languages:} Spanish (Native), English (Fluent), French (B1)  \par\endgroup
         \vspace{0.2 cm}
        \begingroup\leftskip=0.2 cm
        \advance\csname @rightskip\endcsname 0.2 cm
        \advance\rightskip 0.2 cm
        \textbf{Interests:} Sports (Gym, Running, Tennis and Boxing in particular), Technological Advancements, Nature, Rubik Cube, Teaching \par\endgroup



    

\end{document}



RL PROJECT 06.2025 FOR EPFL COURSE EE-REINFORCEMENT-LEARNING
\documentclass{article}

\usepackage[final]{neurips_2024}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}  % for [H]
\usepackage{titling}  % if using \maketitle or custom title spacing
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}

% \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}  % LAST
\usepackage{xcolor}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
% \usepackage[colorlinks=true, linkcolor=mydarkblue, citecolor=mydarkblue, urlcolor=mydarkblue]{hyperref}

% \usepackage[round]{natbib}
\newcommand{\takeaway}[1]{\vspace{1mm}{\color[HTML]{1d5c38}\textbf{{$\triangleright$\hspace{5pt}#1}}}}
% \newcommand{\takeaway}[1]{\vspace{1mm}{\color{mydarkblue}\textbf{{$\triangleright$\hspace{5pt}#1}}}}

\title{Evolution Strategies for Deep RL
pretraining}

\author{
  Adrian Martínez López, Ananya Gupta, Hanka Goralija, Mario Rico Ibáñez, \\
  Saúl Fenollosa Arguedas, Tamar Alphaidze \\
  École Polytechnique Fédérale de Lausanne (EPFL)
}

\begin{document}

\maketitle
 \begin{abstract}
%In this project, we explore Evolution Strategies as both a standalone reinforcement learning method and a pretraining step for gradient-based algorithms like DQN and PPO. We tested these approaches on Flappy Bird, Breakout, and MuJoCo Walker environment, covering both discrete and continuous action spaces. Flappy Bird showed that ES-pretrained agents could learn good policies quickly and reach high rewards faster than DQN alone. In Breakout, ES struggled due to the high-dimensional image input, failing to make progress even with large populations. In MuJoCo, ES pretraining did not improve PPO performance, likely because PPO relies on separate actor and critic networks, while ES only optimizes a single policy. Overall, ES is useful for early exploration in simple environments, but its limitations become clear in more complex tasks.

Although Deep Reinforcement Learning has proven highly effective for complex decision-making problems, it demands significant computational resources and careful parameter adjustment in order to develop successful strategies. Evolution strategies offer a more straightforward, derivative-free approach that is less computationally costly and simpler to deploy. However, ES generally do not match the performance levels achieved by DRL, which calls into question their suitability for more demanding scenarios.
This study examines the performance of ES and DRL across tasks of varying difficulty, including Flappy Bird, Breakout and Mujoco environments, as well as whether ES could be used for initial training to enhance DRL algorithms. The results indicate that ES do not consistently train faster than DRL. When used as a preliminary training step, they only provide benefits in less complex environments (Flappy Bird) and show minimal or no improvement in training efficiency or stability across different parameter settings when applied to more sophisticated tasks (Breakout and MuJoCo Walker).
The code is on \href{https://github.com/talphaidze/Evolution-Strategies-for-Deep-RL-Pretraining}{GitHub}.
 \end{abstract}

%\begin{abstract}
%Deep Reinforcement Learning (DRL) has shown remarkable success in complex sequential decision-making tasks. However, this power comes with a cost, as it often requires extensive training and careful tuning to converge to effective policies. In contrast, Evolution Strategies (ES) have surfaced as a simpler, gradient-free alternative that is computationally cheaper and easier to implement. However, ES methods typically fall short of the performance achieved by DRL, raising questions about their applicability to more challenging environments. In this work, we compare the performance of ES and DRL across environments of varying complexity (Flappy Bird, Breakout and Mujoco) and analyze the trade-offs between these approaches. Additionally, we also investigate the use of ES as a pretraining strategy for DRL algorithms. Our findings show that ES is not consistently faster than DRL and that its effectiveness as a pretraining method is limited to simpler environments, offering little to no impact in training speed and robustness to hyperparameter selection in more complex tasks.
%\end{abstract}
\vspace{-10pt}
\section{Introduction}
\vspace{-5pt}
Deep Reinforcement Learning (DRL) has shown remarkable success in tackling complex sequential decision-making problems \citep{sutton2018reinforcement}. However, this power comes with a cost, as it often requires extensive training and careful tuning to converge to effective policies~\citep{dulacarnold2019challengesrealworldreinforcementlearning, ghosh2021generalizationrldifficultepistemic}. In contrast, Evolution Strategies (ES) have surfaced as a simpler, gradient-free alternative that is computationally cheaper and easier to implement~\citep{salimans2017evolution}. These algorithms estimate gradients by performing a search over small parameter perturbations, thus avoiding the need to perform expensive gradient computations. However, ES methods typically fall short of the performance achieved by DRL, raising questions about their applicability to more challenging environments.

In this work, we evaluate and compare the performance of these two distinct paradigms: \textbf{DRL}, with algorithms such as Deep Q-Networks (DQN)~\citep{mnih2013playingatarideepreinforcement} and Proximal Policy Optimization (PPO)~\citep{schulman2017proximalpolicyoptimizationalgorithms}, and \textbf{ES}. We perform this analysis across environments that differ in complexity and reward structure. Our experiments span two main domains: arcade-style games with discrete action spaces, such as Flappy Bird and Breakout, and continuous control tasks in high-dimensional state and action spaces, modeled using the Mujoco physics simulator \citep{todorov2012mujoco}. Additionally, we investigate the applicability of ES as a pretraining strategy for DRL algorithms, in order to increase training speed and robustness to hyperparameter selection. 

Our findings show that ES is not consistently faster than DRL approaches, denying the first claim about training speed enhancement. Additionally, pretraining strategies appear to be effective for simpler environments, while they show little to no benefit in more complex tasks. In simple environments such as Flappy Bird, pretraining with ES appears to accelerate the learning curve for DRL algorithms, reaching higher rewards faster than DRL-only approaches. Unfortunately, this behaviour is not consistent on more complex environments, such as Breakout or Mujoco, where ES pretraining does not accelerate training and does not either make DRL algorithms more robust to hyperparameter selection.

% \section{Introduction}

% Reinforcement Learning (RL) has shown remarkable success in tackling complex sequential decision-making problems \citep{sutton2018reinforcement}. In this project, we evaluate and compare the performance of three distinct RL approaches, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Evolution Strategies (ES), across environments that differ in complexity and reward structure. Our experiments span two main domains: arcade-style games such as Flappy Bird and Breakout, implemented using OpenAI Gym \citep{brockman2016openai} with discrete actions. On the other hand, we have continuous control tasks in high-dimensional state and action spaces, modeled using the MuJoCo physics simulator \citep{todorov2012mujoco}, in specific, we used the BRAX version of these environments, \citep{freeman2021braxdifferentiablephysics}. We analyze how traditional gradient-based methods like DQN and PPO perform relative to the gradient-free ES approach, particularly in scenarios with sparse rewards or deceptive gradients \citep{salimans2017evolution}. Additionally, we explore a hybrid training strategy, where ES is used for pre-training to enhance exploration before fine-tuning with gradient-based methods to improve sample efficiency. Our findings highlight the strengths, weaknesses, and synergies of these algorithms in terms of convergence speed, final performance, and sensitivity to hyperparameters.

\vspace{-5pt}
\section{Related Work}
\vspace{-5pt}
\subsection{Evolution Strategies}
\label{sec: ES}
Traditional reinforcement learning approaches typically optimize policy parameters using gradient-based methods, such as policy gradients or actor-critic algorithms \citep{sutton2018reinforcement}. These methods assume that the environment is smooth and differentiable with respect to actions, which allows gradients to propagate through trajectories using the chain rule. However, in many practical scenarios, such as physical simulations or real-world robotic systems, the environment may be non-differentiable, highly stochastic, or possess sparse and delayed rewards. In such cases, standard RL algorithms struggle to find effective policies \citep{dulac2019challenges}.

An alternative to gradient-based RL is to frame the learning problem as a black-box optimization task. In this setting, the policy is viewed as a mapping from states to actions, parameterized by $\theta \in \mathbb{R}^d$, and the objective is to maximize the expected cumulative reward obtained by executing the policy in the environment. Evolution Strategies (ES) offer a solution by searching for optimal parameters through random perturbations and selection based on observed rewards, without requiring access to environmental gradients or backpropagation \citep{salimans2017evolution}.


\subsubsection{Mathematical Formulation}

Let $F(\theta)$ denote the total reward achieved by running a policy with parameters $\theta$ in the environment. The goal is to find parameters $\theta$ that maximize $F(\theta)$. However, since $F$ is assumed to be a black-box function, meaning we do not have access to how the reward depends on individual actions or parameters, direct gradient computation is infeasible. To overcome this, Evolution Strategies optimize a smoothed version of the objective:

\begin{equation}
\tilde{J}(\theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}[F(\theta + \sigma \epsilon)],
\end{equation}

where $\sigma > 0$ is a small noise parameter and $\epsilon \sim \mathcal{N}(0, I)$ is a standard multivariate Gaussian random vector. This formulation introduces smoothing by averaging over nearby parameter vectors, thereby making the objective amenable to gradient estimation.

The gradient of $\tilde{J}$ with respect to $\theta$ can be derived as follows. By the linearity of expectation and the chain rule:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \mathbb{E}_{\epsilon}\left[ \nabla_\theta F(\theta + \sigma \epsilon) \right].
\end{equation}

Since $F$ is a black-box, we cannot directly compute $\nabla_\theta F(\theta + \sigma \epsilon)$. Instead, using the log-likelihood trick, we rewrite the gradient in terms of the score function:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \mathbb{E}_{\epsilon}\left[ F(\theta + \sigma \epsilon) \nabla_\theta \log p_\theta(\theta + \sigma \epsilon) \right],
\end{equation}

where $p_\theta(\cdot)$ is the probability density function of $\mathcal{N}(\theta, \sigma^2 I)$.

Since the Gaussian log-probability satisfies:

\begin{equation}
\log p_\theta(x) = -\frac{1}{2\sigma^2} \|x - \theta\|^2 + \text{const} \implies \nabla_\theta \log p_\theta(x) = \frac{x - \theta}{\sigma^2}.
\end{equation}

Substituting $x = \theta + \sigma \epsilon$ yields:

\begin{equation}
\nabla_\theta \log p_\theta(\theta + \sigma \epsilon) = \frac{\sigma \epsilon}{\sigma^2} = \frac{\epsilon}{\sigma}.
\end{equation}

Thus, the gradient becomes:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) = \frac{1}{\sigma} \mathbb{E}_{\epsilon} \left[ F(\theta + \sigma \epsilon) \cdot \epsilon \right].
\end{equation}

In practice, this expectation is approximated with Monte Carlo sampling. Given $n$ independent samples $\epsilon_1, \dots, \epsilon_n$, the gradient estimate is:

\begin{equation}
\nabla_\theta \tilde{J}(\theta) \approx \frac{1}{n \sigma} \sum_{i=1}^n F(\theta + \sigma \epsilon_i) \cdot \epsilon_i.
\end{equation}

\subsubsection{Algorithm}

The practical implementation of Evolution Strategies consists of sampling perturbations, evaluating the corresponding perturbed policies in the environment, estimating the gradient based on the collected rewards, and updating the policy parameters accordingly. The complete algorithm can be described as follows:

\begin{algorithm}
\caption{Evolution Strategies Algorithm}
\label{alg:es}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Initial parameters $\theta_0$, learning rate $\alpha$, noise standard deviation $\sigma$, population size $n$
\FOR{iteration $t = 0, 1, 2, \dots$}
    \STATE Broadcast $\theta_t$ to all workers
    \FOR{each worker $i = 1$ to $n$ in parallel}
        \STATE Sample perturbation $\epsilon_i \sim \mathcal{N}(0, I)$
        \STATE Compute perturbed parameters $\theta_i = \theta_t + \sigma \epsilon_i$
        \STATE Execute policy with $\theta_i$ and obtain reward $F_i$
    \ENDFOR
    \STATE Aggregate all $(F_i, \epsilon_i)$ pairs
    \STATE Estimate gradient: $g_t = \frac{1}{n\sigma} \sum_i F_i \epsilon_i$
    \STATE Update parameters: $\theta_{t+1} = \theta_t + \alpha g_t$
\ENDFOR
\end{algorithmic}
\end{algorithm}

Because the evaluations of perturbed policies are independent, Evolution Strategies are naturally suited for parallel computation. In practice, to reduce variance, rewards $F_i$ are often replaced by rank-transformed and mean-centered values, a technique known as fitness shaping.
The current parameters $\theta_t$ are broadcast once to all workers at the start of each iteration. Each worker independently samples noise vectors and computes rewards locally.

To minimize communication overhead, workers do not transmit full parameter vectors. Instead, they communicate only the random seeds used to generate $\epsilon_i$ and the corresponding scalar rewards $F_i$. Given a common random seed initialization, all workers can deterministically reconstruct the sampled perturbations. This design ensures that only $O(1)$ data per rollout is communicated, making the method scalable to very high-dimensional policy spaces.



Unlike policy gradient methods, which inject noise into the action space at every timestep, Evolution Strategies perturb the policy parameters once at the beginning of each episode. This leads to a gradient estimator whose variance is independent of the episode length, making ES particularly robust in long-horizon or sparse-reward environments. Additionally, since ES avoids backpropagation through time, its gradient computation is significantly lighter than that of policy gradient methods.
A drawback, however, is that ES requires complete episode rollouts to compute returns, so the overall update can be delayed if even a single episode within the population takes a long time to finish.


\vspace{-5pt}
\section{Methodology}
\vspace{-5pt}

We aim to evaluate and compare the performance of gradient-based deep reinforcement learning (DRL) algorithms with gradient-free evolution strategies (ES). Specifically, we investigate two hypotheses: (1) whether ES can achieve intermediate performance benchmarks (e.g., reaching 25\% of the optimal reward) faster than DRL algorithms, and (2) whether ES can serve as an effective pretraining method to improve DRL training speed and robustness.

We conduct experiments across three benchmark environments of varying complexity. Concretely, we focus on two main environment domains: two arcade-style games with discrete action spaces, \textbf{Flappy Bird} and \textbf{Breakout}, and one continuous control task modeled using the \textbf{Mujoco} physics simulator \citep{todorov2012mujoco}.

For discrete action environments, we compare the performance of Deep Q-Networks (DQN) and the basic ES implementation, described in Section \ref{sec: ES}. In the continuous Mujoco domain, we use Proximal Policy Optimization (PPO) as the representative DRL method, since DQN is not applicable in continuous settings.

\subsection*{Performance Comparison}

% We train agents using DRL and ES from scratch under controlled and identical experimental conditions. Each agent's performance is evaluated based on final reward and total training time required to reach a maximum reward. The goal is to compare training speed and post-training model performance, while assessing how robust a training algorithm is to hyperparameter, environment and seed variation

% To evaluate ES as a pretraining method, we first train agents with ES for a fixed number of episodes and use the resulting parameters to initialize the DRL networks. This allows us to test whether such initialization leads to faster convergence or improved stability in DRL training. Full implementation details are provided in Section~\ref{sec:experimental_setup}.

We train DRL and ES agents from scratch under identical conditions, comparing them based on final reward, training time, and robustness to hyperparameters, environment, and seed variation. To evaluate ES as a pretraining method, we initialize DRL networks with parameters from ES-trained agents and assess whether this improves convergence or stability. Full implementation details are in Section 4.

% \section{Methodology}

% Our goal is to compare the performance of gradient-based DRL algorithms against gradient-free ES methods. Additionally, we also evaluate the applicability of ES as a pretraining step for DRL algorithms. Our hypothesis is that, being a simpler and cheaper training method, this pretraining strategy could help speed up training or make algorithms more robust to hyperparameter selection.

%  We perform our analysis across three environments of varying complexity. Concretely, we focus on two main environment domains: arcade-style games with discrete action spaces, \textbf{Flappy Bird} and \textbf{Breakout}, and continuous control tasks in high-dimensional state and action spaces, modeled using the \textbf{Mujoco} physics simulator \cite{todorov2012mujoco}.

% For the discrete action space environments, we perform a comparison between DQN and ES, whereas for the Mujoco environment we use PPO as the DRL algorithm, given that DQN cannot work with continuous action spaces.

% \subsection*{Performance Comparison}
% We compare the performance of DRL and ES in Flappy Bird, Breakout and Mujoco under identical conditions. For each environment and strategy, we train an agent from scratch and evaluate their performance based on final reward and total training time required to reach a maximum reward. The goal is to compare training speed and post-training model performance, while assessing how robust is a training algorithm to hyperparameter, environment and seed variation. Regarding the pretraining strategy, we first train an agent for a short period using ES and then initialize the DRL network with the set of obtained params. Implementation details are described in Section \ref{sec:experimental_setup}.


% \subsection{Pretraining}
% Mention the Pretraining Setting

% Mention the two hypotheses:
% 1. Does pretraining with ES speed up DRL algorithms?
% 2. Does it make them more robust to hyperparam selection?


% Our goal was to compare the performance of gradient-based and gradient-free reinforcement learning methods across environments of varying complexity. We implemented and evaluated performance of Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Evolution Strategies (ES) across three environments: Flappy Bird, Breakout, and MuJoCo (HalfCheetah, Hopper and Walker). The first two environment, Flappy Bird and Breakout, have discrete action spaces, while MuJoCo involves continuous control. DQN was applied to the discrete environments, PPO to the continuous one, and ES was evaluated across all three settings.

% Each algorithm was trained independently under comparable conditions, with hyperparameters selected based on standard implementations or empirical tuning. For each environment, we trained agents from scratch and measured learning progress over time. In addition, we explored a hybrid approach in which ES was used for pretraining the network parameters before fine-tuning with a gradient-based algorithm. This approach was motivated by ES's strong exploration capabilities, particularly in sparse-reward environments. Specifically, the best-performing policy obtained through ES was used to initialize the hidden layers of a DQN (or PPO) agent, while the output layers were retrained during fine-tuning. By doing this we leverage the low-computation, short-time convergence properties of Evolution Strategies (ES), we can quickly obtain a reasonable policy initialization. However, ES methods often reach a plateau where further learning stagnates. To overcome this limitation, we subsequently fine-tune the model using Reinforcement Learning (RL), which allows for more granular reward optimization and continued policy improvement.
\vspace{-0.3cm}
\section{Experimental Setup}
In this section, we present the configuration details for the environments used in our experiments. Specifically, \cref{sec:experimental_setup_flappy_bird} outlines the setup for the Flappy Bird environment, \cref{sec:experimental_setup_breakout} covers the Breakout environment, and \cref{sec;experimental_setup_mujoco} details the configuration for the MuJoCo environment. 

\label{sec:experimental_setup}
\vspace{-0.15cm}
\subsection{Flappy Bird}
In \cref{sec:flappy_bird_environment_details}, we describe the configuration of the Flappy Bird environment used in our experiments, including the state representation and reward structure. In \cref{sec:flappy_bird_model_arch}, we present the model architectures explored and explain how they were integrated into the hybrid approach. The parameters used for this environment can be seen in \cref{sec:flappy_bird_training_params}.
\label{sec:experimental_setup_flappy_bird}
\vspace{-0.15cm}
\subsubsection{Environment Details}
\label{sec:flappy_bird_environment_details}
% The experiments were conducted in the FlappyBirdEnv, a custom environment implemented using the Python Learning Game Engine. The environment simulates the dynamics of the Flappy Bird game and provides visual observations. Frame skipping was set to 4, meaning that each selected action was repeated for four consecutive frames, with the corresponding rewards accumulated. The reward structure included a small positive reward of +0.1 for each timestep the bird remained alive and an additional +1.0 reward for each pipe successfully passed. To penalize failure, each time a terminal state was reached, typically due to collision with an obstacle or the ground, the agent received a negative reward of -1.0. This setup encouraged agents to maximize survival while progressing through the game.
Experiments were conducted in FlappyBirdEnv, a custom environment built with the Python Learning Game Engine that simulates the dynamics of the Flappy Bird game. Frame skipping was set to 4, repeating each action for four frames with cumulative rewards. The agent received +0.1 reward per timestep survived and +1.0 for each pipe passed, while collisions or failures resulted in a -1.0 penalty. This reward structure encouraged maximizing survival and progression.
\vspace{-0.15cm}
\subsubsection{Model Architecture}
\label{sec:flappy_bird_model_arch}
Both DQN and ES agents were based on a fully connected multilayer perceptron policy with two hidden layers of 64 units each and Tanh activation functions. The output layer produced a vector of scores corresponding to each discrete action. For DQN, this represented Q-values used for greedy or $\epsilon$-greedy action selection. In ES, the same output was used to deterministically select actions via argmax, with the reward used to guide population updates. This shared architecture facilitated weight transfer when initializing DQN from a pretrained ES policy.
% \subsubsection{Logging and Checkpointing}
% During training, we logged the mean episode reward over time to track how well the agents were learning. For DQN, we saved checkpoints regularly so we could go back to earlier points in training if needed, or continue training from where we left off. For ES, we didn’t use checkpoints in the same way, since it works differently - each generation is evaluated and replaced based on performance. However, at the end of training, the best-performing policy in the ES population was exported for initialization in DQN. All results were plotted with respect to cumulative training time, allowing fair comparison of learning dynamics across DQN, ES, and hybrid ES→DQN configurations.
\vspace{-0.15cm}
\subsection{Breakout}
\label{sec:experimental_setup_breakout}
In \cref{sec:breakout_environment_details}, we detail the configuration of the Breakout environment used in our experiments, including both image-based and RAM-based setups. \Cref{sec:breakout_model_arch} outlines the model architectures selected for each input type and their compatibility with the hybrid training framework. The parameters used for the breakout environment can be seen in \cref{sec:breakout_training_parmas}.
\vspace{-0.15cm}
\subsubsection{Environment Details}
\label{sec:breakout_environment_details}
The experiments were conducted using the Atari Breakout environment, a classic arcade game where the agent controls a paddle to bounce a ball and break bricks. The goal is to clear all bricks without letting the ball fall. Breakout is a standard reinforcement learning benchmark due to its visual complexity, sparse rewards, and need for precise control.

The action space includes four discrete actions: NOOP, FIRE, MOVE LEFT, and MOVE RIGHT. The agent earns +1 for each brick broken. An episode ends when all lives are lost or the level is cleared. To evaluate different input modalities, we used two environment variants:

For \textbf{Image-based setup (ALE/Breakout-v5)}, The agent observes stacked grayscale frames (84×84×4), capturing both spatial and temporal dynamics. This high-dimensional input emphasizes the role of vision and sequence learning.

For \textbf{RAM-based setup (Breakout-ram-v4)}, The agent receives a 128-dimensional vector representing the game’s internal memory state, offering a compact but semantically rich input. This setup tests whether ES performs better in low-dimensional, non-visual spaces.

% \begin{itemize}
%     \item{\textbf{Image-based setup (ALE/Breakout-v5)}: In this version, the agent observes raw pixel data consisting of game screen frames. To capture motion and temporal dependencies, each observation was formed by stacking the four most recent grayscale frames. This results in a high-dimensional 3D observation space capturing spatial and temporal information.}
%     \item{\textbf{RAM-based setup (Breakout-ram-v4)}: This configuration uses a compact, 128-dimensional vector representing the game's internal memory state, bypassing visual input entirely. It provides a lower-dimensional but semantically rich representation of the environment. The action and reward structure remains the same as in the image-based version. This setup was specifically used to test whether evolutionary strategies could perform better in a low-dimensional, non-visual input space.}
% \end{itemize}
\vspace{-0.25cm}
\subsubsection{Model Architecture}
\label{sec:breakout_model_arch}
% The model architecture was selected dynamically (RAM-based (vetorized) input for ES to simplify the environment and standard image-based for DQN) based on the input type. For pixel-based environments (ALE/Breakout-v5), a convolutional neural network (CNN) architecture (CnnPolicy) was used, matching the original DQN architecture suitable for high-dimensional visual input. For RAM-based environments (Breakout-ram-v4), a fully connected multilayer perceptron (MlpPolicy) was used with a custom architecture defined by two hidden layers of 256 units each (net\_arch: [256, 256]). The same model implementation was compatible with both the gradient-based DQN training and the gradient-free Evolution Strategy training, as it adhered to a shared BaseModel interface supporting parameter access, perturbation, and evaluation.
For \textbf{Image-based setup (ALE/Breakout-v5)}, A convolutional neural network (CNN) was used to process stacked grayscale frames (84×84×4), following the standard DQN design. This architecture, consisting of three convolutional layers and two fully connected layers, includes approximately 850,000 parameters. It is well-suited for extracting spatial and temporal features but is computationally intensive.

For \textbf{RAM-based setup (Breakout-ram-v4)}, A multilayer perceptron (MLP) with two hidden layers of 256 units each was used for the 128-dimensional RAM input. This simpler architecture has around 90,000 parameters, making it more efficient but less expressive for complex spatial reasoning.

Both models were implemented under a shared interface supporting parameter access, perturbation, and evaluation, enabling use with both DQN and Evolution Strategies.

\vspace{-0.15cm}
\subsection{Mujoco Environments}
\label{sec;experimental_setup_mujoco}
In \cref{sec:environment_details_mujoco} and \cref{sec:model_arch_mujoco}, we describe how the MuJoCo environments from the Brax library were configured, trained, and architecturally implemented for our experiments. These environments were selected due to their diversity in dynamics, reward structures, and control complexity, making them ideal testbeds for evaluating and comparing gradient-based (PPO) and gradient-free (ES) reinforcement learning methods under consistent and reproducible conditions. The parameters are detailed in \cref{sec:Training_parameters_mujoco}.
\vspace{-0.25cm}
\subsubsection{Environment Details}
\label{sec:environment_details_mujoco}
We evaluated agent performance on three MuJoCo benchmark environments from the Brax library: HalfCheetah, Hopper, and Walker2d. Each environment involves planar locomotion in 2D with continuous control over joint torques. HalfCheetah tasks a 6-actuator robot with maximizing forward velocity and does not terminate episodes early. Hopper is a single-legged agent with a 3-dimensional action space, terminating episodes upon falling. Walker2d, a bipedal version of Hopper with six joints, also ends episodes if the agent becomes unstable. All environments allow configuration of key parameters like control cost, forward reward weight, and initial state noise. These tasks vary in dynamics and reward structure, making them well-suited for comparing gradient-free and gradient-based reinforcement learning methods. Furthermore, BRAX environments are differentiable, making them a fair comparision of ES and RL. 
\vspace{-0.25cm}
\subsubsection{Model Architecture}
\label{sec:model_arch_mujoco}

For all MuJoCo environments, we used a fully connected neural network as the policy architecture. The network consisted of 4 hidden layers, each with 32 units and non-linear activation functions. This architecture was kept lightweight to facilitate efficient optimization and compatibility across algorithms.

When using ES as a pretraining step for PPO, this network was employed as the actor component. The weights obtained from ES training were transferred to initialize the PPO actor. For the critic network, however, we initialized a separate randomly seeded network, allowing PPO to learn the value function independently. This setup enabled seamless integration between ES and PPO while preserving the actor-critic structure essential for PPO’s learning dynamics.
\section{Results}
\subsection{Flappy Bird}
\begin{figure}[htbp]
\centering
  \includegraphics[width=0.825\textwidth]{images/flappy.pdf}
  \caption{Smoothed learning curves for ES, DQN, and ES-pretrained DQN versus cumulative training time in Flappy Bird environment}
  \label{fig:flappy}
\end{figure}
In the Flappy Bird environment, as we can see on Figure \cref{fig:flappy}, ES demonstrated strong learning capabilities. Within a relatively short training time, ES was able to find a stable policy that achieved consistent survival and reward accumulation. While DQN eventually reached higher final rewards, it required significantly more training steps and showed greater sensitivity to hyperparameters and random seeds. When comparing the two methods, it’s important to note that DQN was trained in parallel across multiple environments, whereas ES was trained sequentially. Additionally, DQN experienced sudden drops in performance during training, from which it often did not recover. This kind of instability might be caused by large updates in the wrong direction, possibly due to overestimated Q-values or noisy gradients, which pushed the policy into worse regions and caused it to reinforce poor decisions through the replay buffer. When DQN was initialized with policy parameters obtained from ES, the agent reached competitive performance much faster compared to training from scratch. These results suggest that ES is particularly effective in simple, sparse-reward environments like Flappy Bird, where its stable and robust exploration helps provide a strong starting point for gradient-based fine-tuning.
\subsection{Breakout}

% Figure~\ref{fig:breakout} showcases a performance comparison between DQN and ES on different versions of the Breakout environment, as described in Section~\ref{sec:experimental_setup_breakout}. After analyzing the results, we highlight two main takeaways:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.825\linewidth]{images/breakout_es_vs_dqn.pdf}
    \caption{Smoothed learning curves for ES and DQN versus cumulative training time in Breakout environment}
    \label{fig:breakout}
\end{figure}

% In our experiments, the Deep Q-Network (DQN) using a convolutional neural network (CNN) policy consistently achieved higher mean rewards compared to Evolution Strategies (ES), as we can see on figure \ref{fig:breakout}. However, the DQN's learning curve exhibited greater variance and was more sensitive to hyperparameter settings. When ES was applied to the DQN CNN policy in the original Breakout environment with image-based input, it performed poorly, plateauing at a low mean reward of around 1.5 even when using a population size of 50. To address the high input dimensionality, we tested ES with a DQN multilayer perceptron (MLP) policy in the “Breakout-ram-v4” environment, where the agent receives raw RAM states instead of pixel observations. This version of ES showed faster initial learning and achieved a mean reward of 4 within a few generations, but similarly plateaued early and failed to improve further. These results indicate that ES struggles to scale in environments with high-dimensional or temporally extended input spaces, whereas DQN maintains a performance advantage due to its capacity for fine-grained gradient-based updates and its use of temporal difference learning.

In our experiments, as shown in Figure \ref{fig:breakout}, the Deep Q-Network (DQN) with a CNN policy consistently achieved higher mean rewards of around 30, whereas Evolution Strategies (ES) plateaued at much lower rewards. From these observations, we highlight two main takeaways:

\takeaway{DQN consistently outperforms ES in Breakout, achieving mean rewards around 30 on the image-based environment.}
DQN’s CNN policy effectively handles high-dimensional pixel inputs, but its learning curve exhibits greater variance and is sensitive to hyperparameter tuning.

\takeaway{ES struggles to scale in complex settings, plateauing early in both pixel-based and RAM-based environments.}
ES applied to the CNN policy in the image-based environment plateaued at a mean reward near 1.5 despite a large population size. When using a simpler MLP policy on RAM-based inputs, ES showed faster initial progress with rewards reaching around 4 but failed to improve further. These results underscore ES’s difficulty extracting useful representations in high-dimensional and temporally extended settings, unlike the gradient-based updates and temporal difference learning used by DQN.

% \takeaway{DQN consistently outperforms ES in Breakout, despite greater sensitivity to hyperparameters.}  
% In the image-based Breakout environment, DQN with a CNN policy consistently achieves substantially higher rewards than ES. However, the DQN's learning curve exhibits greater variance and is more sensitive to hyperparameter settings

% \takeaway{ES fails to scale in high-dimensional or temporally complex settings, plateauing even in simplified environments.}  
% ES struggles in both pixel-based and RAM-based versions of Breakout, showing limited progress and early performance plateaus. These results highlight ES's difficulty in extracting useful representations from complex input spaces, in contrast to the more adaptive nature of gradient-based methods like DQN.

\vspace{-1.5mm}

\subsection{MuJoCo}

% SAY THAT WE FIND THAT DEPENDING ON THE ENVIRONMENT THAT WE CHOOSE, THE PPO ALGORITHM MIGHT PERFORM EXTREMELY WELL AND SOLVE THE ENVIRONMENT REALLY FAST, OR NOT SOLVE THE ENVIRONMENT AT ALL. WE SHOW THAT, ON THE OTHER HAND, EVOLUTION STRATEGIES ARE NOT AS FAST, BUT THEY SEEM TO BE MORE ROBUST TO ENVIRONMENT SELECTION.

% THEN WE SAY THAT WE HAVE TRIED PRETRAINING WITH ES AND THEN TRAINING WITH PPO TO SEE IF PPO WOULD BE MORE ROBUST TO HYPERPARAMETER SELECTION, BUT THIS IS NOT THE CASE GIVEN THAT THE MODEL PERFORMS IN THE SAME MANNER.

Figures \ref{fig:mujoco_combined} showcases a performance comparison between PPO and ES on different Mujoco environments, described in Section \ref{sec;experimental_setup_mujoco}. After analyzing the obtained results, we come up with two main takeaways: 

\takeaway{PPO performs inconsistently across seeds and environments, while ES is slower but yields significantly more stable and repeatable outcomes.}

PPO demonstrates strong performance in some environments but can be unstable and highly sensitive to hyperparameter choices. In HalfCheetah (Figure \ref{fig:half_cheetah_es_ppo}), PPO converges 20x faster than ES. However, in other environments such as Walker2d (Figure \ref{fig:walker_es_ppo}) or Hopper (Figure \ref{fig:hopper_es_ppo}), PPO does not manage to converge and oscillates between low reward values. On the other hand, ES reliably solves most of the evaluated environments, failing to fully solve Walker2d. However, its convergence is much slower compared to PPO (20x slower in HalfCheetah).


\takeaway{Pretraining with ES does not improve PPO's training speed nor enhance robustness to hyperparameter selection.}  

While ES eventually solves most environments, it is significantly slower, up to 20× in HalfCheetah, and consistently fails to reach intermediate reward thresholds faster than PPO. This lag in early performance undermines its viability as a pretraining strategy for accelerating PPO's training. On the other hand, despite initializing PPO with parameters obtained from ES, the resulting training behavior remains largely unchanged (Figure \ref{fig:hopper_es_ppo}). PPO continues to exhibit similar sensitivity to hyperparameters, indicating that ES pretraining does not improve training stability. This results could stem from the fact that PPO uses an actor-critic, structure, while ES only accounts for the actor network optimization.

\noindent

% \vspace{-1cm}
\begin{figure}[htbp]
    \centering

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/hopper_combined.pdf}
        \caption{Smoothed learning curves for ES, PPO, and ES-pretrained PPO versus cumulative training time in Hopper environment.}
        \label{fig:hopper_es_ppo}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/half_cheetah_es_vs_ppo.pdf}
        \caption{Smoothed learning curves for ES and PPO versus cumulative training time in Half Cheetah environment.}
        \label{fig:half_cheetah_es_ppo}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/walker_es_vs_ppo.pdf}
                \caption{Smoothed learning curves for ES and PPO versus cumulative training time in Walker environment.}

        \label{fig:walker_es_ppo}
    \end{subfigure}

    \caption{Performance comparison of ES, PPO, and ES pretraining across various Mujoco environments.}
    \label{fig:mujoco_combined}
\end{figure}





% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{HopperReward.png}
% \end{figure}

% \vspace{1em}
% \noindent

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{HalfCheetahReward.png}
%         \caption{HalfCheetah}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{WalkerReward.png}
%         \caption{Walker}
%     \end{subfigure}
 
% \end{figure}

% \vspace{1em}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.4\linewidth]{mujocolegend.png}
%     \caption*{\small{Legend: ES, PPO, and ES-pretrained PPO.}}
% \end{figure}

\vspace{1em}


\vspace{-0.5em}




% \section{Discussion}

% Our experiments show that Evolution Strategies can be a useful tool in reinforcement learning, particularly during the early stages of training. In the Flappy Bird environment, ES quickly found stable policies and provided a strong initialization for DQN, helping it reach high performance faster and with more stability. This highlights the value of ES in sparse-reward, low-dimensional tasks where exploration is more important than fine-tuned value estimates.

% However, the performance of DQN was inconsistent across runs. We observed cases where the agent's reward dropped sharply and failed to recover. This instability is likely due to overestimated Q-values or large updates based on noisy targets, which can lead the network into worse regions of the parameter space and reinforce poor behavior through the replay buffer.

% In contrast, Breakout proved much more challenging for ES. The high-dimensional image input significantly increased the complexity of the policy space, and even with larger populations, ES failed to make meaningful progress. Switching to a RAM-based version helped slightly, but performance still plateaued early. This suggests that ES is less effective in environments that require processing large observation spaces or learning long temporal dependencies.

% In the continuous MuJoCo environments, PPO outperformed ES in terms of final performance. While ES was able to reach moderate rewards, its convergence was slower, and using ES for pretraining PPO did not lead to improvements. This is likely due to architectural differences, as PPO relies on separate actor and critic networks, while ES only optimizes a single policy network. Transferring knowledge between these architectures remains a challenge.
\vspace{-0.3cm}
\section{Limitations and next steps}
%Even though ES have demonstrated strong performance in solving DRL tasks, and in some cases even outperformed traditional gradient-based methods, they do not appear to be effective as a pretraining mechanism for RL algorithms, contrary to our initial hypothesis. Due to architectural and learning paradigm mismatch:
%This is perhaps the most significant obstacle. In environments like Flappy Bird and MuJoCo, the knowledge acquired during ES training often failed to transfer meaningfully to subsequent reinforcement learning. We attribute this to two main factors: (i) structural differences between the algorithms-particularly in how actor-critic methods like PPO use separate value and policy networks versus the single-policy optimization in ES-and (ii) fundamental differences in the optimization process itself. While ES relies on population-based black-box search, RL depends on local gradient information. As a result, the representations learned by ES may not align with the assumptions or learning dynamics of gradient-based methods, limiting their effectiveness for fine-tuning.

%Although Evolution Strategies (ES) have shown strong performance in solving deep reinforcement learning (DRL) tasks - and in some cases even outperform traditional gradient-based methods - they did not prove effective as a pretraining mechanism for reinforcement learning algorithms, contrary to our initial hypothesis. We identify two key limitations:

%First, there is an architectural and learning paradigm mismatch. In environments such as Flappy Bird and MuJoCo, knowledge acquired during ES training often failed to transfer meaningfully to subsequent reinforcement learning. This stems from:
%(i) structural differences between the algorithms - actor-critic methods like PPO rely on separate value and policy networks, whereas ES uses a single-policy optimization approach; and
%(ii) fundamental differences in the optimization process - ES is a population-based black-box search method, while RL leverages local gradient information. As a result, representations learned through ES may not align with the assumptions or learning dynamics of gradient-based algorithms, limiting their effectiveness for fine-tuning. 

%Second, ES struggled to perform well in more complex environments such as Breakout. Even when using a vectorized (feature-based) version of the environment instead of the raw-pixel CNN-based version, performance remained poor. This suggests that ES has difficulty scaling to environments with high-dimensional or visually complex state spaces, further limiting its utility in such settings.

%Future research should focus on developing adaptive or architecture-aware hybrid approaches to improve transfer between ES and deep RL algorithms. This may include methods that align ES-learned representations with RL objectives, use shared modules between phases, or adapt architectures to reduce incompatibility. Such strategies could enhance the effectiveness of ES as a pretraining stage, especially in complex environments where pure ES struggles.
Although Evolution Strategies (ES) have shown strong performance in some DRL tasks - occasionally outperforming gradient-based methods - they were ineffective as a pretraining mechanism for RL, contrary to our hypothesis. We identify two main limitations:

First, ES and RL differ significantly in architecture and learning dynamics. In environments like Flappy Bird and MuJoCo, ES-trained policies did not transfer well due to differences in structure (e.g., PPO's separate value and policy networks vs. ES's single-policy optimization) and optimization methods (black-box search vs. gradient-based learning), leading to incompatible representations.
Second, ES performed poorly in complex environments such as Breakout. Even with vectorized (feature-based) inputs, results were weak, and performance degraded further with raw-pixel (CNN-based) inputs - highlighting ES’s difficulty in scaling to high-dimensional tasks.

Future work should explore adaptive, architecture-aware hybrid approaches that improve transfer between ES and RL, such as aligning learned representations, using shared modules, or modifying architectures to bridge the gap.


\vspace{-0.3cm}
\section{Conclusion}

Our results show that ES can effectively accelerate early learning in simple environments like Flappy Bird, especially when used to initialize gradient-based methods like DQN. However, ES struggled to scale to high-dimensional tasks like Breakout and was not compatible with PPO due to architectural differences. While DQN and PPO achieved better final performance overall, they were more sensitive to hyperparameters and showed variability across runs. These findings suggest that ES is a useful tool for exploration in low-complexity settings, but combining it with gradient-based methods in more complex tasks remains a challenge. Future work could explore adaptive or architecture-aware hybrid approaches that improve transfer between ES and deep RL algorithms.

\newpage
\bibliographystyle{chicago}
\bibliography{ref}


\newpage
\appendix
\section{Deep Q-Networks (DQN)}

The Deep Q-Network (DQN) algorithm combines Q-learning with deep neural networks to handle high-dimensional state spaces in reinforcement learning. It approximates the action-value function $Q(s, a; \theta)$ with a neural network parameterized by weights $\theta$, where $s$ is the current state and $a$ is the action taken.

\subsection{Mathematical Formulation}

The goal is to learn an optimal policy that maximizes the expected return by estimating the action-value function:

\begin{equation}
Q^*(s, a) = \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \,|\, s_0 = s, a_0 = a, \pi \right],
\end{equation}

where $\gamma \in [0, 1]$ is the discount factor and $r_t$ is the reward at timestep $t$. In Q-learning, the optimal $Q$-function satisfies the Bellman equation:

\begin{equation}
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \,|\, s, a \right].
\end{equation}

DQN minimizes the temporal-difference (TD) error between the current estimate $Q(s, a; \theta)$ and the target value $y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})$, where $\theta^{-}$ are the parameters of a separate target network. The loss function is:

\begin{equation}
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( y - Q(s, a; \theta) \right)^2 \right],
\end{equation}

where $D$ is the experience replay buffer. The target $y$ is treated as a fixed value during the optimization step:

\begin{equation}
y = r + \gamma \max_{a'} Q(s', a'; \theta^{-}).
\end{equation}

To stabilize training, DQN introduces two key mechanisms:

\begin{itemize}
    \item \textbf{Experience Replay:} A buffer $D$ stores past transitions $(s, a, r, s')$, and mini-batches are sampled uniformly from it to break correlation between sequential data.
    \item \textbf{Target Network:} A separate network with parameters $\theta^{-}$ is used to compute the target values. Its parameters are periodically updated to match $\theta$.
\end{itemize}

\subsection{Algorithm}

The DQN algorithm can be summarized as follows:

\begin{algorithm}
\caption{Deep Q-Network (DQN)}
\label{alg:dqn}
\begin{algorithmic}[1]
\STATE Initialize replay buffer $D$ with capacity $N$
\STATE Initialize Q-network with weights $\theta$
\STATE Initialize target Q-network with weights $\theta^{-} = \theta$
\FOR{each episode}
    \STATE Initialize environment and receive initial state $s$
    \FOR{each timestep}
        \STATE With probability $\epsilon$ select a random action $a$, otherwise $a = \arg\max_{a} Q(s, a; \theta)$
        \STATE Execute action $a$, observe reward $r$ and next state $s'$
        \STATE Store transition $(s, a, r, s')$ in $D$
        \STATE Sample random minibatch from $D$
        \STATE Compute target $y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})$
        \STATE Perform gradient descent on $\left( y - Q(s, a; \theta) \right)^2$
        \STATE Update $s \leftarrow s'$
        \STATE Every $C$ steps: $\theta^{-} \leftarrow \theta$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

By combining off-policy Q-learning with deep neural networks, DQN has been successfully applied to challenging domains such as Atari games from raw pixels, achieving human-level performance in many cases.


\section{Parameters}
\label{sec:Parameters}
In \cref{sec:flappy_bird_training_params}, we detailed the hyperparameters used to train agents in the Flappy Bird environment. \Cref{sec:breakout_training_parmas} presents the training parameters for the Breakout experiments, covering both DQN and ES configurations. Finally, \cref{sec:Training_parameters_mujoco} outlines the training setup adopted for the MuJoCo environments, including environment-specific hyperparameters and standardized PPO configurations from the Brax benchmark suite. Together, these sections provide a comprehensive overview of the experimental conditions under which each algorithm was evaluated.

\subsection{Flappy Bird Training Parameters}
\label{sec:flappy_bird_training_params}
The Deep Q-Network (DQN) agent was trained for 1,000,000 timesteps, corresponding to approximately 5000 episodes. Training was parallelized across 8 synchronous environments to speed up sample collection and stabilize learning. The agent followed an $\epsilon$-greedy exploration strategy, with $\epsilon$ annealed linearly from 0.2 to 0.0001 over the first 10\% of training. Additional hyperparameters included a learning rate of 5e-5, a replay buffer size of 10,000 transitions, a mini-batch size of 32, and a discount factor $\gamma$ of 0.90.

The Evolution Strategies (ES) algorithm used a population size of 16 and a Gaussian noise standard deviation of $\sigma=0.05$. It was trained sequentially for 1000 generations with a learning rate of 0.005. ES operated directly on the policy parameters, using returns as a fitness measure to guide the search. After training, the trained ES policy was used to initialize the hidden layers of the DQN network. This provided a warm start for DQN, allowing it to begin learning from a reasonably good policy rather than from scratch.
\subsection{Breakout Training Parameters}
\label{sec:breakout_training_parmas}
For DQN, we trained the agent using the following hyperparameters: a learning rate of 2e-4, buffer size of 100,000 transitions, batch size of 32, training frequency of every 4 steps, and one gradient update per training step. Target network updates were performed every 1,000 steps. Training followed an $\varepsilon$-greedy policy, where $\varepsilon$ decayed from 0.1 to 0.01 over time. DQN training was managed through Stable-Baselines3 and included support for both CnnPolicy and MlpPolicy depending on the input type.

For Evolution Strategy, training was conducted over 500 generations with a population size of 50. Each individual was evaluated over 5 episodes, and symmetric Gaussian noise was applied to the model parameters using a standard deviation (sigma) of 0.2. The learning rate for ES updates was set to 0.01. Parameters were updated using the reward-weighted average of perturbations, normalized by the population standard deviation. Checkpoints were saved every 100 generations, and metrics were logged using Weights \& Biases (wandb). For both ES and DQN we tried different hyperparameters and arrived on the parameters specified in this report.

\subsection{Mujoco Training Parameters}
\label{sec:Training_parameters_mujoco}

For all MuJoCo tasks, we followed the default PPO training configurations provided in the Brax benchmark suite \citep{freeman2021braxdifferentiablephysics}. These setups were optimized for efficient and stable learning in continuous control environments using large-scale parallel simulation (num envs = 8192).
\begin{itemize}
\item \textbf{Walker2d}: Trained for $7{,}864{,}320$ timesteps with $20$ evaluation points. The reward was scaled by $5$, and the discount factor was $\gamma = 0.997$. We used a learning rate of $6 \times 10^{-4}$, batch size $128$, and $32$ gradient updates per environment step. Training began after $8192$ transitions were collected, with a replay buffer of size $2^{20}$.

\item \textbf{HalfCheetah} and \textbf{Hopper}: Both trained for $6{,}553{,}600$ timesteps using similar settings, but with a higher reward scaling factor of $30$, batch size $512$, and $64$ gradient updates per step to support faster locomotion dynamics.
\end{itemize}



All environments used observation normalization to stabilize training, with each state input standardized to reduce variance across features. Actions were applied at every simulation step without repetition (i.e., action repeat was set to 1), ensuring fine-grained control. A fixed random seed (set to 1) was used to ensure reproducibility of results. Training was constrained to one computational device per host, matching the single-device setup used in the original Brax benchmarks. These standardized settings ensured fair comparison between the PPO and ES approaches across all environments.



\end{document}

MNLP PROJECT 06.2025 FOR EPFL COURSE MNLP
\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage[table]{xcolor}  % Optional, only needed if you want to color cells
\usepackage{graphicx}
\usepackage{makecell} % Include in preamble if not already present
\usepackage{siunitx}
\usepackage[most]{tcolorbox}
\usepackage{fvextra} % for breakable verbatim
\tcbuselibrary{listings}
\sisetup{round-mode=places, round-precision=3}
\usepackage{listings}

\definecolor{codebg}{rgb}{0.97,0.97,0.97}
\definecolor{keywordcolor}{rgb}{0.2,0.2,0.6}
\definecolor{stringcolor}{rgb}{0.6,0.1,0.1}

\lstdefinestyle{config}{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{keywordcolor}\bfseries,
  stringstyle=\color{stringcolor},
  showstringspaces=false,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  captionpos=b,
}


%%%%%%%%%%%%%%%%%%% TITLE AND AUTHORS %%%%%%%%%%%%%%%%%%%

\title{Chatsplaining: EPFL best classmate}

\author{\normalfont 
Tamar Alphaidze | 393635 | \texttt{tamar.alphaidze@epfl.ch} \\
Mario Rico Ibáñez | 395172 | \texttt{mario.ricoibanez@epfl.ch} \\
Adrián Martínez López | 396379 | \texttt{adrian.martinez@epfl.ch} \\
Jon Lecumberri Arteta | 386801 | \texttt{jon.lecumberriarteta@epfl.ch} \\
Chatsplaining \\
}

%%%%%%%%%%%%%%%%%%% PROPOSAL %%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%  MACROS  %%%%%%%%%%%%%%%%%%%%
\newcommand{\qwenb}{Qwen-0.6B }
\newcommand{\qwenbbase}{Qwen-0.6B-Base }
\newcommand{\qwentulu}{QWEN + TuLU }
\newcommand{\qwentulumcqa}{TuLU + MCQA  }
\newcommand{\qwentulumcqamath}{TuLU + MCQA + MATH }
\newcommand{\lora}{LoRA }
\newcommand{\omath}{OpenMath }
\newcommand{\tuluds}{tulu-v2-sft-mixture }
\newcommand{\unimcqa}{unified-4choice-mcqa}

%%%%%% datasets of unified %%%%%%
\newcommand{\medmcqa}{MedMCQA }
\newcommand{\mmlu}{MMLU }
\newcommand{\swag}{SWAG }
\newcommand{\race}{RACE }
\newcommand{\hellaswag}{HellaSwag }
\newcommand{\cosmosqa}{CosmosQA }
\newcommand{\quail}{QuAIL }
\newcommand{\openbookqa}{OpenBookQA }
\newcommand{\sciq}{SciQ }
\newcommand{\aquarat}{AquaRat }

\newcommand{\allenaimath}{ARC }
\newcommand{\allenaiarc}{MathQA }

\newcommand{\todomario}[1]{\textcolor{red}{TODO: #1}}


\newcommand{\graniteembed}{granite-embedding-107m-multilingual}








\begin{document}
\maketitle
\begin{abstract}
% This project investigates and compares multiple training strategies for natural language understanding using the \texttt{\qwenbbase} model, with a focus on multiple-choice question answering (MCQA) in STEM domains. We implement and evaluate four model variants: (1) supervised fine-tuning (FT), (2) quantization for efficiency, (3) retrieval-augmented generation (RAG) for enhanced factual grounding, and (4) direct preference optimization (DPO) for alignment with human preferences. The MCQA models are evaluated on datasets such as SciQ, MedQA, and AquaRat, while open-answer tasks and preference datasets inform the generative and alignment components. Our results show that ...
The convergence of advanced natural language processing with educational pedagogy presents a unique opportunity to democratize high-quality, personalized tutoring experiences on a large scale. In this paper, we investigate and compare multiple training strategies for natural language understanding using the \texttt{Qwen/Qwen0.6b-Base} language model, with a focus on multiple-choice question answering (MCQA) in STEM domains. We implement and evaluate four model variants: (1) supervised fine-tuning (SFT), (2) quantization for efficiency, (3) retrieval-augmented generation (RAG) for enhanced factual grounding, and (4) direct preference optimization (DPO) for alignment with human preferences. We demonstrate that a two-stage curriculum learning approach, composed of instruction-based SFT followed by succesive SFTs on specific labeled MCQA data, is critical for reliable reasoning, and that DPO can effectively align model outputs to human judgments.
\end{abstract}


\section{Introduction}

Although human one-on-one tutoring is excellent at providing personalised instruction and emotional support, its high cost, limited availability and scalability issues mean that many learners do not have access to individualised educational support \cite{bloom1984two}. The recent advances in Large Language Models offer a solution to this problem \cite{KASNECI2023102274, brown2020languagemodelsfewshotlearners}. The natural language capabilities of LLMs can be leveraged to engage students in meaningful dialogue, provide personalized explanations, and adapt pedagogical strategies regardless of the time, place, and the student's social background. 
Unlike conventional computer-assisted instruction, which relies on pre-programmed responses, LLMs can generate contextually appropriate feedback and identify and address specific misconceptions to guide students towards a deeper understanding, rather than simply providing answers \cite{KASNECI2023102274}. Furthermore, their ability to communicate in natural language eliminates interaction barriers, making educational support more accessible and intuitive for learners of all ages and technical proficiency. The convergence of advanced natural language processing with educational pedagogy presents a unique opportunity to democratize high-quality, personalized tutoring experiences on a large scale.
In this paper, we attempt to build such intelligent tutor by fine-tuning the \texttt{Qwen3-0.6B-Base} language model from Alibaba \cite{qwen3technicalreport}.

In order to achieve this, we explore the development and training of four different models of natural language understanding, all based on the \texttt{\qwenbbase} language model. The task spans multiple paradigms of model training and optimization, including supervised fine-tuning, quantization, retrieval-augmented generation (RAG), and direct preference optimization (DPO). The final objective is to evaluate the performance of each approach across multiple-choice question answering (MCQA) tasks, especially in STEM-related domains.

Our baseline MCQA model employs traditional supervised fine-tuning on labeled multiple-choice data, while a LoRA variant \cite{hu2021loralowrankadaptationlarge} enables parameter-efficient training through low-rank adaptation. To address computational constraints, we develop Quantized MCQA models using 4-bit and 8-bit quantization (W4A8 and W8A8) via the \texttt{llm-compressor} library  \cite{llmcompressor2024}, trading some accuracy for significant reductions in memory usage and inference latency. Our RAG model \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} addresses knowledge limitations by dynamically retrieving external information during inference, enabling factual grounding beyond the base model's encoded knowledge. Finally, we implement Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} to align model outputs with human preferences through direct fine-tuning on preference pairs, bypassing traditional reward modeling approaches. These methodologies collectively examine the spectrum of efficiency, accuracy, and alignment considerations in modern language model deployment.

In the sections that follow, we describe the training methodologies, evaluation benchmarks, and findings associated with each approach.


\section{Approach}
\label{sec:approach}

We followed an approached based on different stages. This is done mainly because the \qwenbbase is a pre-trained non usable model, it is just trained on a bunch of data but does not answer in a proper way. Then, we first trained it to follow instructions, by this way the model is more responsible and usable on different domains. We called this a \textbf{foundational training}. After that we have other stages for \textbf{MCQA} models and \textbf{DPO}. For the first ones, we trained on general knowledge MCQA and then improve it on math topics, to be more correlated with the final objective, \textbf{EPFL related questions}. For the latter we perform a DPO finetuning to align the model with human preferences and enhance its preference selection capabilities.

\subsection{Multiple-Choice Question Answering}

Across this project there are three models than rely on an MCQA one as their backbone: MCQA, Quantized MCQA and RAG. In this section the common approach for these is presented. The goal is to be able to predict the correct answer of STEM-based EPFL course material multiple choice questions. From the foundational training presented above, this task required the implementation of a significative prompt as well as further fine-tuning towards more MCQA-related data. In all models cross-entropy loss was used between the predicted token distribution and the correct answer label.

\subsubsection*{MCQA Base model}

In all cases the MCQA task was framed as a multiple-choice classification problem over natural language input. Each input instance consisted of a context passage (optional), a question $q$, and a set of four answer choices $\{a_1, a_2, a_3, a_4\}$. The model subsequently learned to select the correct answer $a^* \in \{a_1, \dots, a_4\}$. 
%We finetune the \texttt{\qwenbbase} model using supervised learning to solve this task.
Each example was formatted into a prompt with the following structure, while full prompt can be seen in Appendix \ref{apen: prompt}:
{\scriptsize
\begin{verbatim}
Context: <context> \n Question: <question> \n 
A. <answer_1> \n B. <answer_2> \n
C. <answer_3> \n D. <answer_4> \n
Answer:
\end{verbatim}
}

As mentioned above, the model was trained to predict the correct letter (A-D) as the next token. 

Our approach combines the Instruction Following Phase with MCQA and domain-specific fine-tuning. Following the foundational training based on Instruction Following, we fine-tuned the model on a mixture of MCQA datasets to develop general proficiency in multiple-choice question answering across diverse domains. This stage allows the model to learn the specific format patterns required for MCQA tasks while maintaining broad knowledge coverage. Finally, recognizing our ultimate objective of creating an intelligent model for STEM and EPFL-like content, we conducted a third training stage focused specifically on mathematics and logical reasoning MCQA data. This targeted approach ensures that the model develops the specialized capabilities necessary for handling the complex quantitative and analytical problems characteristic of EPFL coursework, while building upon the general instruction-following and MCQA foundations established in the previous stages. Figure \ref{fig:mcqa_model} shows the full pipeline implemented for the MCQA model.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{MCQA-fine_tuning.png}
  \caption{Architecture of the MCQA model using Combined Approach}
  \label{fig:mcqa_model}
\end{figure}

\subsubsection*{Quantized model}

To reduce inference latency and VRAM usage, we applied post-training quantization to the supervised MCQA model using the \texttt{llm-compressor} library  \cite{llmcompressor2024}. Specifically, we experimented with both W8A8 (8-bit weights, 8-bit activations) and W4A8 (4-bit weights, 8-bit activations) quantization schemes.

We applied SmoothQuant as a pre-processing step to reduce activation outliers and improve quantization robustness. SmoothQuant applies a layer-wise scaling transformation:
\[
\tilde{x} = x / \alpha, \quad \tilde{W} = \alpha W
\]
where $\alpha$ is a learned smoothing factor for each layer, $x$ is the activation input, and $W$ is the weight matrix. This reduces dynamic range in activations and concentrates representational power in quantized weights.

Following this, we applied GPTQ (Group-wise Post-training Quantization) to all linear layers except the final \texttt{lm\_head}. GPTQ optimizes for minimal quantization error by analytically solving for the best low-bit approximation to the original weights, using second-order statistics from calibration data.

We evaluated both quantized models using log-likelihood accuracy on held-out MCQA samples. %While W8A8 retained similar performance to the original model with reduced VRAM usage, W4A8 showed degradation in accuracy-highlighting the trade-off between compression and task performance.
\subsubsection*{Retrieval-Augmented Generation}
To enhance the model's factual grounding with specific and evolving information from EPFL coursework, we implement a \textbf{Retrieval-Augmented Generation (RAG)} system consisting of a \textbf{retriever} and a \textbf{generator}. The retriever's role is to identify relevant context using the \textbf{\graniteembed{}} model \cite{awasthy2025graniteembeddingmodels}, an efficient 107-million-parameter dense retriever. For this, an external corpus of EPFL materials and textbooks is chunked and encoded into 384-dimensional vectors, which are indexed in a \textbf{FAISS} (Facebook AI Similarity Search) vector store for efficient retrieval. At inference time, a user's query is embedded with the same model, and a \textbf{Maximum Inner Product Search (MIPS)} equivalent to cosine similarity for normalized vectors retrieves the top-2 most similar passages. This context is then passed to the generator.

\subsection{Direct Preference Optimization}

We frame the DPO task as a binary classification problem over natural language input. Each input instance consists of a prompt $p$, which provided context and a question, and a pair of answer choices $\{chosen, rejected\}$. The model subsequently learns to select the human-preferred, chosen answer, thus aligning model behaviour to a more human style. 
Each example is formatted into a triplet with the following structure:
\begin{verbatim}
Prompt: <prompt>
Chosen: <chosen>
Rejected: <rejected>
\end{verbatim}
As mentioned above, the model is trained to maximize the difference between the rewards assigned to the chosen and rejected responses.

In order to train the model, we perform a DPO finetuning on top of the instruction-following SFT, described in subsection \ref{sec:approach}. Performing a prior SFT is helpful since the model follows instructions more reliably and the \textit{chosen} and \textit{rejected} answer pairs are not out-of-distribution. Figure \ref{fig:dpo_model} shows the full pipeline for the DPO model.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{DPO_finetuning.png}
  \caption{Architecture of the DPO model using Combined Approach}
  \label{fig:dpo_model}
\end{figure}

\section{Experiments}

This section contains all relevant information regarding the different experiments that were done in order to produce the final models. 

\subsection{Data}

\subsubsection{Instruction Following Dataset}

As commented in section \ref{sec:approach}, we perform an initial instruction-following finetuning on our 2 model architectures. The goal of this first training stage is to obtain a model that behaves like an assistant, capable of following instructions reliably. To accomplish that, we used \texttt{\tuluds}, a high-quality instruction-tuning mixture that integrates data from sources such as FLAN, ShareGPT, OpenAssistant, Alpaca, and others \cite{ivison2023camelschangingclimateenhancing}. Each open-answer entry is formatted with: \texttt{question} (input prompt), \texttt{answer\_text} (free-form answer), \texttt{source} (dataset identifier), and \texttt{explanation} (optional reasoning or context).

\subsubsection{DPO Dataset}
\label{sec:dpo_data}
We introduce the following dataset composed of different preference question pairs from different domains and sources. This dataset is a blend of the following sets:
\vspace{-0.25em}
\paragraph{M1 Preference Data:}M1 preference questions with answer pairs regenerated by Google Gemini 2.5 Pro in order to have better quality answers. The dataset was crafted in a meticulous manner, avoiding length biases and ensuring that the rejected response is slightly worse than the chosen one. This dataset replaces the original M1 preference set annotated by MNLP's students.
\vspace{-0.25em}
\paragraph{Nectar:}a 7-wise comparison dataset with 7 responses per prompt, ranked by GPT-4 based on helpfulness and harmlessness. It covers diverse domains such as math, science, coding, and writing. Responses come from models like GPT-4, LLaMA-2, and Mistral. We select the best response as chosen and randomly select the rejected answer from the rest~\cite{starling2023}.
\vspace{-0.25em}
\paragraph{StepDPO:}A math-reasoning preference pair dataset consisting of 10k prompts. This dataset looks to reinforce the model's mathematical capabilities~\cite{lai2024stepdpostepwisepreferenceoptimization}.
\vspace{-0.25em}
\paragraph{UltraFeedback:}A collection of 20k prompts with GPT-4-ranked responses for helpfulness and honesty \cite{cui2023ultrafeedback}.

All datasets are converted into a unified for-
mat containing the following fields: \texttt{prompt}, \texttt{chosen}, \texttt{rejected}.

\subsubsection{MCQA Dataset}
% We use several multiple-choice QA datasets, each targeting different domains and reasoning types, highlighting:
% \begin{itemize}[noitemsep, topsep=0pt, leftmargin=0.2em]
%     \item \textbf{\sciq}: General science questions intended for middle and high school students \cite{SciQ}.
%     \item \textbf{\medmcqa}: Medical reasoning questions requiring professional-level domain knowledge \cite{wu2025medreasonelicitingfactualmedical}.
%     \item \textbf{\aquarat}: Multi-step arithmetic and logical reasoning problems that test mathematical and deductive capabilities \cite{ling2017program}.
%     \item \textbf{\allenaimath}:  large-scale dataset of math word problems \cite{amini-etal-2019-mathqa}. 
%     \item \textbf{\hellaswag}:  formatted as a collection of multiple-choice questions with the goal of finishing  \cite{zellers2019hellaswag}.
%     \item \textbf{\mmlu}: massive multitask test consisting of multiple-choice questions from various branches of knowledge \cite{hendryckstest2021}. Only non benchmark splits were used.
%     \item \textbf{\swag}: consists of multiple choice questions about grounded situations \cite{zellers2018swagaf}.
%     \item \textbf{\race}:  large-scale reading comprehension dataset \cite{lai-etal-2017-race}
%     \item \textbf{\cosmosqa}: problems that require commonsense-based reading comprehension, formulated as multiple-choice questions \cite{huang-etal-2019-cosmos}.
%     \item \textbf{\quail}: reading comprehension dataset \cite{DBLP:conf/aaai/RogersKDR20}. 
%     \item \textbf{\openbookqa}: questions necessitating multiple steps of reasoning, integration of extra common and commonsense knowledge, as well as deep text comprehension skills \cite{OpenBookQA2018}.
% \end{itemize}

We employ several multiple‐choice QA datasets grouped by domain:  
\textbf{Science}: \sciq{} \cite{SciQ} for middle‐ and high‐school science;  
\textbf{Medical}: \medmcqa{} \cite{wu2025medreasonelicitingfactualmedical} for professional‐level medical reasoning;  
\textbf{Math \& Logic}: \aquarat{} \cite{ling2017program} for multi‐step arithmetic and logical problems, \allenaimath{} \cite{amini-etal-2019-mathqa} for large‐scale math word problems, and MathQA \cite{amini2019mathqainterpretablemathword} for mathematical reasoning;
\textbf{Commonsense \& Situational}: \hellaswag{} \cite{zellers2019hellaswag} and \swag{} \cite{zellers2018swagaf} for grounded and situational commonsense completion;  
\textbf{Reading Comprehension}: \race{} \cite{lai-etal-2017-race}, \cosmosqa{} \cite{huang-etal-2019-cosmos}, and \quail{} \cite{DBLP:conf/aaai/RogersKDR20};  
\textbf{Multi‐disciplinary Knowledge}: \mmlu{} (non‐benchmark splits) \cite{hendryckstest2021} for broad subject coverage;  
\textbf{Open‐Book Reasoning}: \openbookqa{} \cite{OpenBookQA2018} for multi‐step reasoning integrating external knowledge. Finally, the MCQA from the M1 was also added.






% %%%%%% datasets of unified %%%%%%
% \newcommand{\medmcqa}{MedMCQA }
% \newcommand{\mmlu}{MMLU }
% \newcommand{\swag}{SWAG }
% \newcommand{\race}{RACE }
% \newcommand{\hellaswag}{HellaSwag }
% \newcommand{\cosmosqa}{CosmosQA }
% \newcommand{\quail}{QuAIL }
% \newcommand{\openbookqa}{OpenBookQA }
% \newcommand{\sciq}{SciQ }
% \newcommand{\aquarat}{AquaRat }

% \newcommand{\allenaimath}{ARC }
% \newcommand{\allenaiarc}{MathQA }


Importantly, all datasets but \texttt{ARC} and \texttt{MathQA} are combined to make single merged dataset for MCQA fine-tuning (\texttt{\unimcqa}), and \texttt{ARC} and \texttt{MathQA} are combined separately to make a dataset for subsequent domain-specific fine-tuning (\texttt{arc\_math\_qa\_merged}).

All datasets are converted into a unified format containing the following fields: \texttt{question}, \texttt{choices} (answer options), \texttt{answer\_index} (correct option index), \texttt{answer\_text} (correct answer content), \texttt{source} (original dataset), and \texttt{explanation}. 


\subsubsection{Quantization Dataset}
Calibration set is used to estimate the distribution of activations and optimize quantization thresholds.
We employ a held-out calibration set for quantization, sampled from the MCQA fine-tuning datasets \texttt{\unimcqa} and \texttt{arc\_math\_qa\_merged} -- which are used for MCQA and domain-specific fine-tunings respectively, as explained above. 
We employed a mixed-dataset approach, combining 256 samples each from \texttt{\unimcqa} and \texttt{arc\_math\_qa\_merged} datasets (512 total samples), which were shuffled with a fixed seed (42) for reproducibility. The calibration samples were formatted into a structured prompt template including \texttt{context}, \texttt{question}, \texttt{multiple-choice}, \texttt{options}, and \texttt{answer} fields, then tokenized with a maximum sequence length of 512 tokens without padding or special tokens.
While using the validation split would be methodologically preferable, we utilized the training split due to the unavailability of validation data. 

\subsubsection{RAG Dataset}
\noindent The efficacy of our RAG system depends on its external knowledge base, for which we curated a composite corpus from two distinct sources for broad and specialized coverage. The first source provides a general scientific foundation through a collection of freely available STEM textbooks. The second, more critical source consists of specialized EPFL course materials, including lecture notes and past exams, sourced from a student repository. This EPFL dataset underwent a rigorous cleaning pipeline to remove duplicate and incomplete files, ensuring index integrity. All documents were subsequently segmented into coherent passages of up to 512 tokens, resulting in a final knowledge base of approximately 39,000 chunks for indexing.

\subsection{Evaluation Method}

\subsubsection{MCQA-based models}

We evaluated our model's performance on a broad suite of established benchmarks. For mathematical and logical reasoning, we used \omath, \aquarat, the \allenaimath dataset, and \allenaiarc. For other skills, we employed \hellaswag for commonsense inference, \medmcqa for medical knowledge, \mmlu for broad academic knowledge, and \sciq for science question answering. All reported scores are on the official validation and test sets of these benchmarks.

\subsubsection{DPO model}
\label{subsec:dpo_evals}
We evaluate our DPO model across a range of preference-based benchmarks. For code generation, we use a code preferences dataset (CP) specially tailored for DPO~\cite{vezora_code_preference_pairs}, and for mathematical reasoning, we include RewardMath (RM) and Metamath-DPO (MM)~\cite{kim2024evaluatingrobustnessrewardmodels, pal2024smaugfixingfailuremodes}. We also evaluate on the MNLP DPO benchmark via the LightEval framework. To assess broader alignment, we use the held-out test set from UltraFeedback (UF). Finally, we evaluate over RewardBench~\cite{RewardBench}, a benchmark for evaluating reward models across chat, safety, and reasoning. It uses human-verified prompt–chosen–rejected trios and supports DPO-style evaluation to assess alignment quality.



\subsection{Baselines}
 For the general model, references \qwenbbase and \qwenb are employed to compare fine-tuning improvements and quality against an official post-trained version. For the Quantized model, the baseline is the final MCQA model with instruction following, MCQA, and domain-specific knowledge to track accuracy changes during quantization. For the DPO model, \qwenb serves as the sole baseline, since \qwenbbase is used as the reference model. We also compare against other DPO-aligned models such as Qwen1.5-Chat~\cite{qwen}, 
Zephyr-7B-beta~\cite{tunstall2023zephyr} or Llama-3-tulu-2-dpo-8b~\cite{ivison2023camels}. 


\subsection{Experimental details}

We provide an extensive explanation of the hyperparameters, base models and code details used for each model training phase in Appendix \ref{apen: experimental details}

% \subsubsection{Foundational training Experimental Details}

% We fine-tuned \texttt{Qwen/Qwen3-0.6B-Base} on the \texttt{\tuluds} for one epoch using per-device batch size 2, gradient accumulation 8, learning rate $2\times10^{-5}$, and max gradient norm 1.0. Training employed BF16 precision, gradient checkpointing, and a linear LR scheduler with warmup ratio 0.03. Logging every 20 steps, saving every 100 steps, reporting to WandB and with Seed 42.

% \subsubsection{SFTs for MCQA Experimental Details}

% We fine-tuned the model outputted from the foundational training stage using a two-phase curriculum learning approach. 
% First, for general MCQA adaptation, the model was trained on a broad mixture of datasets (\texttt{\unimcqa}) with a learning rate of $6 \times 10^{-6}$.
% Subsequently, to enhance mathematical and logical reasoning, the resulting model was further fine-tuned on the \texttt{\allenaiarc} dataset split using a more conservative learning rate of $2 \times 10^{-6}$. For both stages, we trained for one epoch using the AdamW optimizer, BF16 precision, gradient checkpointing, and a cosine learning rate scheduler with 50 warmup steps. The best model was selected based on evaluation loss.

% \subsubsection{Quantized Experimental Details}
% We conducted model quantization experiments using the \texttt{llm-compressor} library to optimize the MCQA final model for efficient inference. The base model was loaded with automatic device mapping and \texttt{float16} precision to manage memory usage effectively. For calibration data, We applied a two-stage quantization recipe combining SmoothQuant (smoothing strength 0.8) for activation conditioning followed by GPTQ quantization targeting all Linear layers while excluding the language modeling head (lm\_head). Two quantization schemes were evaluated: W8A8 (8-bit weights and 8-bit activations) and W4A8 (4-bit weights and 8-bit activations), both implemented through the oneshot API. The quantization process utilized the full calibration dataset for compression statistics, and the resulting models were saved using the save\_compressed flag. Post-quantization validation was performed through sample text generation to verify model functionality.

\subsection{Results}

The results of MCQA-based models across the selected benchmarks can be seen in Table \ref{tab:evaluations}. Appendix \ref{apen: comp} contains more detailed results as another experiment skipping the foundational training and just focusing on SFT and SFT + LoRA was implemented to further evaluate performances. Tables \ref{tab:DPO-pairs-result} and \ref{tab:DPO rewardbench} contain results for the DPO models. Appendix \ref{appendix:rag_performance} shows a detailed explanation of RAG results. 
Regarding the memory usage of the quantized model,  W4A8 quantization exhibited slightly higher peak VRAM usage (1237.18 MB) compared to W8A8 (1221.32 MB) during inference, despite producing a smaller final model and despite them having identical benchmark results.


\begin{table}[h!]
\scriptsize
\centering
\begin{tabularx}{\linewidth}{lX X X X X}
\toprule
\textbf{Task} & \textbf{Qwen-0.6B-Base} & \textbf{Qwen-0.6B} & \textbf{\qwentulumcqamath} & \textbf{Quantized (W8A8)/ (W4A4)} & \textbf{RAG} \\
\midrule
Hellaswag    & 0.2507   & 0.2504   & \textbf{0.4633} & 0.4600 & 0.4563 \\
MathQA       & 0.2419   & 0.2419   & \textbf{0.3142} & 0.3114 & 0.3114 \\
Aqua         & 0.2200 & 0.2200 & 0.3415 & \textbf{0.3527} & 0.3415 \\
MedQA      & 0.1000 & 0.1000 & 0.4231 & \textbf{0.4248} & 0.4196 \\
MMLU         & 0.1600 & 0.1600 & \textbf{0.5016} & 0.4993 & 0.4956 \\
SciQ         & 0.2700 & 0.2700 & \textbf{0.8426} & 0.8396 & 0.8416 \\
EPFL         & 0.1896 & 0.1879 & \textbf{0.3238} & 0.3104 & 0.3171 \\
\bottomrule
\end{tabularx}
\caption{Accuracy comparison across QA benchmarks.}
\label{tab:evaluations}
\end{table}

\begin{table}[h!]
\scriptsize
\centering
\begin{tabularx}{\linewidth}{lX X X X X}
\toprule
\textbf{Model} & \textbf{CP} & \textbf{MNLP} & \textbf{RM} & \textbf{UF} & \textbf{MM} \\
\midrule
Qwen1.5-0.5B-Chat & 0.665 & 0.472 & 0.641 & 0.416 & 0.819 \\
Qwen1.5-4B-Chat & 0.855 & 0.557 & 0.628 & 0.579 & 0.944 \\
Qwen1.5-72B-Chat & \textbf{0.975} & 0.790 & \textbf{0.708} & \textbf{0.650} & 0.917 \\
Llama-3-tulu-2-dpo-8b & 0.849 & 0.583 & 0.364 & 0.595 & 0.950 \\
Zephyr-7b-beta & 0.813 & 0.536 & 0.350 & 0.598 & 0.826 \\
Qwen3-0.6B & 0.619 & 0.599 & 0.588 & 0.455 & 0.845 \\
Base + SFT & 0.643 & 0.572 & 0.513 & 0.509 & 0.769 \\
Base + DPO & 0.918 & 0.814 & 0.609 & 0.627 & 0.949 \\
\textbf{Base + SFT + DPO} & 0.928 & \textbf{0.841} & 0.624 & 0.645 & \textbf{0.947} \\
\bottomrule
\end{tabularx}
\caption{DPO Performance of selected models across different preference pair selection benchmarks. Evaluation benchmarks are detailed in subsection \ref{subsec:dpo_evals}.}
\label{tab:DPO-pairs-result}
\end{table}



% \begin{table*}[h!]
% \small
% \centering
% \begin{tabularx}{\textwidth}{lXXXXXX}
% \toprule
% \textbf{Task} & \textbf{MCQA final} & \textbf{Quantized (W8A8)} & \textbf{Quantized (W4A8)} & \textbf{Qwen-0.6B-Base}\\
% \midrule
% hellaswag       & 0.4633 & 0.4600 & 0.4600 & n/a \\
% mathQA            & 0.3142 & 0.3114 & 0.3114  & n/a \\
% aqua      & 0.3415 & 0.3527 & 0.3527  & 0.2200 \\
% medmcqa        & 0.4231 & 0.4248 & 0.4248  & 0.1000 \\
% mmlu            & 0.5016 & 0.4993 & 0.4993  & 0.1600 \\
% sciq            & 0.8426 & 0.8396 & 0.8396  & 0.2700 \\

% \bottomrule
% \end{tabularx}
% \caption{Performance accuracy comparison across QA benchmarks}
% \label{tab:evaluations}
% \end{table*}% % % % % % % % % % % % % % % % % % 

% \begin{table*}[h!]
% \small
% \centering
% \begin{tabular*}{\textwidth}{l S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3]}
% \toprule
% \textbf{Model} & \textbf{Code Preferences} & \textbf{MNLP DPO} & \textbf{RewardMath} & \textbf{UltraFeedback} & \textbf{Metamath} \\
% \midrule
% Qwen3-0.6B & 0.619 & 0.599 & 0.588 & 0.455 & 0.845 \\
% Qwen3-0.6B-Base + SFT & 0.643 & 0.572 & 0.513 & 0.509 & 0.769 \\
% Qwen3-0.6B-Base + DPO & 0.918 & 0.814 & 0.609 & 0.627 & 0.949\\
% \textbf{Qwen3-0.6B-Base + SFT + DPO} & \textbf{0.928} & \textbf{0.841} & \textbf{0.624} & \textbf{0.645} &\textbf{0.947} \\
% \bottomrule
% \end{tabular*}
% \caption{DPO Performance of selected models across different preference pair selection benchmarks.}
% \label{tab:DPO pairs result}
% \end{table*}


\begin{table}[h!]
\scriptsize
\centering
\begin{tabular*}{0.95\linewidth}{@{\extracolsep{\fill}} l S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] }
\toprule
\textbf{Model} & \textbf{Chat} & \textbf{Chat Hard} & \textbf{Safety} & \textbf{Reasoning}\\
\midrule
Qwen1.5-0.5B-Chat & 0.355 & 0.629 & 0.570 & 0.598 \\
Qwen1.5-4B-Chat & 0.388 & 0.627 & 0.557 & 0.669 \\
Qwen1.5-72B-Chat & 0.623 & 0.660 & 0.676 & 0.855\\
Zephyr-7b-beta & 0.953 & 0.627 & 0.657 & 0.779 \\
Llama-3-tulu-2-dpo-8b & 0.953 & 0.535 & 0.665 & \textbf{0.866}  \\
Qwen3-0.6B & 0.2458	& \textbf{0.662} & 0.6919 & 0.6586 \\
Base + SFT & 0.673	& 0.5088	& 0.4743	& 0.5103	 \\
Base + DPO & 0.950	& 0.410	& 0.7432	& 0.7441	\\
\textbf{Base + SFT + DPO} & \textbf{0.958} &	0.360 &	\textbf{0.757} &	0.828 \\
\bottomrule
\end{tabular*}
\caption{RewardBench reward modelling metrics for the baseline \qwenb model and different finetuning combinations against the \qwenbbase reference model.}
\label{tab:DPO rewardbench}
\end{table}



\section{Analysis} 

\paragraph{MCQA model}  
\label{sec:analysis_mcqa_model}
% We conducted a qualitative error analysis on held-out examples. The combined \qwentulumcqamath model delivers richer implicit reasoning and fewer misinterpretations of negation or multi-step logic, for instance, it correctly solves chained AquaRat arithmetic where the SFT-only model often guesses. It still struggles with out-of-distribution external facts (e.g., recent figures) due to lack of retrieval. Removing the TuLU stage led to a 4-point drop on HellaSwag and SciQ, confirming the value of diverse instruction pre-training.

We analyzed held‐out examples and found that the \qwentulumcqamath model, trained via instruction, tuning to emit only the correct answer letter delivers the most consistent accuracy and clean outputs. Which then in the evaluation is assesed as correct. Other variants sometimes match or exceed its benchmark scores but frequently produce spurious tokens, repetitions, or full‐text answers. In contrast, our two‐stage model correctly handles chained AquaRat arithmetic where SFT‐only and LoRA models fail. Removing the TuLU stage causes a 4‐point drop on HellaSwag and SciQ, highlighting the critical role of instruction‐based tuning.  

\paragraph{Quantized model}
The quantization experiments demonstrated successful compression with minimal performance degradation. Both W8A8 and W4A8 schemes preserved nearly identical accuracy to the full-precision model across all benchmarks, with differences under 0.01 in most cases. This indicates that aggressive quantization can be applied to MCQA tasks without substantial quality loss.
Notably, W4A8 quantization required higher peak VRAM (1237.18 MB vs 1221.32 MB for W8A8) during inference, likely due to more complex dequantization operations required for 4-bit weights, despite producing a smaller final model. The two-stage SmoothQuant + GPTQ approach effectively maintained model quality while achieving significant memory savings, making quantized deployment viable for resource-constrained environments without compromising educational utility. The quantized models occasionally outperformed the base model because quantization acts as regularization, reducing overfitting, and the calibration process may have aligned representations better with certain tasks, though these small improvements likely fall within normal evaluation variance.

\paragraph{RAG model}
The effectiveness of RAG proved heavily contingent on the generator's foundational training. As detailed in Appendix \ref{appendix:rag_performance}, models extensively fine-tuned for MCQA, such as \qwentulumcqa{} and \qwentulumcqamath, gained no benefit from RAG and occasionally showed slight degradation. We hypothesize that these models overfit to the rigid MCQA prompt structure, treating retrieved context as distracting noise rather than an informative signal.
Conversely, the \qwentulu{} model, built on a broader instruction-following SFT, effectively leveraged retrieved context, including from custom embeddings. This is evidenced by the performance lift seen when comparing its baseline in Table \ref{tab:mcqa_results} to its RAG results in Table \ref{tab:rag_vidyc_base_model_tulu_sft_results}. This outcome reinforces our analysis in Section \ref{sec:analysis_mcqa_model}, confirming that general instruction tuning fosters the adaptability required to integrate external knowledge.
\paragraph{DPO model}

We observe that DPO significantly enhances the model's ability to assign rewards in alignment with human preferences across all evaluated benchmarks. Additionally, SFT prior to DPO leads to further improvements, though the gains are relatively modest. Notably, a compact model like Qwen3-0.6B, when fine-tuned with DPO in the right domains, demonstrates performance on par with or even exceeding that of much larger models such as Qwen1.5-72B-Chat, also fine-tuned with DPO. However, despite these improvements in reward modeling, we have also observed that DPO tends to encourage repetitive generations. We attribute this to a potential side effect of optimizing for preference pair selection, rather than directly improving the model's generative capabilities. In this setting, it is important to recognize that we are effectively training a reward model, not a generator. 

\section{Ethical considerations}
We fine-tune \qwenbbase{} to aid EPFL students with MC and open‐ended questions, raising several ethical points:
\begin{enumerate}[noitemsep, topsep=0pt, leftmargin=*]
  \item \textbf{Language Adaptation}: Extending to high‐resource languages is straightforward; low‐resource languages demand targeted data and fairness evaluation.
  \item \textbf{Benefits vs.\ Risks}: Improves learning support but may enable dishonesty, over‐reliance, or bias propagation.
  \item \textbf{Mitigation}: Provide clear guidelines, restrict to approved materials, conduct bias audits, and emphasize the tool as a supplement.
  \item \textbf{Equity}: Underrepresented students may be disadvantaged; diverse data and user feedback are essential.
\end{enumerate}

\section{Conclusion}
Our exploration of fine-tuning strategies for \qwenbbase reveals that a multi-stage curriculum is superior to direct task-specific training. We found that a foundational instruction-following SFT is essential for model adaptability, as further specialization on MCQA data introduces a critical trade-off: improved task accuracy at the cost of conversational ability due to format overfitting. Our key findings demonstrate that: 1) post-training quantization is highly effective, preserving performance with negligible loss; 2) a model's ability to leverage RAG is contingent on its conversational foundation, not its MCQA specialization; 3) Direct Preference Optimization (DPO) effectively aligns the model with human judgments, boosting the model's preference selection capabilties significantly. Primary limitations include the specialization-vs-adaptability trade-off . Future work should focus on mitigating this trade-off, alongside exploring larger models and adaptive retrieval to advance personalized STEM tutoring.

\clearpage

% Entries for the entire Anthology, followed by custom entries
\newpage
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\onecolumn
\section{Appendix}


\subsection{MCQA - Full prompt}
\label{apen: prompt}
% later, in your appendix
\begin{tcolorbox}[
  breakable,
  sharp corners,
  boxrule=0.5pt,
  colback=white,
  colframe=black,
  listing only,
  listing options={
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakanywhere=true,
    prebreak=\mbox{\tiny\ensuremath\hookleftarrow},
    postbreak=\mbox{\tiny\ensuremath\rightarrow}
  },
  width=\linewidth,
  left=0pt,
  right=0pt,
  top=1mm,
  bottom=1mm,
]
You are a helpful assistant, that answer STEM questions. Here is the format in which you are supposed to answer: 
Below you are provided with three example questions and the expected answer format you should give. Just answer with A, B, C, or D.

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.

Performance enhancing synthetic steroids are based on the structure of the hormone:
A. testosterone.
B. cortisol.
C. progesterone.
D. aldosterone.
Answer:A

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.
Asp235Phe in a molecular report indicates that:
A. asparagine has been replaced by phenylalanine.
B. phenylalanine has been replaced by asparagine.
C. aspartic acid has been replaced by phenylalanine.
D. phenylalanine has been replaced by aspartic acid.
Answer:C

The following are multiple choice questions (with answers) about knowledge and skills in advanced master-level STEM courses.
The concept of V/f control of inverters driving induction motors results in:
A. constant torque operation  
B. speed reversal  
C. reduced magnetic loss  
D. harmonic elimination  
Answer:A

Answer the following question in the same way:
\end{tcolorbox}

\subsection{Comparison of final and intermediate MCQA models across QA benchmarks}

Complementary results of the MCQA model are visible in this section. Table \ref{tab: MCQA2} shows the results across the different experiments of MCQA model. Table \ref{tab:mcqa_results} shows specific baselines and tuned models across benchmarks. highlighting top performers. 

\label{apen: comp}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXX}
\toprule
\textbf{Task} & \textbf{\qwentulumcqamath} & \textbf{MCQA SFT} & \textbf{MCQA SFT + LoRA} \\
\midrule
hellaswag       & 0.4345 & 0.2510 & 0.2600  \\
mathQA            & 0.3041 & 0.2394 & 0.2866  \\
aqua      & 0.3192 & 0.2200 & 0.2300  \\
medmcqa        & 0.4210 & 0.1200 & 0.1300  \\
mmlu            & 0.4981 & 0.3000 & 0.2500  \\
sciq            & 0.8315 & 0.4300 & 0.4500  \\

\bottomrule
\end{tabularx}
\caption{Performance accuracy comparison of final and intermediate MCQA models across QA benchmarks}
\label{tab: MCQA2}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Model} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
\qwenb{} & 0.2430 & 0.2295 & 0.3223 & 0.2419 & 0.2311 & 0.2946 & 0.2504 & 0.1879 \\
\qwenbbase{} & 0.2452 & 0.2344 & 0.3213 & 0.2419 & 0.2402 & 0.3013 & 0.2507 & 0.1896 \\
\qwentulumcqamath{} & 0.7565 & 0.5016 & 0.4231 & \textbf{0.3142} & \textbf{0.8426} & 0.3415 & 0.4633 & 0.3238 \\
\qwentulumcqa{} & \textbf{0.7590} & \textbf{0.5038} & 0.4236 & 0.3049 & 0.8416 & 0.3460 & \textbf{0.4817} & 0.3138 \\
MCQA SFT & 0.3089 & 0.2565 & 0.3261 & 0.2394 & 0.4087 & 0.3147 & 0.2510 & 0.1896 \\
MCQA SFT + LoRA & 0.2895 & 0.2651 & 0.2539 & 0.2866 & 0.4087 & 0.2567 & 0.2600 & 0.2282 \\
\qwentulu{} & 0.7193 & 0.4882 & 0.3854 & 0.2821 & 0.8305 & 0.2344 & 0.3623 & \textbf{0.3607} \\
\bottomrule
\end{tabularx}
\caption{Performance comparison of non-RAG MCQA models.}
\label{tab:mcqa_results}
\end{table*}


\subsection{RAG Performance Analysis}
\label{appendix:rag_performance}

We conducted a series of experiments to evaluate the effectiveness of Retrieval-Augmented Generation. These tests assessed how different generator models, retrieval embeddings (a custom-trained model versus the base \texttt{\graniteembed}), and the number of retrieved documents ($k$) impact performance across our evaluation suite.

Our initial experiments, detailed in Table \ref{tab:rag_Qwen_Qwen3_0_6B_Base_results} and Table \ref{tab:rag_Qwen_Qwen3_0_6B_results}, highlight a critical prerequisite for successful RAG implementation. The \texttt{\qwenbbase} model, which is a raw pretrained model, lacks the ability to follow instructions or synthesize retrieved context, resulting in performance near random chance. Even the instruction-tuned \texttt{\qwenb} model fails to show any meaningful improvement, suggesting that a generic instruction-following capability is insufficient for this task.

The first model to demonstrate a significant positive interaction with RAG is the foundational \texttt{\qwentulu} model (Table \ref{tab:rag_vidyc_base_model_tulu_sft_results}). Having been trained on the high-quality \texttt{\tuluds} mixture, it is adept at leveraging retrieved context, showing a substantial performance lift over the baseline models. This indicates that our foundational instruction-following approach creates a model that is "RAG-ready" and benefits markedly from external knowledge.

Interestingly, for our most specialized models, \texttt{\qwentulumcqa} and \texttt{\qwentulumcqamath}, applying RAG often leads to a slight degradation in performance compared to their non-RAG counterparts (as seen in Table \ref{tab:mcqa_results}). We hypothesize that these models have become highly optimized for the MCQA format patterns. For them, the introduction of external, potentially noisy context from the retriever can act as a distraction rather than an aid, causing their performance to dip below its non-RAG peak. Despite this, these models are our best-performing models overall. Their absolute scores, even with the slight RAG-induced degradation, are significantly higher than the RAG-augmented \texttt{\qwentulu} model on most benchmarks, justifying their selection as our final models.

\paragraph{Custom Embedding Training}
To create the custom retrieval embeddings referenced in our experiments, we fine-tuned a model specifically on our domain data. We employed an unsupervised strategy using the \texttt{sentence-transformers} library, following a Transformer-based Denoising Auto-Encoder (TSDAE) approach. The training corpus was composed of text chunks from our EPFL and STEM PDF knowledge base. Starting with the \texttt{sentence-transformers/all-MiniLM-L6-v2} base model, we generated training examples by corrupting each chunk via random word deletion. The model was then trained for 5 epochs with a \texttt{MultipleNegativesRankingLoss} objective. This method teaches the model to map a corrupted text to the same embedding space as its original version, thereby learning robust, domain-specific representations. A 10\% validation split and an early stopping protocol were used to prevent overfitting.

%--- The new tables go here ---

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & \textbf{0.2458} & {0.2324} & \textbf{0.3242} & {0.2419} & {0.2331} & {0.2969} & {0.2504} & \textbf{0.1896} \\
Custom Granite (k=5) & 0.2432 & \textbf{0.2314} & 0.3235 & 0.2419 & {0.2331} & 0.2946 & {0.2504} & 0.1862 \\
Custom Granite (k=10) & 0.2438 & 0.2305 & 0.3227 & 0.2419 & 0.2321 & 0.2946 & \textbf{0.2504} & 0.1862 \\
IBM Granite (k=2) & 0.2430 & 0.2296 & 0.3218 & 0.2419 & 0.2311 & 0.2946 & {0.2504} & 0.1879 \\
IBM Granite (k=5) & 0.2432 & 0.2295 & 0.3223 & 0.2419 & 0.2311 & 0.2946 &{0.2504} & 0.1862 \\
IBM Granite (k=10) & 0.2432 & 0.2295 & 0.3225 & 0.2419 & 0.2311 & 0.2946 & {0.2504} & 0.1846 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwenbbase{} generator model.}
\label{tab:rag_Qwen_Qwen3_0_6B_Base_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
Custom Granite (k=5) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
Custom Granite (k=10) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=2) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=5) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
IBM Granite (k=10) & {0.2430} & {0.2295} & {0.3223} & {0.2419} & {0.2311} & {0.2946} & {0.2504} & {0.1879} \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwenb{} generator model.}
\label{tab:rag_Qwen_Qwen3_0_6B_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7435 & 0.4944 & 0.4186 & 0.3118 & 0.8345 & 0.3393 & 0.4333 & 0.3087 \\
Custom Granite (k=5) & 0.7455 & 0.4951 & 0.4121 & 0.3138 & 0.8385 & 0.3348 & 0.4200 & 0.3087 \\
Custom Granite (k=10) & 0.7444 & 0.4946 & \textbf{0.4210} & \textbf{0.3159} & 0.8375 & 0.3281 & 0.4191 & 0.2987 \\
IBM Granite (k=2) & \textbf{0.7483} & \textbf{0.4956} & 0.4196 & 0.3114 & \textbf{0.8416} & 0.3415 & \textbf{0.4563} & \textbf{0.3171} \\
IBM Granite (k=5) & 0.7472 & 0.4953 & 0.4143 & 0.3057 & \textbf{0.8416} & 0.3415 & 0.4328 & 0.3037 \\
IBM Granite (k=10) & 0.7438 & 0.4925 & 0.4162 & 0.3126 & 0.8365 & \textbf{0.3460} & 0.4334 & 0.3003 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulumcqamath{} generator model.}
\label{tab:rag_RikoteMaster_tulu_ft_arc_math_mcqa_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7489 & 0.4971 & \textbf{0.4198} & 0.2972 & 0.8355 & 0.3415 & 0.4485 & 0.3003 \\
Custom Granite (k=5) & 0.7506 & 0.4996 & 0.4181 & \textbf{0.3024} & 0.8385 & \textbf{0.3616} & 0.4429 & 0.3020 \\
Custom Granite (k=10) & 0.7497 & 0.4973 & 0.4176 & 0.2959 & 0.8375 & 0.3393 & 0.4383 & 0.2869 \\
IBM Granite (k=2) & \textbf{0.7523} & \textbf{0.5025} & 0.4153 & 0.2972 & \textbf{0.8396} & 0.3393 & \textbf{0.4726} & \textbf{0.3121} \\
IBM Granite (k=5) & 0.7486 & 0.4989 & 0.4172 & 0.2943 & 0.8365 & 0.3549 & 0.4538 & 0.2852 \\
IBM Granite (k=10) & 0.7483 & 0.4993 & 0.4186 & 0.2963 & 0.8365 & 0.3504 & 0.4504 & 0.2768 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulumcqa{} generator model.}
\label{tab:rag_RikoteMaster_tulu_ft_mcqa_results}
\end{table*}

\begin{table*}[h!]
\small
\centering
\begin{tabularx}{\textwidth}{lXXXXXXXX}
\toprule
\textbf{Retriever (k docs)} & \textbf{ARC} & \textbf{MMLU} & \textbf{MedMCQA} & \textbf{MathQA} & \textbf{SciQ} & \textbf{AQuA} & \textbf{Hellaswag} & \textbf{EPFL} \\
\midrule
Custom Granite (k=2) & 0.7125 & \textbf{0.4923} & 0.3715 & \textbf{0.3008} & 0.8325 & 0.2746 & 0.4116 & \textbf{0.3507} \\
Custom Granite (k=5) & 0.7148 & 0.4868 & 0.3756 & 0.2919 & \textbf{0.8345} & 0.2746 & 0.4139 & 0.3440 \\
Custom Granite (k=10) & \textbf{0.7227} & 0.4907 & \textbf{0.3760} & 0.2919 & 0.8254 & \textbf{0.2835} & \textbf{0.4165} & 0.3389 \\
IBM Granite (k=2) & 0.7083 & 0.4816 & 0.3569 & 0.2829 & 0.8325 & 0.2790 & 0.3917 & 0.3490 \\
IBM Granite (k=5) & 0.7182 & 0.4848 & 0.3732 & 0.2780 & 0.8295 & 0.2612 & 0.4000 & 0.3322 \\
IBM Granite (k=10) & 0.7159 & 0.4871 & 0.3732 & 0.2785 & 0.8274 & 0.2701 & 0.4039 & 0.3406 \\
\bottomrule
\end{tabularx}
\caption{RAG performance using the \qwentulu{} generator model.}
\label{tab:rag_vidyc_base_model_tulu_sft_results}
\end{table*}

\clearpage
\subsection{In-depth Experimental details}
\label{apen: experimental details}
The different training parameters, base models and datasets used in each case are shown in this section.

\subsubsection{Foundational training Experimental Details}

We fine-tuned \texttt{Qwen/Qwen3-0.6B-Base} on the \texttt{\tuluds} for one epoch using per-device batch size 2, gradient accumulation 8, learning rate $2\times10^{-5}$, and max gradient norm 1.0. Training employed BF16 precision, gradient checkpointing, and a linear LR scheduler with warmup ratio 0.03. Logging every 20 steps, saving every 100 steps, reporting to WandB and with Seed 42.

\begin{lstlisting}[style=config, language=yaml, caption={SFT fine-tuning hyperparameters}]
seed: 42 
overwrite_output_dir: True

# Training settings
num_train_epochs: 1       
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 0.00002
max_grad_norm: 1.0

# Precision & stability
bf16: True
gradient_checkpointing: True
remove_unused_columns: False

# Logging & tracking
logging_steps: 20
save_steps: 100
disable_tqdm: False

# LR scheduler
lr_scheduler_type: "linear"
warmup_ratio: 0.03
\end{lstlisting}
\subsubsection{SFTs for MCQA Experimental Details}

We fine-tuned the model outputted from the foundational training stage using a two-phase curriculum learning approach, with hyperparameters detailed in Listing \ref{lst:sft_hyperparams}.
First, for general MCQA adaptation, the model was trained on a broad mixture of datasets (\texttt{\unimcqa}) with a learning rate of $6 \times 10^{-6}$.
Subsequently, to enhance mathematical and logical reasoning, the resulting model was further fine-tuned on the \texttt{\allenaiarc} dataset split using a more conservative learning rate of $2 \times 10^{-6}$. For both stages, we trained for one epoch using the AdamW optimizer, BF16 precision, gradient checkpointing, and a cosine learning rate scheduler with 50 warmup steps. The best model was selected based on evaluation loss.
\clearpage
\begin{lstlisting}[style=config, language=yaml, caption={SFT fine-tuning hyperparameters for the two-phase curriculum. The primary difference between stages is the learning rate.}, label={lst:sft_hyperparams}]
# General settings for both stages
seed: 42
num_train_epochs: 1
optim: "adamw_torch"
lr_scheduler_type: "cosine"
warmup_steps: 50
weight_decay: 0.01
max_grad_norm: 1.0

# Batching (effectively managed by smart batching)
per_device_train_batch_size: 1
gradient_accumulation_steps: 1

# Precision & Memory Optimization
bf16: True
gradient_checkpointing: True
remove_unused_columns: True

# Checkpoint Saving & Evaluation
save_strategy: "steps"
save_steps: 200
save_total_limit: 4
load_best_model_at_end: True
metric_for_best_model: "eval_loss"
greater_is_better: False
eval_strategy: "steps"
eval_steps: 200

# Logging
logging_steps: 50
report_to: "wandb"
disable_tqdm: False

# --- Stage-Specific Learning Rates ---
# Stage 1: General MCQA adaptation (\unimcqa)
learning_rate: 6.0e-6

# Stage 2: Math & logic reasoning (\allenaiarc)
# learning_rate: 2.0e-6
\end{lstlisting}
\subsubsection{Quantized Experimental Details}
We conducted model quantization experiments using the \texttt{llm-compressor} library to optimize the MCQA final model for efficient inference. The base model was loaded with automatic device mapping and \texttt{float16} precision to manage memory usage effectively. For calibration data, We applied a two-stage quantization recipe combining SmoothQuant (smoothing strength 0.8) for activation conditioning followed by GPTQ quantization targeting all Linear layers while excluding the language modeling head (lm\_head). Two quantization schemes were evaluated: W8A8 (8-bit weights and 8-bit activations) and W4A8 (4-bit weights and 8-bit activations), both implemented through the oneshot API. The quantization process utilized the full calibration dataset for compression statistics, and the resulting models were saved using the save\_compressed flag. Post-quantization validation was performed through sample text generation to verify model functionality.

\subsubsection{DPO Experimental Details}
We fine-tuned our DPO models using a single epoch training regime optimized for resource efficiency. Training was conducted with a batch size of 6 per device and gradient accumulation over 8 steps to simulate larger batch training. A linear learning rate scheduler was employed with an initial learning rate of 5e-5 and a warmup ratio of 0.1. Optimization was performed using the AdamW algorithm as implemented in PyTorch. Mixed precision training was enabled via \texttt{bf16}, and gradient checkpointing was used to reduce memory consumption. A regularization term with a strength of 0.01 was included via the \texttt{beta} parameter.

We performed training with the DPOTrainer class from Huggingface over the described DPO dataset in section \ref{sec:dpo_data}

% \clearpage
\begin{lstlisting}[style=config, language=yaml, caption={DPO fine-tuning hyperparameters}]
seed: 42

# Training settings
num_train_epochs: 1
per_device_train_batch_size: 6
per_device_eval_batch_size: 6
gradient_accumulation_steps: 8
dataset_num_proc: 6
learning_rate: 0.00005
optim: "adamw_torch"
beta: 0.01

# Precision & stability
bf16: True
gradient_checkpointing: True
remove_unused_columns: False

# Logging & tracking
logging_steps: 10
save_strategy: "epoch"
eval_strategy: "steps"
eval_steps: 100

# LR scheduler
lr_scheduler_type: linear
warmup_ratio: 0.1
\end{lstlisting}

\end{document}


ML PROJECT 02.2025 FOR EPFL COURSE CS-MACHINE-LEARNING
\documentclass[conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{booktabs} % For improved table formatting
\usepackage{caption}  % For customizing captions
\usepackage{anyfontsize}
\usepackage{amsmath, amssymb, siunitx}


\begin{document}
%\fontsize{9pt}{11.1pt}\selectfont
\title{The Impact of Whole-Slide Image Resolution on Foundation Model Performance in Computational Pathology}

\author{
  Mario Rico Ibáñez, Daniel López Gala, Carlos Hurtado Comín\\
  Supervised by: Sevda Öğüt, Cédric Vincent-Cuaz, Vaishnavi Subramanian\\
  \textit{LTS4, EPFL}
}

\maketitle

% FEEDBACK ON CODE
% + codes are well-documented
% - there are not enough comments in your codes 

% GENERAL FEEDBACK
% - no citations

\begin{abstract}
    Digital pathology leverages deep learning to analyze Whole-Slide Images (WSIs), but foundation models like UNI are typically trained at a fixed magnification. In this paper, we systematically evaluate UNI’s robustness across multiple magnifications using three breast histology datasets (BACH, BRACS, BreakHis). Our results show that while UNI generally maintains performance under resolution changes, magnification shifts influence feature quality and classification outcomes. Gated attention and non-linear heads improve consistency, guiding the development of more adaptable digital pathology pipelines.
\end{abstract}

\section{Introduction}

Machine learning is at the forefront of research across diverse domains, including biomedicine. Within this landscape, digital pathology—digitizing and analyzing tissue samples on glass slides using high-resolution Whole-Slide Images (WSIs)—has gained increasing prominence. These WSIs are generated by scanning traditional glass slides at high magnification, producing gigapixel-scale images suitable for computational analysis. Deep learning has revolutionized the workflow in digital pathology, enabling automated disease diagnosis, prognostic assessment, and therapy response prediction.

As deep learning has matured, \textit{foundation models} have emerged as large-scale models, often based on Vision Transformers (ViTs)~\cite{dosovitskiy2020image}, that are pre-trained on vast and diverse datasets and can then be adapted to a broad array of downstream tasks with minimal fine-tuning. These models have become increasingly popular due to their capability to learn general-purpose representations. For instance, the recently introduced UNI model~\cite{chen2024towards}, a foundation model for histopathological images, was trained exclusively on images scanned at a specific magnification (20x, corresponding to approximately 0.42 microns per pixel). Analyzing WSIs across magnifications is critical because diagnostic-relevant features appear at multiple scales, and without understanding how resolution shifts impact performance, models trained at a single magnification risk failing under real-world conditions where scanning parameters naturally vary.

While UNI’s authors report that the model is robust to magnification differences, a systematic study on this is lacking, raising questions about the stability of these representations at varying scales. The existing literature does not comprehensively address whether foundation models pre-trained at a single magnification level can generalize to lower or higher magnifications. Understanding this is critical, as magnification-dependent variability can influence patch-level embeddings and subsequent classification performance. In this work, we seek to fill this gap by analyzing the robustness of the UNI model across multiple magnifications and datasets. We evaluate how feature quality is affected by changes in WSI resolution through a series of experiments on three distinct breast histology datasets (BACH~\cite{aresta2019bach}, BRACS~\cite{brancati2022bracs}, and BreakHis~\cite{spanhol2015dataset}). We compare the performance of downstream tasks across magnification levels on an array of Multiple-Instance Learning (MIL)~\cite{dietterich1997solving} models. 
Our results identify when foundation models work well at different magnifications, helping researchers and clinicians develop and use these models more effectively in real-world practice.

In summary, we (i) present a systematic approach to evaluating resolution effects across datasets and magnification levels, and (ii) apply our approach to the UNI model. These findings pave the way for digital pathology solutions that maintain accuracy and reliability, regardless of magnification differences.

\section{Tissue sample Classification}

Our approach to evaluating resolution effects on UNI embeddings is based on an image classification pipeline. We assess UNI's robustness to differences in magnification by training a set of models over the same dataset processed at different magnifications and comparing performance on an image classification task. Classifying an image involves three steps: First, we split the images into equally sized patches; Second, we get the UNI model embeddings from those patches; Last, we classify the image from the image patch embeddings with Multiple-Instance Learning (MIL).

\subsection{Tiling and Preprocessing}

Tiling is critical in the preprocessing pipeline, uniformly dividing the regions of interest (RoIs) of WSIs across multiple datasets and magnification levels. This process ensures efficient patch extraction while maintaining consistency in resolution and magnification, which is necessary to have a consistent input for our models. This section describes the methodology used to generate and validate the tiles.

We generate tiles by dividing the RoI images into non-overlapping patches. The grid generation ensures no tiles overlap based on patch size, desired magnification level, and base magnification level specific to the dataset. An image's microns-per-pixel (MPP) defines the base magnification level. The desired magnification level is achieved by downsampling the image.
We calculate the patch pixel size by adjusting the sampling grid using the MPP values corresponding to the desired magnification so that they are proportional to 224x224 pixels. Each dataset has a predefined base MPP value(s). Tiles are scaled proportionally for uniformity across magnifications.
We save the tiling and relevant metadata including the coordinates, magnification level, and other attributes in a structured JSON for each processed image.

Based on MIL principles, we create non-overlapping tiles, even if this results in some tiles containing padding. MIL is a machine learning framework in which data is organized into bags containing multiple instances. Labels are assigned at the bag level, and the model must learn to infer the information at the instance level that contributes to the bag prediction. A key assumption in MIL is that instances are independently and identically distributed (i.i.d.). 
Ensuring that tiles do not overlap helps maintain this assumption, preventing information leakage between tiles and increasing the reliability of downstream analyses. This design choice is particularly relevant to our project, as it is consistent with the MIL foundations and ensures that the extracted tiles provide robust and unbiased input to train our models.

We developed a verification pipeline to ensure this process was correct and that the generated coordinates corresponded to tiles without overlapping.
\begin{itemize}
    \item \textbf{Visualization}: Tiles are drawn on the WSIs as rectangles of different colors. This way, we can verify proper coverage of the image and alignment with the computed grid.
    \item \textbf{Overlap Validation}: An overlap detection algorithm calculates the percentage of overlap between any two tiles, logging the results to confirm no overlap between tiles.
\end{itemize}

Figure \ref{fig:tiling_visualization} shows an example of this process, comparing a RoI from the BRACS dataset across different magnifications (5x, 10x, and 20x). Rectangles of different colors represent individual tiles.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{assets/n009_5x_tiles.png}
    \includegraphics[width=0.3\textwidth]{assets/n009_10x_tiles.png}
    \includegraphics[width=0.3\textwidth]{assets/n009_20x_tiles.png}
    \caption{Comparison of tiling applied to a Region of Interest (RoI) from the BRACS dataset at 5x (left), 10x (center), and 20x (right) magnifications. Each rectangle represents a tile, ensuring non-overlapping coverage.}
    \label{fig:tiling_visualization}
\end{figure*}


\subsection{UNI Embedding and Modeling}

Once we have the image patches, we get the UNI embedding for each patch. For each image, we aggregate the patches to obtain a single embedding. In Section ~\ref{sec:experiments} we provide more details on the approaches and datasets used for classification.

% describe models tested and why: knn, deep learning and their setups

\section{Experimental setup}
\label{sec:experiments}

\subsection{Datasets}

\textbf{BACH}. A dataset of $400$ microscopy images, i.e., regions of interest of the breast-tissue whole-slide images. The $400$ samples are uniformly distributed as: \textit{Normal} ($100$), \textit{Benign} ($100$), \textit{in situ carcinoma} ($100$), and \textit{invasive carcinoma} ($100$). All the images are the same size and at a pixel scale of $0.42$ MPP, a 20x magnification. We classify the images between the 4 classes and compare performance over the magnification levels: 20x, 10x, and 5x.


\textbf{BRACS}. A dataset of $4539$ labeled regions of interest of breast-tissue whole-slide images of $189$ patients. The tumor samples are distributed as \textit{Benign}: \textit{Normal} ($484$), \textit{Pathological Benign} ($836$), or \textit{Usual Ductal Hyperplasia} ($517$); \textit{Atypical}: \textit{Flat Epithelial Atypia} ($756$), and \textit{Atypical Ductal Hyperplasia} ($507$); and \textit{Malignant}: \textit{Ductal Carcinoma in situ} ($790$), and \textit{Invasive Carcinoma} ($649$). The ROI images are of varying sizes and $0.25$ MPP, i.e., 40x. We classify the images between the 7 classes and compare performance over the magnification levels: 40x, 20x, 10x, and 5x.

\textbf{BreakHis}. A dataset of $1995$ labeled microscopic images of breast tumor tissue collected from $82$ patients. The samples are distributed as \textit{Benign}: \textit{Adenosis} ($114$), \textit{Fibroadenoma} ($253$), \textit{Phyllodes Tumor} ($109$), or \textit{Tubular Adenoma} ($149$); and \textit{Malignant}: \textit{Ductal Carcinoma} ($864$), \textit{Lobular Carcinoma} ($156$), \textit{Mucinous Carcinoma} ($205$), or \textit{Papillary Carcinoma} ($145$). All the images are the same size and at a magnification level of 40x. We classify the images between the 8 classes and compare performance over the magnification levels: 40x, 20x, 10x, and 5x.

\subsection{Models}
\label{sec:experimental_setup}

To evaluate the performance of embeddings generated by the UNI model at various magnification levels, we implemented a structured training, validation, and testing protocol. Our experiments consider two primary aggregation strategies—\textbf{mean-pooling} and \textbf{Gated Attention}—combined with two different classification heads: a linear classifier and an MLP with a 128-dimensional hidden layer. We also probe UNI embeddings with k-NN to define a non-parametric baseline and explore class separability without transforming the embeddings.

Regarding aggregation, recall that each image $I$ is subdivided into $T$ non-overlapping tiles. The UNI model is applied to each tile, producing a set of embeddings $\{h_k\}_{k=1}^T$. Under a MIL framework, these tile-level embeddings must be integrated into a single image-level representation before classification. We therefore examine the following aggregation methods:

\begin{itemize}
    \item \textbf{Mean-Pooling}
    \[
    h_I = \frac{1}{T} \sum_{k=1}^{T} h_k.
    \]

    \item \textbf{Gated Attention}
    \[
    h_I = \sum_{k=1}^{T} \alpha_k h_k,
    \]
    where the attention weights $\alpha_k$ are defined as:
    \[
    \alpha_k = \frac{\exp \left( \mathbf{w}^\top \left( \tanh(\mathbf{V} h_k) \odot \mathrm{sigm}(\mathbf{U} h_k) \right) \right)}{\sum_{j=1}^{T} \exp \left( \mathbf{w}^\top \left( \tanh(\mathbf{V} h_j) \odot \mathrm{sigm}(\mathbf{U} h_j) \right) \right)},
    \]
    where $\mathbf{U} \in \mathbb{R}^{d \times D}$, $\mathbf{V} \in \mathbb{R}^{d \times D}$ are trainable weight matrices, and $\mathbf{w} \in \mathbb{R}^d$ is a learnable weight vector. The symbol $\odot$ denotes element-wise multiplication, while $\tanh$ and $\mathrm{sigm}$ represent the hyperbolic tangent and sigmoid functions, respectively~\cite{ilse2018attention}.
\end{itemize}

By integrating tile-level information through these aggregation methods, we obtain a comprehensive feature representation $h_I$ for the entire image. This feature vector is then passed to either a linear classifier or an MLP classifier. The MLP consists of a single hidden layer of size 128, followed by a non-linear activation and a final linear layer mapping to the output classes.

To facilitate a fair comparison of models and magnification levels, we employ the weighted F1 score, a metric that mitigates the impact of class imbalance on the evaluation. To enhance reproducibility and reliability, we use a 5-fold cross-validation scheme. Models are trained until convergence with early stopping based on validation set performance, employing a patience of 20 epochs to prevent overfitting. After selecting the best model configuration (aggregation strategy and classifier type) for each magnification level and dataset, we use the chosen model to generate predictions on the held-out test set.

%---------------------------------------------------------
% Draft for the Results Section
%---------------------------------------------------------
\section{Results}
\label{sec:results}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/validation_metrics.png}
    \caption{Validation weighted F1 scores comparing Mean vs. Gated Attention aggregation and Linear vs. MLP classification heads across different magnification levels. Shaded areas represent standard deviations over 5-fold cross-validation folds. The figure shows that GA and MLP configurations generally yield stronger results.}
    \label{fig:validation_metrics_aggregation}
\end{figure}

Figure~\ref{fig:validation_metrics_aggregation} presents the validation weighted F1 scores under different combinations of aggregation (Mean vs. GA) and classifier heads (Linear vs. MLP) across various magnification levels. Each subplot (Mean Linear, Mean MLP, GA Linear, GA MLP) shows trends for the three datasets, enabling a direct comparison.
We observe that aggregation through gated attention performs marginally better than mean pooling, indicating that learning instance-level importance better captures discriminative patterns in histopathological images. Moreover, the MLP head generally provides a performance boost over the linear head. The additional nonlinearity helps model complex tissue patterns that simple linear boundaries cannot.

Our final evaluation focuses on the test sets of the considered datasets. After using the validation phase to select the best combination of aggregation strategy, classifier head, and magnification level, we assessed the chosen models on the held-out test data.
Figure~\ref{fig:test_aug_performance} illustrates the weighted F1 scores on the test sets across various magnification factors for BACH, BRACS, and BreakHis. Classification performance on different magnification levels is generally consistent across datasets. Notably, \textit{BreakHis} consistently benefits from higher magnification levels. In contrast, \textit{BACH} and \textit{BRACS} demonstrate relatively stable performance across magnification factors. Both show slightly lower performance on higher resolutions, which may imply that relevant features for classification may not be captured with finer-grained tiling.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/test_metrics.jpeg}
    \caption{Best test weighted-F1 scores across varying magnification factors for BACH, BRACS, and BreakHis. We observe comparable performance across magnification levels for all datasets, with BreakHis benefiting from higher-resolution tiling.}
    \label{fig:test_aug_performance}
\end{figure}

Finally, tables~\ref{tab:knnvalidation_metrics_bach}, \ref{tab:knnvalidation_metrics_bracs}, and \ref{tab:knnvalidation_metrics_breakhis} show a strong performance of our k-NN probing relative to the previously discussed parametric models. This indicates that the UNI embeddings of images in the same class are clustered. Moreover, this explains why the marginal gain of the MLP head is not that significant, as the data are already almost linearly separable.



\section{Ablation Studies}
\label{sec:ablation}

We conducted ablation studies to understand the factors influencing performance. We isolate the impact of magnification levels, aggregation strategies, and classification models. Our results for this section are in the Appendix, in tables~\ref{tab:validation_metrics_bach}-\ref{tab:knnvalidation_metrics_breakhis}
The ablation studies highlight several findings that are consistent with the results discussed in~\ref{sec:results}, mainly that aggregation method matters: GA outperforms simple mean-pooling, indicating the importance of learning instance-level weights; and model complexity slightly pays off: The MLP classifier generally surpasses the linear alternative, by learning non-linear relationships in the data.

We also observe that introducing dropout regularization does not significantly impact the models' classification performance. Moreover, some models perform better without regularization. Another interesting finding is that k-NN performance is best on cosine similarity and with small values of k. This is consistent with the literature on the curse of dimensionality, where most of the data lies at the class boundaries, and cosine similarity works better than Euclidean distance on high dimensional data~\cite{lee-1999-measures}.


\section{Conclusion}

In this paper, we investigated how the UNI foundation model for histopathology—trained exclusively at a fixed magnification—performs when applied to WSIs at multiple resolutions. Our experiments across BACH, BRACS, and BreakHis datasets show that UNI can maintain competitive performance despite changes in magnification levels. Future work should explore domain adaptation strategies, incorporate additional tissue types, and assess model resilience against more pronounced variations in imaging parameters, ultimately aiming to create universally applicable foundation models in computational pathology.

\section{Ethical risks}

% see https://github.com/epfml/ML_course/blob/main/projects/project2/project2_description.pdf

The primary ethical risk identified in this project is related to the datasets used, which include WSIs containing sensitive patient information, particularly concerning cancer diagnosis. These datasets, i.e., BACH, BRACS, and BreakHis, are critical to advancing computational pathology but must be handled with the highest care to protect patient privacy.

\textbf{Risk description:} The WSIs used in this study are derived from \textbf{patient pathology slides}. Although the images are anonymized, there is a residual risk of re-identification, particularly when combining metadata such as patient IDs, tumor characteristics, or diagnostic results. This risk affects patients, the primary stakeholders, and healthcare providers, responsible for maintaining data confidentiality. Negative impacts include patient privacy breaches and potential misuse of sensitive health information.

\textbf{Severity and likelihood:} The severity of this risk is high due to the sensitive nature of cancer diagnostic data which, if compromised, could have significant personal, social, or professional consequences. However, the likelihood of occurrence is low due to the anonymization practices applied to the datasets and the established protocols for secure data sharing.

\textbf{Risk Mitigation:} We have taken the following measures to address this risk:

\begin{itemize}
    \item Ensure that the datasets used in this project are fully anonymized before use. This includes removing all identifiable patient information and restricting access to authorized collaborators only.
    \item Follow the ethical guidelines for medical datasets, including compliance and data licensing agreements.
    \item Use EPFL resources such as Izar and the Research Computing Platform (RCP), which prevent unauthorized use and encrypt all data storage.
\end{itemize}

\textbf{Evaluation process:} This risk was evaluated by a close review of the documentation of the dataset, ensuring that all images are anonymized and free of identifiable metadata.

By implementing these safeguards, we minimize the ethical risks associated with patient re-identification while contributing to advancing computational pathology research.

\bibliographystyle{unsrt}
\bibliography{literature}

\onecolumn
\appendix

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.933 $\pm$ 0.058 & 0.940 $\pm$ 0.036 & 0.944 $\pm$ 0.028 & 0.939 $\pm$ 0.043 & 0.928 $\pm$ 0.049 & 0.946 $\pm$ 0.033 & 0.952 $\pm$ 0.029 & \textbf{0.959 $\pm$ 0.024} & 0.955 $\pm$ 0.033 & 0.944 $\pm$ 0.014 & 0.947 $\pm$ 0.025 & 0.943 $\pm$ 0.028 \\
10x & 0.953 $\pm$ 0.011 & 0.953 $\pm$ 0.011 & 0.947 $\pm$ 0.018 & 0.959 $\pm$ 0.018 & \textbf{0.966 $\pm$ 0.020} & 0.959 $\pm$ 0.021 & 0.941 $\pm$ 0.018 & 0.944 $\pm$ 0.008 & 0.944 $\pm$ 0.018 & 0.953 $\pm$ 0.019 & 0.956 $\pm$ 0.013 & 0.956 $\pm$ 0.007 \\
20x & 0.934 $\pm$ 0.020 & 0.938 $\pm$ 0.024 & 0.947 $\pm$ 0.024 & 0.953 $\pm$ 0.022 & 0.944 $\pm$ 0.018 & 0.956 $\pm$ 0.020 & 0.922 $\pm$ 0.028 & 0.928 $\pm$ 0.044 & 0.934 $\pm$ 0.028 & 0.950 $\pm$ 0.030 & 0.960 $\pm$ 0.026 & \textbf{0.969 $\pm$ 0.022} \\

\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, BACH.}
\label{tab:validation_metrics_bach}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.711 $\pm$ 0.037 & 0.757 $\pm$ 0.013 & 0.755 $\pm$ 0.010 & 0.769 $\pm$ 0.006 & 0.767 $\pm$ 0.008 & 0.769 $\pm$ 0.006 & 0.733 $\pm$ 0.039 & 0.768 $\pm$ 0.010 & 0.769 $\pm$ 0.010 & \textbf{0.783 $\pm$ 0.009} & 0.776 $\pm$ 0.011 & 0.776 $\pm$ 0.010 \\
10x & 0.765 $\pm$ 0.018 & 0.764 $\pm$ 0.013 & 0.764 $\pm$ 0.012 & 0.783 $\pm$ 0.014 & 0.787 $\pm$ 0.012 & 0.785 $\pm$ 0.015 & 0.768 $\pm$ 0.005 & 0.770 $\pm$ 0.012 & 0.769 $\pm$ 0.008 & \textbf{0.798 $\pm$ 0.015} & 0.792 $\pm$ 0.011 & 0.794 $\pm$ 0.014 \\
20x & 0.777 $\pm$ 0.006 & 0.773 $\pm$ 0.007 & 0.774 $\pm$ 0.010 & 0.797 $\pm$ 0.009 & 0.800 $\pm$ 0.007 & 0.795 $\pm$ 0.010 & 0.769 $\pm$ 0.007 & 0.764 $\pm$ 0.002 & 0.769 $\pm$ 0.008 & 0.801 $\pm$ 0.009 & 0.796 $\pm$ 0.007 & \textbf{0.802 $\pm$ 0.005} \\
40x & 0.760 $\pm$ 0.007 & 0.756 $\pm$ 0.008 & 0.748 $\pm$ 0.036 & 0.787 $\pm$ 0.008 & \textbf{0.787 $\pm$ 0.006} & 0.782 $\pm$ 0.006 & 0.742 $\pm$ 0.008 & 0.732 $\pm$ 0.011 & 0.743 $\pm$ 0.011 & 0.781 $\pm$ 0.004 & 0.782 $\pm$ 0.010 & 0.786 $\pm$ 0.008 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, for BRACS.}
\label{tab:validation_metrics_bracs}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Aggregation}  & \multicolumn{6}{c}{\textbf{Mean}} & \multicolumn{6}{c}{\textbf{Gated Attention}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13} \textbf{Model} 
                   & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} & \multicolumn{3}{c}{\textbf{Linear}} & \multicolumn{3}{c}{\textbf{MLP}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}  \textbf{Dropout}
                   & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 & 0.0 & 0.2 & 0.4 \\
\midrule
5x & 0.811 $\pm$ 0.036 & 0.833 $\pm$ 0.022 & 0.848 $\pm$ 0.022 & 0.884 $\pm$ 0.018 & 0.881 $\pm$ 0.018 & 0.878 $\pm$ 0.020 & 0.803 $\pm$ 0.042 & 0.845 $\pm$ 0.027 & 0.827 $\pm$ 0.027 & \textbf{0.886 $\pm$ 0.021} & 0.879 $\pm$ 0.020 & 0.881 $\pm$ 0.024 \\
10x & 0.874 $\pm$ 0.058 & 0.910 $\pm$ 0.026 & 0.897 $\pm$ 0.049 & \textbf{0.938 $\pm$ 0.025} & 0.938 $\pm$ 0.018 & 0.934 $\pm$ 0.018 & 0.906 $\pm$ 0.032 & 0.906 $\pm$ 0.028 & 0.911 $\pm$ 0.024 & 0.936 $\pm$ 0.025 & 0.933 $\pm$ 0.022 & 0.933 $\pm$ 0.022 \\
20x & 0.888 $\pm$ 0.036 & 0.906 $\pm$ 0.062 & 0.944 $\pm$ 0.010 & 0.966 $\pm$ 0.012 & 0.963 $\pm$ 0.008 & 0.967 $\pm$ 0.008 & 0.930 $\pm$ 0.012 & 0.934 $\pm$ 0.010 & 0.939 $\pm$ 0.013 & 0.965 $\pm$ 0.011 & 0.965 $\pm$ 0.007 & \textbf{0.969 $\pm$ 0.010} \\
40x & 0.840 $\pm$ 0.047 & 0.875 $\pm$ 0.060 & 0.929 $\pm$ 0.018 & 0.964 $\pm$ 0.017 & 0.965 $\pm$ 0.011 & \textbf{0.965 $\pm$ 0.015} & 0.912 $\pm$ 0.036 & 0.908 $\pm$ 0.013 & 0.907 $\pm$ 0.027 & 0.955 $\pm$ 0.011 & 0.962 $\pm$ 0.009 & 0.959 $\pm$ 0.020 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{Validation weighted F1 score, with mean and standard deviation, for BreakHis.}
\label{tab:validation_metrics_breakhis}
\end{table*}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x & \textbf{0.928 $\pm$ 0.033} & 0.925 $\pm$ 0.046 & 0.899 $\pm$ 0.051 & 0.879 $\pm$ 0.063 & 0.874 $\pm$ 0.077 & 0.896 $\pm$ 0.068 & 0.915 $\pm$ 0.023 & 0.899 $\pm$ 0.046 & 0.864 $\pm$ 0.073  & 0.854 $\pm$ 0.061 & 0.823 $\pm$ 0.093 & 0.832 $\pm$ 0.078 \\
10x & 0.922 $\pm$ 0.037 & \textbf{0.928 $\pm$ 0.038} & 0.903 $\pm$ 0.031 & 0.897 $\pm$ 0.026 & 0.896 $\pm$ 0.017 & 0.887 $\pm$ 0.028 & 0.884 $\pm$ 0.031 & 0.865 $\pm$ 0.050 & 0.851 $\pm$ 0.055 & 0.818 $\pm$ 0.056 & 0.813 $\pm$ 0.058 & 0.783 $\pm$ 0.083 \\
20x & \textbf{0.906 $\pm$ 0.037} & 0.887 $\pm$ 0.013 & 0.856 $\pm$ 0.038 & 0.844 $\pm$ 0.040 & 0.842 $\pm$ 0.037 & 0.817 $\pm$ 0.047 & 0.893 $\pm$ 0.023 & 0.827 $\pm$ 0.023 & 0.798 $\pm$ 0.022 & 0.746 $\pm$ 0.027 & 0.732 $\pm$ 0.050 & 0.739 $\pm$ 0.041 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BACH.}
\label{tab:knnvalidation_metrics_bach}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x  & 0.729 $\pm$ 0.013 & 0.735 $\pm$ 0.013 & 0.741 $\pm$ 0.006 & \textbf{0.746 $\pm$ 0.011} & 0.741 $\pm$ 0.010 & 0.739 $\pm$ 0.005 & 0.718 $\pm$ 0.011 & 0.700 $\pm$ 0.016 & 0.708 $\pm$ 0.010 & 0.706 $\pm$ 0.008 & 0.707 $\pm$ 0.005 & 0.706 $\pm$ 0.011 \\
10x & 0.741 $\pm$ 0.007 & 0.759 $\pm$ 0.014 & \textbf{0.760 $\pm$ 0.011} & 0.750 $\pm$ 0.015 & 0.749 $\pm$ 0.013 & 0.742 $\pm$ 0.005 & 0.731 $\pm$ 0.016 & 0.724 $\pm$ 0.021 & 0.724 $\pm$ 0.015 & 0.719 $\pm$ 0.020 & 0.716 $\pm$ 0.016 & 0.708 $\pm$ 0.019 \\

20x & 0.766 $\pm$ 0.015 & \textbf{0.770 $\pm$ 0.022} & 0.769 $\pm$ 0.016 & 0.765 $\pm$ 0.021 & 0.762 $\pm$ 0.026 & 0.756 $\pm$ 0.025 & 0.758 $\pm$ 0.022 & 0.743 $\pm$ 0.020 & 0.742 $\pm$ 0.015 & 0.729 $\pm$ 0.018 & 0.727 $\pm$ 0.020 & 0.715 $\pm$ 0.021 \\

40x & 0.756 $\pm$ 0.017 & \textbf{0.762 $\pm$ 0.016} & 0.757 $\pm$ 0.012 & 0.754 $\pm$ 0.008 & 0.748 $\pm$ 0.013 & 0.741 $\pm$ 0.016 & 0.751 $\pm$ 0.025 & 0.738 $\pm$ 0.023 & 0.725 $\pm$ 0.016 & 0.726 $\pm$ 0.010 & 0.708 $\pm$ 0.019 & 0.703 $\pm$ 0.023 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BRACS.}
\label{tab:knnvalidation_metrics_bracs}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Distance}  & \multicolumn{6}{c}{\textbf{Cosine Similarity}} & \multicolumn{6}{c}{\textbf{Euclidean Distance}} \\
 \cmidrule(lr){2-7} \cmidrule(lr){8-13}
\textbf{k}
                   & 1 & 3 & 5 & 7 & 9 & 11 & 1 & 3 & 5 & 7 & 9 & 11 \\
\midrule
5x  & 0.862 $\pm$ 0.013 & \textbf{0.866 $\pm$ 0.016} & 0.849 $\pm$ 0.020 & 0.842 $\pm$ 0.019 & 0.826 $\pm$ 0.005 & 0.814 $\pm$ 0.009 & 0.861 $\pm$ 0.015 & 0.845 $\pm$ 0.015 & 0.825 $\pm$ 0.022 & 0.824 $\pm$ 0.012 & 0.806 $\pm$ 0.012 & 0.783 $\pm$ 0.009 \\
10x & \textbf{0.927 $\pm$ 0.016} & 0.925 $\pm$ 0.025 & 0.915 $\pm$ 0.026 & 0.906 $\pm$ 0.026 & 0.900 $\pm$ 0.025 & 0.891 $\pm$ 0.028 & 0.925 $\pm$ 0.017 & 0.916 $\pm$ 0.018 & 0.902 $\pm$ 0.025 & 0.883 $\pm$ 0.020 & 0.878 $\pm$ 0.024 & 0.869 $\pm$ 0.035 \\
20x & 0.953 $\pm$ 0.014 & 0.947 $\pm$ 0.014 & 0.942 $\pm$ 0.015 & 0.939 $\pm$ 0.012 & 0.931 $\pm$ 0.008 & 0.916 $\pm$ 0.008 & 0.950 $\pm$ 0.014 & \textbf{0.956 $\pm$ 0.010} & 0.944 $\pm$ 0.011 & 0.938 $\pm$ 0.004 & 0.924 $\pm$ 0.008 & 0.906 $\pm$ 0.013 \\
40x & 0.948 $\pm$ 0.009 & 0.947 $\pm$ 0.014 & 0.945 $\pm$ 0.016 & 0.937 $\pm$ 0.018 & 0.931 $\pm$ 0.022 & 0.925 $\pm$ 0.021 & 0.948 $\pm$ 0.013 & \textbf{0.952 $\pm$ 0.011} & 0.947 $\pm$ 0.015 & 0.936 $\pm$ 0.013 & 0.929 $\pm$ 0.014 & 0.906 $\pm$ 0.024 \\
\bottomrule
\end{tabular}%
}
\centering
\caption{k-NN probing metrics for BreakHis.}
\label{tab:knnvalidation_metrics_breakhis}
\end{table*}


\end{document}

BACHELOR THESIS PAPER FOR IEE ICC WORKSHOP 2025



\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{comment}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{bm}
\usepackage[left=1.62cm,right=0.64in,top=0.75in]{geometry}

%\usepackage{geometry}
%\setlength{\textheight}{23cm}  % Adjust text height for consistent bottom margins

%\usepackage{geometry}
\usepackage{array}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{balance}

%\geometry{a4paper, margin=1in}

%\geometry{a4paper, margin=1in}
\bibliographystyle{IEEEtran}


%\usepackage[skip=2pt]{caption} % Ajusta el espaciado de los subtítulos
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization }


\author{
\IEEEauthorblockN{Mario Rico Ibáñez$^1$; Azim Akhtarshenas$^1$; David L\'opez-P\'erez$^1$; Giovanni Geraci$^2$}
\IEEEauthorblockA{\textit{$^1$Universitat Politècnica de València, Valencia, Spain}} 
\IEEEauthorblockA{\textit{$^2$Telefónica Research and Universitat Pompeu Fabra, Barcelona, Spain}} 
%\IEEEauthorblockA{\textit{$3$}} 
\textit{mriciba@upv.edu.es}}

\maketitle


\begin{abstract}

\footnotetext{This research is supported by the Generalitat Valenciana
through the CIDEGENT PlaGenT, Grant CIDEXG/2022/17, Project iTENTE, and the action CNS2023-144333, financed by
MCIN/AEI/10.13039/501100011033 and the European Union
“NextGenerationEU”/PRTR.}
Unmanned aerial vehicle (UAV)-based base stations offer a promising solution in emergencies where the rapid deployment of cutting-edge networks is crucial for maximizing life-saving potential. 
Optimizing the strategic positioning of these UAVs is essential for enhancing communication efficiency. 
This paper introduces an automated reinforcement learning approach that enables UAVs to dynamically interact with their environment and determine optimal configurations. 
By leveraging the radio signal sensing capabilities of communication networks, 
our method provides a more realistic perspective, 
utilizing state-of-the-art algorithm ---proximal policy optimization--- to learn and generalize positioning strategies across diverse user equipment (UE) movement patterns. 
We evaluate our approach across various UE mobility scenarios, including static, random, linear, circular, and mixed hotspot movements.
The numerical results demonstrate the algorithm's adaptability and effectiveness in maintaining comprehensive coverage across all movement patterns.
 
\textbf{UAV, Aerial Base Station, DRL, PPO, Sensing}
\end{abstract}

%\vspace{-9pt}

\section{Introduction} 

Effective communication during disasters is crucial for successful search and rescue operations. 
However, this can be particularly challenging when existing networks are compromised, 
leaving user equipment (UE) in urgent need of assistance. 
One innovative approach to address this issue is the use of unmanned aerial vehicles (UAVs),
which offers a practical solution for quickly establishing emergency communication networks~\cite{uav_emergency}. 
UAVs, acting as flying base stations (BSs), can play a critical role in scenarios where most ground BSs are overloaded, collapsed, or entirely absent. 
Such situations were evident during events like 9/11~\cite{11s_comm}, Typhoon Haiyan~\cite{aquino2013typhoon}, and in many urban, suburban, and rural search and rescue operations. 
In these critical circumstances, search and 
the rapid deployment of broadband and/or ultra-reliable low-latency communication (URLLC) networks is essential to support first responders as quickly as possible, 
maximizing the chances of saving lives.

\subsection{Related Works}

Extensive recent work on UAV-based emergency communication networks has primarily focused on UAV positioning optimization, 
which we discuss along with their drawbacks below.

%Traditional methods
The author in~\cite{cicek2019uav} surveyed UAV-BS location optimization,
introduced a generic mixed-integer non-linear programming (MINLP) framework,
proposed a taxonomy of solutions,
and outlined future research directions for 5G and beyond.
In \cite{8038869}, 
the authors developed a framework to optimize 3D UAV placement, mobility, device-to-UAV association, and uplink power control for IoT devices, 
aiming to minimize energy consumption while ensuring network connectivity.
%
Unfortunately, all these works lack real-time decision-making capabilities due to their static nature and their inability to model the complexity of real-world environments,
which are inherently non-convex, non-linear, and stochastic.

%AI and ML
As a promising alternative, 
the integration of artificial intelligence (AI)---particularly reinforcement learning (RL)---into UAV networks enables autonomous, real-time optimization of 3D positioning, flight paths, and resource management.
For instance, the authors in \cite{8877247} addressed emergency scenarios by proposing a Q-learning-based solution to optimize the 3D positioning and transmit power of a network with multiple UAV-based small cells,
with the objective of maximizing network coverage while accounting for UE mobility.
Similarly, the study in \cite{ghanavi2018efficient} improved terrestrial networks using an aerial BS,
aiming to enhance quality of service (QoS) by mitigating performance degradation due to UE mobility.
This work employed Q-learning to determine the optimal aerial BS placement based on past experiences.
%
However, Q-learning suffers from the curse of dimensionality, 
making it impractical for large-scale problems wherein the state-action space is vast—as is the case in our scenario, 
making deep RL (DRL) a more suitable alternative where the Q-table is approximated by a deep neural network (DNN).

%DRL papers
Assuming simple UE mobility scenarios, 
the authors in \cite{8432464} developed a DRL-based approach for UAV flight control,
aiming to maximize energy efficiency while balancing network coverage and UE data rates.
%
%In \cite{liu2020machine}, 
%the authors integrated reconfigurable intelligent surfaces (RIS) with UAV networks to further reduce energy consumption and improve spectrum efficiency using non-orthogonal multiple access (NOMA). 
%They optimize UAV movement, RIS phase shifts, power allocation, and NOMA decoding order with a decaying deep Q-network (D-DQN) algorithm.
%However, while these methods offer promising solutions, they do not fully address the unique challenges posed by emergency scenarios, such as the need for real-time adaptation to rapidly changing environments, the inability to rely on precise UE location data, and the requirement to operate effectively across diverse and unpredictable mobility patterns of first responders.
%
Ding~\textit{et al.} in \cite{ding20203d} modeled UAV energy consumption as a function of 3D movement,
and designed a DRL algorithm using deep deterministic policy gradient (DDPG) 
to optimize flight direction and speed for throughput maximization while satisfying energy constraints.
%Further exploring UAV positioning, the authors in~\cite{gopi2021reinforcement} aim to optimize the positions of UAV-macro BSs using reinforcement learning. Their approach prevents collisions between multiple exploratory UAVs and restricts their movements to a predefined area. By applying Q-learning, UAV BS positions are optimized based on UE data rates, with the framework transitioning from exploratory to exploitative movements to maximize overall data rates.
%In \cite{gopi2021reinforcement}, the authors used RL to optimize UAV-macro BS positions, preventing collisions and confining movements to a specific area. They applied Q-learning to enhance UAV BS positions based on UE data rates, transitioning from exploration to exploitation to maximize overall data rates.
%
%remove
%{\color{red}In \cite{liu2019reinforcement},a novel UAV optimization method based on quality of experience (QoE) was proposed, involving three steps: UE cell partitioning with a genetic algorithm-based K-means (GAK-means), UAV 3D positioning optimization with a Q-learning algorithm, and tracking roaming UEs with a Q-learning-based movement strategy.}
%
Focusing on convergence efficiency, 
the authors in \cite{parvaresh2023continuous} proposed a continuous actor-critic DRL (ACDRL) approach,
which reduced UAV-BS placement convergence time by 85\,\% compared to traditional DRL-based methods.
%
These studies, nonetheless, rely on discrete state-action spaces, often leading to suboptimal performance.
More critically, the above-mentioned DRL methods suffer from training instability due to high policy update variance and hyperparameter sensitivity,
limiting their effectiveness in dynamic environments.

%TRPO
To address this issue, 
trust region policy optimization (TRPO) was introduced,
enforcing a strict KL-divergence constraint to ensure stable policy updates~\cite{schulman2015trust}.
For instance, the authors in \cite{ho2021uav} applied TRPO to address instabilities in DDPG,
improving both training stability and learning efficiency.
Their approach optimized UAV control for energy efficiency and data rate maximization in dynamic wireless environments,
enhancing UAV-based wireless service provisioning.
%
However, TRPO’s KL-divergence hard constraint makes it computationally expensive.

Proximal policy optimization (PPO)  overcomes TRPO’s limitations by replacing its strict KL-divergence constraint with a clipped objective function, 
improving both efficiency and computational simplicity~\cite{schulman2017proximal}.
Saxena~\textit{et al.} in \cite{saxena2019optimal} introduced flow-level models (FLM) to evaluate UAV-BS network performance across multiple metrics,
and proposed a PPO-based approach to optimize traffic-aware UAV trajectories.
Their offline training aimed to maximize cumulative performance metrics,
while online simulations demonstrated a three-fold increase in average UE throughput and more balanced BS traffic loads compared to initial UAV placements.
Unfortunately, their method assumes perfect knowledge of UE positions,
which limits its applicability in real-world environments where UE mobility and uncertainty must be considered.
\cite{li2020trajectory} was among the first works to propose a PPO-based approach for UAV trajectory design, 
optimizing the sum rate for all UEs while considering their movement.
Their PPO-based solution outperformed Q-learning when UAVs followed specific paths
where UE velocity and distribution remained relatively stable.
To further adapt to dynamic UE distributions, 
they introduced a random-training algorithm (RT-PPO).
However, their approach restricts UAV locations to a set of predefined points,
which simplifies the problem but limits flexibility.

%Extensive work has been done in recent years on aspects related to ---but not always entirely aligned with--- UAV-based emergency communication networks. While many studies focus on optimizing UAV placement and movement, they often do so in contexts that are not specific to emergency scenarios involving first responders.
%For instance, in~\cite{8038869}, the authors proposed a framework to jointly optimize 3D UAV placement, mobility, device-to-UAV association, and uplink power control for serving static Internet of Things (IoT) devices, aiming to minimize total transmit power. The framework determines optimal UAV locations and device-to-UAV associations based on IoT device activities, and then derives optimal UAV trajectories to minimize energy consumption while maintaining connectivity. 
%Further expanding on UAV optimization, 
%the work in~\cite{wang2024joint} introduced the height of the UAV as an optimization variable, 
%which had been widely overlooked in the literature. 
%They formulated their optimization problem as a constrained markov decision process (CMDP) and used a Lagrangian-based PPO-CMDP algorithm to derive the optimal UAV service height among other key variables. 
%
\subsection{Motivation and Contributions}

Although previous studies on UAV positioning have provided valuable insights,
many rely on unrealistic assumptions, including static UEs, known UE positions, predefined UAV paths, and discrete UAV movements.
These assumptions fail to capture the stochastic nature of UE mobility, the uncertainty of UE locations, and the flexibility of UAVs to navigate—especially in emergency scenarios.
As a result, existing approaches lack adaptability to dynamic environments, limiting their real-world applicability.

To address these challenges, we leverage PPO~\cite{schulman2017proximal}
to develop a more realistic and adaptive framework for UAV trajectory planning.
Specifically, we eliminate state-action space discretization and replace GPS-based positioning
with measurements over practical reference signals, 
allowing UAVs to position themselves based on real-time UE signal characteristics.

Our key contributions are:
\begin{itemize}
\item Enabling flexible UAV movement to accurately model continuous trajectory adjustments in real-world deployments.
\item Utilizing UE reference signals and \ac{AOA} measurements, instead of GPS data, improving robustness in scenarios with unknown UE locations.
\item Evaluating performance across diverse UE mobility patterns to validate PPO’s effectiveness in dynamic emergency scenarios.
\end{itemize}

\section{System Model}
\label{sec:system_model}

\subsection{System Description}

In this paper, 
we consider an emergency cellular network comprised of multiple UAVs serving as mobile BSs,
which attempt to serve the UEs of a team of first responders. 
This cellular network operates in both downlink and uplink using time division multiple access (TDMA). 

Let $\mathcal{U} = \{u_{\mathit{1}}, \ldots,u_{\mathit{u}}, \ldots, u_{U}\}$, represent the set of first responders, 
each one with his/her own UE, 
and its cardinality (i.e., the number of elements in the set $\mathcal{U}$) be denoted by $U$.  
The geographical position of the first $u^{th}$ responder is determined by
$\rho^\mathrm{U}_u = \{x^\mathrm{U}_u, y^\mathrm{U}_u, z^\mathrm{U}_u\}$,
and the position matrix of all first responders is represented as 
$\bm{\rho}^\mathrm{U} = \{\rho^\mathrm{U}_\mathit{1}, \ldots, \rho^\mathrm{U}_\mathit{u}, \ldots,  \rho^\mathrm{U}_U\}$. 

\begin{comment}   
{\color{red}Let $\mathcal{D}$ represent the set of UAVs,
each one with its own BS, 
with its cardinality denoted by $D$,
i.e.
\begin{equation}
    \mathcal{D} = \{d_{\mathit{1}}, \ldots,  d_{\mathit{d}}, \ldots,  d_{D}\},
\end{equation}
where the location information of UAV $d$ is determined by:
\begin{equation}
    \rho^\mathrm{D}_d = \{x^\mathrm{D}_d, y^\mathrm{D}_d, z^\mathrm{D}_d\},
\end{equation}
and the position matrix of all UAVs is represented as:
\begin{equation}
    \bm{\rho}^\mathrm{D}= \{\rho^\mathrm{D}_\mathit{1}, \ldots, \rho^\mathrm{D}_\mathit{d}, \ldots, \rho^\mathrm{D}_D\}.
\end{equation}
The rest of equations are also revised!!!!}\\
\end{comment}
Let $\mathcal{D} = \{d_1 , . . . , d_d , . . . , d_D\}$, denote the set of drones or UAVs, 
each one with its own BS, 
with its cardinality denoted by $D$. So, the location information of the $d^{th}$ UAV is represented by 
$\rho^\mathrm{D}_d = \{x^\mathrm{D}_d, y^\mathrm{D}_d, z^\mathrm{D}_d\}$,
and the position matrix of all UAVs is represented as 
$\bm{\rho}^\mathrm{D} = \{\rho^\mathrm{D}_1,...,\rho^\mathrm{D}_d,...,\rho^\mathrm{D}_D\}$.

\subsection{Channel Model}

We assume the network operates with a bandwidth $B$ at a frequency $f$. 
All radio links in the network experience slow and fast channel gains. 
We denote by $G_{u,d,k}$ the overall channel gain between the $u^{th}$ UE and the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource. 
This gain can be further decomposed into antenna gain ($G^{\rm a}$), path gain ($G^{\rm p}$), outdoor-to-indoor gain ($G^{\rm e}$), shadow gain ($G^{\rm s}$), and fast-fading gain ($G^{\rm{ff}}$),
i.e.
\begin{equation}
    G_{u,d,k} = G^{\rm a}_{u,d} \cdot G^{\rm p}_{u,d} \cdot G^{\rm e}_{u,d} \cdot G^{\rm s}_{u,d} \cdot \left|G^{\rm ff}_{u,d,k}\right|^2.
\end{equation}
In this study, 
we use the Urban Macro models specified by the third generation partnership project (3GPP) in TR25.814 to drive the calculation of each one of the presented channel components, 
with the following amendments: 
BS antennas are considered to be omnidirectional and the multi-path fading follows a Rician model. 
It is worth noting that the overall channel gain $G_{u,d,k}$ depends on both the position of the UAV $\rho^\mathrm{D}_d$ (our optimization variable) and the position of the first responder $\rho^\mathrm{U}_u$,
where this last one is unknown to us. 
Thus, the channel gain $G_{u,d,k}$  can be represented as 
$G_{u,d,k}(\rho^\mathrm{U}_u,\rho^\mathrm{D}_d)$.

The power received by the $u^{th}$ first responder from the BS of the $d^{th}$ UAV  within the $k^{th}$ frequency resource can be expressed as~\cite{meyers1946nonlinearity}:  
\begin{equation}
    P^{\rm{rx}}_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d) = P^{\rm{tx}}_{d,k} \cdot G_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d),
    \label{eq_intro:Received_power}
\end{equation}
where $P^{\rm{tx}}_d$ is the power transmitted by the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource.

The parameter that indicates the quality of the signal received by the $u^{th}$ first responder from the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource is the signal-to-interference-plus-noise ratio (SINR) $\gamma_{u,k}$, 
and can be calculated as~\cite{mozaffari2015drone, zhang2018downlink}:
\begin{eqnarray}
 	\gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) = \frac{P^{\mathrm rx}_{u,d,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d)}{\sum_{\substack{ d\prime=0 \\ d\prime \neq d }}^{D} P^{\mathrm rx}_{u,d\prime,k}(\rho^{\mathrm{U}}_u, \rho^{\mathrm{D}}_d)+ \sigma^2_k},
	\label{eq_intro:sinr}
\end{eqnarray}
where $\sigma^2_k$ is the noise power in the $k^{th}$ frequency resource.

By applying the Shannon-Hartley theorem,
the data transmission rate of the $u^{th}$ first responder connected to the BS of the $d^{th}$ UAV in the $k^{th}$ frequency resource can be computed as~\cite{Djordjevic2022}:
\begin{eqnarray}
 	R_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) = B_k \log_2(1+\gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D})),
    \label{eq_intro:rate}
\end{eqnarray}
where $B_k$ is the bandwidth of the $k^{th}$ frequency resource.
If a scheduler is used to fairly distribute the available resources among the first responders in the cell, 
such as round-robin~\cite{rasmussen2008round},  
the transmission rate can derived as:
\begin{equation}
    R_u(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D}) =  \frac{B}{U} \log_2(1+\bar \gamma_{u,d,k}(\rho^{\mathrm{U}}_u, \bm{\rho}^\mathrm{D})),
    \label{eq_intro:mean_rate}
\end{equation}
where $\frac{B}{U}$ indicates the portion of the bandwidth that is allocated to each UE, on average, in the communication.  
Note that the average SINR $\bar \gamma_{u,d,k}$ of the UE in its allocated frequency resources is used as its effective SINR.

It should be noted that,
unlike many other approaches, 
our proposed scheme does not rely on GPS data from UEs for optimization, 
due to the uncertainty of its availability and reliability in emergency situations. 
Instead, 
we assume the use of reference signals, 
and angle of arrival (AoA) estimations over them, 
as proxies. 
In more details, 
we assume that the BS of the $d^{th}$ UAV  can estimate 
---using an antenna array and signal processing--- 
the AoA $\alpha_{u,d}$ of the reference signals received from the $u^{th}$ UE.
To comprehensively evaluate our approach, 
we adopt a two-phase analysis. 
First, we assume perfect AoA estimation by the BS of the $d^{th}$ UAV to establish a performance benchmark, 
allowing us to understand the full potential of our algorithm in ideal conditions. 
Then, we introduce Gaussian noise to simulate real-world imperfections in AoA estimation, 
assessing the algorithm’s robustness and effectiveness under more practical conditions.
\section{Problem Statement}
The objective of this work is to determine, in real-time, the UAV positions $\bm{\rho}^\mathrm{D}$ that maximize the total fair transmission rate \( R_{\text{fair}} \) of first responders.
This rate is defined as the sum of the logarithms of the transmission rates of all UEs~\cite{9878252},
that is:
\begin{equation}
    R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})  = \sum_{u \in \mathcal{U}}  \log_{10} (R_{u}
     (\rho^\mathrm{U}_u, \bm{\rho}^\mathrm{D})).
     \label{eq_intro:fair_rate}
\end{equation}
This ensures a balanced approach where increases in rates for UEs with lower rates are given more importance compared to those with higher rates, 
thus promoting fairness.
%Let us emphasize that it depends on the positions of the UAVs \(\bm{\rho}^\mathrm{D}\) and the positions of the UEs, \(\bm{\rho}^\mathrm{U}\), 
%as indicated earlier, 
%The implementation of a sum of the logarithms of individual transmission rates, \( R_{\text{fair}} \), is a strategy that makes the calculation fairer{\color{blue}This is because the logarithm is a concave function, which implies that additional increases in the transmission rate of a UE with an already high rate contribute less} to \( R_{\text{fair}} \) compared to increases in the transmission rate of a UE with a low rate.

With this in mind,
our optimization problem can then be formally formulated as:
\begin{equation}
    \text{max}_{\bm{\rho}^\mathrm{D}} R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D}).
\end{equation}

The problem's high dimensionality, stochasticity, and non-linearity makes this real-time UAV trajectory optimization highly challenging.
The continuous movement of first responders and fluctuations in the radio channel demand adaptive decision-making,
which traditional optimization techniques struggle to handle effectively.
To address these challenges, 
and given the advantages of PPO highlighted in the introduction,
we adopt a PPO algorithm to dynamically adjust UAV positions.
PPO enables our UAVs to continuously optimize $R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})$,
adapting to UE mobility patterns and radio condition variations in real-time.
The following section provides a detailed RL formulation and our PPO implementation.

\begin{comment}
The complexity of this problem, 
characterized by high dimensionality, stochasticity, and non-linearity, 
poses significant challenges for traditional optimization methods. 
The continuous movement of first responders and fluctuations in the radio channel demand constant adaptation, 
making these methods struggle with the system's dynamics and complexity. 
Given these challenges, 
machine learning (ML) emerges as a promising approach, 
having demonstrated remarkable performance in various complex tasks. 
However, while ML excels in classification and regression problems,
the sequential decision-making nature of our scenario calls for a more specialized solution.
This leads us to RL, 
a subfield of ML based on Markov decision processes, 
which offers a powerful framework for solving such problems \cite{sutton2018reinforcement}.
RL is particularly well-suited for our scenario as it excels at real-time adaptation through direct interaction with the environment. 
This enables RL to find optimal and robust policies that dynamically maximize the total fair transmission rate $R_{\text{fair}}(\bm{\rho}^\mathrm{U}, \bm{\rho}^\mathrm{D})$, 
while continuously adjusting to changes in first responders' positions and radio conditions.
\end{comment}

\section{DRL-based PPO Algorithm}

In RL, 
an agent interacts with an environment,
learning to make decisions by receiving feedback in the form of rewards.  
This interaction is formalized through four key components:  
\begin{itemize}
    \item \textbf{States (S):} The possible situations the agent can observe.  
    \item \textbf{Actions (A):} The decisions the agent can take.  
    \item \textbf{Rewards (R):} The feedback the agent receives after taking an action.  
    \item \textbf{Policy ($\pi$):} The strategy the agent follows to decide which action to take in each state.  
\end{itemize}

Like its predecessor TRPO, 
PPO is an on-policy, model-free algorithm,
and belongs to the actor-critic family~\cite{schulman2015trust}.  
It extends the REINFORCE algorithm~\cite{williams1992simple} by incorporating a value function estimator,  
which stabilizes training and improves sample efficiency.  

Specifically,
PPO employs an advantage function to determine how much better a particular action is compared to the average action in a given state.  
This advantage function is defined as:  
\begin{equation}
     \hat{A}_t = r_t - V(s_t), 
\end{equation}  
where \( s_t \) and \( r_t \) represent the state and reward at time step \( t \),  
and \( V(s) \) is the value function.  
This advantage-based approach, 
combined with a value function, 
gives PPO its actor-critic nature,  
allowing simultaneous optimization of both the policy \( \pi \) and the value function \( V(s) \).  

Compared to traditional policy gradient methods~\cite{sutton2018reinforcement},
PPO is designed to address three key challenges:  
\begin{itemize}
    \item PPO improves training stability by using a clipped surrogate objective, 
    which limits the magnitude of policy updates and prevents large, destabilizing changes that can occur in traditional policy gradient methods.
    \item Despite being an on-policy method, 
    PPO is relatively sample-efficient, achieving good performance with fewer environment interactions compared to other RL methods.
    \item PPO also operates in both continuous  states-actions spaces, eliminating the need for discretization and providing a realistic and accurate representation.
\end{itemize}

\subsection{PPO Key Details}

PPO uses a clipped objective to ensure efficiency and stability,
while being easier to implement compared to other methods. 
The algorithm optimizes the policy by implementing a ``surrogate" objective function~\cite{schulman2017proximal}. 
The surrogate objective is given by:
\begin{equation}
    \begin{aligned}
    L^{\text{CLIP}}(\theta) &= \\
    & \hat{\mathbb{E}}_t[\min\big(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\big)],
    \end{aligned}
    \label{eq:clipperd_loss}
\end{equation}
where \(\theta\) represents the weight distribution of DNNs used in the algorithm, 
\( \hat{A}_t \) is the advantage function estimator at time-step \( t \), 
\( \epsilon \) is a hyperparameter controlling the extent of the policy update, 
and \(r_t(\theta)\) measures how much the policy has changed between updates.
Specifically, \(r_t(\theta)\) is the ratio of the probability of taking action \(a_t\) given state \(s_t\) under the current policy \(\pi_\theta\) to the probability of taking that same action under the old policy \(\pi_{\theta_{\text{old}}}\)~\cite{schulman2017proximal}:
\begin{equation}
    r_t(\theta) =\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}.
\end{equation}

PPO differs from TRPO by using the clipping mechanism instead of KL divergence.
The clipping ensures that the policy's updates do not deviate too far,
effectively saturating the objective value when the policy's update becomes too large. 
This discourages excessive changes to the policy by taking the minimum of the clipped and unclipped objectives.
Additionally, PPO incorporates generalized advantage estimation (GAE)~\cite{schulman2015high} to refine the advantage function. 
GAE applies a discount factor \( \lambda \) to balance the consideration of immediate and future rewards, 
which results in more efficient policy learning. 
This enables PPO to navigate complex environments, 
such as those encountered in UAV flights for first responders.

%In summary, PPO represents a cutting-edge approach in reinforcement learning, combining stability, efficiency, and simplicity. Its robust design makes it highly effective for addressing challenging sequential decision-making problems in real-world applications.

\subsection{Proposed PPO Implementation}

Without loss of generality,
considering a single UAV through subsequent experiment, 
we aim to analyze how the PPO optimizes the $d^{th}$ UAV position to serve the UEs of the first responders efficiently.
To this end, 
we define the state-space environment of our problem as follows:
\begin{equation}
    \tilde{\rho} = [\rho_{d,t}^{D}, \rho_{d,t-1}^{D}, \dots, \rho_{d,t-M}^{D}],
\end{equation}
\begin{equation}
    \tilde{\gamma}_u = [\tilde{\gamma}_{u,t}, \tilde{\gamma}_{u,t-1}, \dots, \tilde{\gamma}_{u,t-M}] ,
\end{equation}
\begin{equation}
    \mu_{\alpha} = [\mu_{\alpha,t}, \mu_{\alpha,t-1}, \dots, \mu_{\alpha,t-M}],
\end{equation}
\begin{equation}
    \sigma_{\alpha} = [\sigma_{\alpha,t}, \sigma_{\alpha,t-1}, \dots, \sigma_{\alpha,t-M}],
\end{equation}
\begin{equation}
    \text{S} = [\tilde{\rho}, \tilde{\gamma}_u, \mu_{\alpha}, \sigma_{\alpha}],
\end{equation}
where $\tilde{\rho}$ is the time-dependent vector of the positions of $d^{th}$ UAV,
%David: Notation is confusing. It seems Pos only relates to one UAV but in the paper so far it seems that we may have more than one UAV. We always talked about UAV d in the system model. We should work with the genreal case of D UAVs or  indicate at some point that we consider only 1 UAV. Done
$\tilde{\gamma}_u$ is the time-dependent vector of SINR measurements of the $u^{th}$ UE connected to the BS of the $d^{th}$ UAV, 
and \(\mu_{\alpha}\) and \(\sigma_{\alpha}\) denote the mean and standard deviation of the AoAs \(\alpha_{u,d}\ \forall u\) of the reference signals transmitted by the $u^{th}$ UEs of the first responders and received by the BS of the $d^{th}$ UAV. 
%David: That we are using radio signals and not position is one of our differentiators. We should give more importance to this. Present the capability of the BS to estiamte the angle of arrival from each UE in the system model having a given nomenclature and the you can present here that we get the average and the standard deviation. I repeat we must emphasize this part in the system model.  Working on it

Putting all these variables together, 
S is the state vector formed by Pos, $\tilde{\gamma}_u$, $\mu_{\alpha}$, and $\sigma_{\alpha}$. 
Note that the parameter $M$ represents the memory length, 
i.e. we store the last $M$ values in each vector. 

%The objective of utilizing these states is for the drone to infer the position of the UEs without directly being given their positions through the BS.
As we have a continuous action space, 
the agent has two variables to choose from: 
direction and movement magnitude. 
The direction is defined by the angle $\alpha_d$, 
and the magnitude is defined by the distance $r$. 
Hence, the action space is defined as $\mathcal{A} = \{(\alpha_d, r)\}$, 
where $\alpha_d \in [-180, 180)$ and $r \in [0, r_{max}]$. 
Here, $r_{max}$ is the maximum distance the UAV can travel in one action, 
and the angle $\alpha_d$ is defined with respect to the east. 
It is important to note that our implementation leverages the PPO's ability to work with continuous action spaces. 
By avoiding discretization, 
the agent can explore the full continuum of possible positions, 
enabling it to find optimal locations that maximize system throughput with high precision. 
This is particularly beneficial in our scenario, 
where small adjustments in UAV position can significantly impact network performance.

Through our proposed framework, 
the reward function is designed to maximize the total fair transmission rate $R_{\text{fair}}$, 
as introduced earlier. 
To ensure consistency and comparability of the reward values during training, 
a min-max normalization is applied. 
% \begin{equation}
% \text{Reward} = 2 \times (R_{fair} - 4) / (6.104 - 4) - 1
% \end{equation}
%David: We should have  $R_{\text{fair}}$ in the equation.
%David: I dont like this. Do not use numbers but variables, and we will say later in the results section that after optimizing this variables their values are ...  
%David: Explain with the 2 and the -1. 
This normalization centers the reward values and scales them, 
improving the stability and efficiency of the RL algorithm.

\section{Numerical Results and Discussion}

This section evaluates our UAV-based BS flights algorithm across various dynamic emergency scenarios, focusing on convergence, learned flights strategies, and network performance, particularly the total fair transmission rate $R_{fair}$.

\subsection{Experimental Setup}

\subsubsection*{Scenario}

To evaluate our algorithm's effectiveness, 
we consider a 200m × 200m area,
and that the BS of $d^{th}$ UAV operates a bandwidth $B=$10\,MHz at frequency $f=$ 2\,GHz band.
We use the system model presented in Section \ref{sec:system_model},
embracing the 3GPP UMa model in TR25.814.

\subsubsection*{UE Mobility Models}
We implement various UE mobility patterns within the designated area, increasing in complexity, where UEs reflect off boundaries upon contact. The UE mobility patterns are as follows:
\begin{itemize}
    \item \textit{Static UEs (No Move)}: A cluster of 10 UEs in a 10m × 10m square at the center, representing stationary scenarios.
    
    \item \textit{Linear Motion (Straight Walk)}: The same cluster moving in a straight line at 8 m/s, simulating unified group movement 
    ---the direction of the movement is randomly selected.
    
    \item \textit{Circular Motion}: The same cluster initialized at $x \in [0, 10]$, $y \in [-190, -200]$, 
    moving in a circular path (radius 200m, center at origin), 
    representing a more complex search pattern.
    
    \item \textit{Crossed Linear Motion}: Two clusters of 5 UEs each, 
    moving perpendicularly (90\textdegree\ gap) or in opposite directions (180\textdegree\ gap), simulating multiple independent groups.

    \item \textit{Random Hotspot and Mixed Movement}: Two groups of 5 UEs each: one cluster moving uniformly and another set of independently moving UEs. 
    Velocities are randomly assigned within 0 to 8 m/s, 
    simulating both coordinated and unpredictable movements in dynamic scenarios.
\end{itemize}
\subsubsection*{PPO Algorithm Configuration}

Our proposed algorithm based on PPO is trained using the following key parameters and configuration. 
The PPO algorithm is trained for 12,000 episodes, 
with each episode consisting of 128 frames. 
We employed a learning rate of 3e-4, 
and implemented three hidden layers in both the actor and critic networks, 
each containing 128 neurons. 
The algorithm used a discount factor $\gamma$ of 0.99 and a GAE parameter $\lambda$ of 0.95, 
balancing immediate and future rewards effectively. 
To ensure stable learning, 
we applied gradient clipping with a maximum norm of 1.0, 
and used a clip $\epsilon$ of 0.2 for the surrogate objective. 
Our model incorporated a memory size $M$ of 4, 
enabling the agent to have an idea of what the UE movement is without having the UE position directly.

\subsubsection*{Results and Discussion}

Figure~\ref{fig:throughput_analysis} illustrates the average throughput per episode during the evaluation phase for the various movement scenarios.
For reference, 
given the configuration,
the maximum achievable UE rate is around 8\,Mbps.
\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{Figs/NUMERICAL RESULT/throughput_analysis.pdf}
\caption{Average throughput per evaluation episode for different movement types}
\label{fig:throughput_analysis}
\end{figure}
The results reveal distinct trends for each movement type, highlighting the algorithm's adaptability and effectiveness across diverse scenarios. In scenarios with a single cluster of first responders (``No Move'', ``Straight Random Walk'', and ``Circular Movement''), the agent consistently achieves throughputs exceeding 7\,Mbps. This performance is maintained even with randomly varying speeds, demonstrating the agent's robust ability to optimize its position based solely on reference signal information. It is worth noting that the complex scenarios resulted in performance degradation and failed to achieve the maximum rate.
For scenarios involving two clusters of first responders (``Straight 90'' and ``Straight 180''), the agent effectively learns to optimize its position, progressively improving throughput over the episodes. This improvement indicates the algorithm's capacity to handle more complex spatial distributions of UEs, balancing performance between multiple groups. The ``Straight 180" scenario is the most challenging here, as the UE clusters get farther apart compared to the ``Straight 90" scenario, resulting in lower performance.
The ``Hotspot Random'' scenario, representing the most complex movement pattern, shows a gradual improvement in throughput, albeit with more variability. This scenario challenges the agent with both clustered and dispersed UEs moving at random velocities, yet the algorithm still manages to enhance performance over time.
These results collectively underscore the algorithm's adaptability across diverse movement patterns and its capacity to enhance network performance using only information derived from reference signals without relying on UE location information.

To further evaluate the effectiveness of our proposed approach, 
we compare the performance of our PPO-based agent against another baseline scenario where the UAV is statically positioned at the center of the environment. 
Table~\ref{tab:comparison} presents this comparison for each movement scenario.
\begin{table}[htbp]
\centering
\caption{Comparison of Average Throughput: PPO vs. Static}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Scenario} & \textbf{PPO} & \textbf{Static} & \textbf{Diff.}&\textbf{Gain} \\
& \textbf{(Mbps)} & \textbf{(Mbps)} & \textbf{(Mbps)}& \textbf{(\%)} \\
\midrule
No Move & 8.00 & 7.95 &+0.05 &2\% \\
Straight Random & 7.87 & 4.59 & +3.28&71\% \\
Circular & 7.08 & 3.59 & +3.49&97\% \\
Straight 90\textdegree & 6.27 & 5.33 & +0.94&17\% \\
Straight 180\textdegree & 5.91 & 5.31 & +0.60&11\% \\
Hotspot Random & 5.19 & 4.77 & +0.42&8\%\\
\bottomrule
\end{tabular}
\end{table}
The results for the ``Straight 90°" and ``Straight 180°" scenarios are particularly noteworthy, with the PPO algorithm outperforming the static solution by 17.64\% and 11.30\%, respectively.
This improvement may be surprising, as one might expect a centrally located static UAV to be an optimal solution for two groups moving in perpendicular or opposite directions. However, the fact that our PPO-based approach, which takes radio propagation conditions and network performance into account through the reward function, identifies a strategy that surpasses this geometrically optimal static position suggests that the algorithm is effectively leveraging its mobility to optimize coverage in a dynamic and non-obvious way.

Furthermore, we analyze the convergence when Gaussian noise is added to the AoA estimation. 
Table~\ref{tab:throughput_std_straight_random} presents the results of this simulation for the ``Straight Random" movement scenario.
\begin{table}[htbp]
    \centering
    \caption{Throughput values for the ``Straight Random" scenario with different noise levels}
    \label{tab:throughput_std_straight_random}
    \small
    \begin{tabular}{@{}lcccccccc@{}}
        \toprule
        \textbf{STD} & 0 & 1 & 5 & 100 & 50 & 100 \\
        \midrule
        \textbf{PPO (Mbps)} & 7.87 & 7.84 & 7.82 & 7.80 & 7.70 & 7.16 \\
        \bottomrule
    \end{tabular}
\end{table}
The results demonstrate the algorithm's robustness to noise in AoA estimation. Even with a significant noise level (STD = 100), the throughput remains high at 7.16 Mbps, only a 0.71 Mbps decrease from the noise-free scenario. This resilience to noise underscores the practical viability of our approach in real-world deployments, where perfect AoA estimations may be unlikely. The algorithm's ability to maintain high throughput even with noisy AoA measurements highlights its potential for reliable performance in challenging emergency communication scenarios.

%% \subsection{Sum of Rewards Analysis}
% Figure~\ref{fig:reward_analysis} shows the sum of rewards per step for different movement setting during training.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{Figs/NUMERICAL RESULT/reward_analysis.pdf}
% \caption{Sum of Rewards per Step for Different Movement Types}
% \label{fig:reward_analysis}
% \end{figure}
% Key observations corresponded to each scenario are as follows:
% \begin{itemize}
% \item All scenarios trend towards higher rewards over time.
% \item ‘‘No Move’’ scenario shows the most stable and highest rewards.
% \item The algorithm adapts well to complex scenarios like ‘‘Straight Walk’’ and ‘‘Circular Movement’’.
% \item ‘‘Random Hotspot’’ exhibits the highest volatility and lowest rewards.
% \item ‘‘Crossed Straight Walk’’ shows rapid improvement and stabilization.
% \end{itemize}
% %\subsection{Average Distance Analysis}
% Figure~\ref{fig:distance_analysis} presents the average distance per step for different movement setting.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{Figs/NUMERICAL RESULT/distance_analysis.pdf}
% \caption{Average Distance per Step for Different Movement Types}
% \label{fig:distance_analysis}
% \end{figure}
% Key insights concluded from each scenario are as follows:
% \begin{itemize}
% \item General trend towards lower average distances per step.
% \item ‘‘No Move’’ scenario quickly converges to minimal movement.
% \item ‘‘Straight Walk’’ and Circular Movement’’ show gradual decrease in distances.
% \item ‘‘Crossed Straight Walk’’ maintains a relatively constant average distance.
% \item ‘‘Random Hotspot’’ exhibits the highest and most volatile distances.
% \item Decreasing trends indicate the UAV learns to make more precise movements.
% \end{itemize}
% %\subsection{Summary of Results}

% These results demonstrate the effectiveness of our PPO-based approach in learning optimal UAV-based BS positioning strategies across a diverse range of UE mobility scenarios. The algorithm shows particular strength in adapting to predictable movement patterns while maintaining the ability to handle more chaotic scenarios. This adaptability is crucial for real-world applications where UE mobility patterns may vary or change over time.

\balance

\section{Conclusion}

%This paper demonstrates the effectiveness of UAV-MBS for emergency communications, emphasizing the crucial role of optimal UAV positioning in enhancing emergency network performance. To ensure a realistic model,we introduced a continuous action space DRL approach, where the agent selects both the direction and movement magnitude dynamically. Our method, leveraging PPO, enables UAVs to adapt to various UE movement patterns and network topologies effectively. A key contribution of this work is the use of widely available RSs, rather than relying solely on UAV and UE positions,  to drive the state-action space. This approach enhances the model's realism, especially in scenarios where GPS or accurate UE location data may be unavailable. The results confirm the algorithm’s adaptability and effectiveness in maintaining comprehensive coverage across different scenarios, representing a significant advancement in real-time UAV positioning for improved emergency response.
This paper studies the UAV-based BS problem in emergency communications by emphasizing the importance of optimal UAV flights to enhance network performance. It introduces a continuous action space DRL approach using PPO, allowing UAVs to dynamically select both direction and movement magnitude. The key innovation is the use of widely available reference signals to drive the state-action space, improving model realism in scenarios where GPS or accurate UE location data is unavailable. The results confirm the algorithm's adaptability and effectiveness in maintaining comprehensive coverage across various scenarios, marking a significant advancement in real-time UAV flights for improved emergency response.
As a future work, 
we will consider deploying multiple UAVs as aerial BSs to further enhance network coverage and reliability,
and explore advanced coordination strategies. 
%As the future work, we plan to explore advanced coordination strategies and dynamic resource allocation among the UAVs. Additionally, we aim to integrate more complex UE mobility models and environmental factors to improve the robustness and scalability of our approach.}

\bibliography{bibliography}



\end{document}

BACHELOR THESIS 06.2024 FOR TELECOM ENGINEERING 

UNIVERSITAT POLITÈCNICA DE VALÈNCIA
Escuela Técnica Superior de Ingeniería de
Telecomunicación
Posicionamiento de estaciones base en tiempo real basado
en aprendizaje de refuerzo profundo para futuras redes 6G.
Trabajo Fin de Grado
Grado en Ingeniería de Tecnologías y Servicios de
Telecomunicación
AUTOR/A: Rico Ibáñez, Mario
Tutor/a: Naranjo Ornedo, Valeriana
Cotutor/a: López Pérez, David
CURSO ACADÉMICO: 2023/2024Resumen
Las comunicaciones móviles son esenciales en la sociedad actual, definiendo cómo nos comunicamos y relacionamos, e influyendo en cómo vivimos. La optimización de este bien tan valioso
es esencial para proporcionar la mejor experiencia posible de forma sostenible. Para mejorar las
prestaciones de la red y su eficiencia energética, se espera que las futuras generaciones de comunicaciones incorporen estaciones base capaces de reposicionarse de forma sencilla, sin intervención
humana, mediante vehículos terrestres o aeronaves no tripuladas, como drones.
En esta tesis, proponemos una solución inteligente que permite a dichas estaciones base móviles
adaptarse y aprender los patrones de movimiento de los usuarios, y encontrar su posición óptima
en tiempo real. Para cuantificar la experiencia del usuario, utilizamos la tasa de datos media de
los usuarios. Para guiar dichas decisiones y aprender políticas óptimas, hemos utilizado técnicas
avanzadas de aprendizaje, en más detalle, aprendizaje reforzado profundo (DRL, del inglés deep
reinforcement learning).
Como primer punto de trabajo, es importante enfatizar el uso, desarrollo y optimización de un
entorno de simulación avanzado de redes de comunicaciones móviles 4G/5G/6G, llamado Giulia,
basado en Python. Dicha optimización implicó la transformación de un número sustancial de operaciones a métodos más eficientes, como el uso de GPU a través de la biblioteca PyTorch. Como
mayor contribución de este trabajo, implementamos técnicas avanzadas de DRL, adaptándolas a
nuestro problema específico, que combinan la potencia de las redes neuronales profundas (DNN,
del inglés deep neural networks) con la optimización del aprendizaje reforzado. A través del trabajo
de esta tesis hemos sentado unas bases sólidas y conocimiento acerca de la integración de DRL con
entornos de simulación complejos para la toma de decisiones en tiempo real. Hemos demostrado
que el uso de estadísticas acerca de la potencia de señal recibida, el ángulo de llegada y la tasa de
datos de los usuarios es información suficiente para aprender patrones de movimiento de grupos
de usuarios, y optimizar en tiempo real el movimiento de estaciones base móviles que pretenden
maximizar la experiencia de dichos usuarios. Los resultados de la tesis indican que DRL puede ser
utilizado en las siguientes generaciones de redes de comunicaciones que pueden ser controladas
por agentes autónomos en tiempo real.Resum
Les comunicacions mòbils són essencials en la societat actual, definint com ens comuniquem i
ens relacionem, i influint en com vivim. L’optimització d’aquest bé tan valuós és essencial per a
proporcionar la millor experiència possible de manera sostenible. Per a millorar les prestacions
de la xarxa i la seua eficiència energètica, s’espera que les futures generacions de comunicacions
incorporen estacions base capaces de reposicionar-se de forma senzilla, sense intervenció humana,
mitjançant vehicles terrestres o aeronaus no tripulades, com drons.
En aquesta tesi, proposem una solució intel∙ligent que permet a aquestes estacions base mòbils
adaptar-se i aprendre els patrons de moviment dels usuaris, i trobar la seua posició òptima en temps
real. Per a quantificar l’experiència de l’usuari, utilitzem la taxa de dades mitjana dels usuaris.
Per a guiar aquestes decisions i aprendre polítiques òptimes, hem utilitzat tècniques avançades
d’aprenentatge, en més detall, aprenentatge reforçat profund (DRL, de l’anglés deep reinforcement
learning).
Com a primer punt de treball, és important enfatitzar l’ús, desenvolupament i optimització d’un
entorn de simulació avançat de xarxes de comunicacions mòbils 4G/5G/6G, anomenat Giulia, basat
en Python. Aquesta optimització va implicar la transformació d’un nombre substancial d’operacions
a mètodes més eficients, com l’ús de GPU a través de la biblioteca PyTorch. Com a major contribució d’aquest treball, implementem tècniques avançades de DRL, adaptant-les al nostre problema
específic, que combinen la potència de les xarxes neuronals profundes (DNN, de l’anglés deep
neural networks) amb l’optimització de l’aprenentatge reforçat. A través del treball d’aquesta tesi
hem assentat unes bases sòlides i coneixement sobre la integració de DRL amb entorns de simulació complexos per a la presa de decisions en temps real. Hem demostrat que l’ús d’estadístiques
sobre la potència de senyal rebuda, l’angle d’arribada i la taxa de dades dels usuaris és informació suficient per a aprendre patrons de moviment de grups d’usuaris, i optimitzar en temps real el
moviment d’estacions base mòbils que pretenen maximitzar l’experiència d’aquests usuaris. Els
resultats de la tesi indiquen que DRL pot ser utilitzat en les següents generacions de xarxes de
comunicacions que poden ser controlades per agents autònoms en temps real.
3Abstract
Mobile communications are essential in today’s society, defining how we communicate and interact, and influencing how we live. The optimization of this valuable asset is crucial to provide
the best possible experience in a sustainable way. To improve network performance and energy
efficiency, it is expected that future generations of communications will incorporate base stations
capable of repositioning easily, without human intervention, using ground vehicles or unmanned
aerial vehicles (UAVs), such as drones.
In this thesis, we propose an intelligent solution that allows these mobile base stations to adapt
and learn user movement patterns, and find their optimal position in real-time. To quantify the
user experience, we use the average data rate of users. To guide these decisions and learn optimal
policies, we have used advanced learning techniques, specifically, deep reinforcement learning
(DRL).
As the first point of work, it is important to emphasize the use, development, and optimization of an
advanced simulation environment for 4G/5G/6G mobile communication networks, called Giulia,
based on Python. This optimization involved transforming a substantial number of operations into
more efficient methods, such as using GPU through the PyTorch library. As the main contribution
of this work, we implemented advanced DRL techniques, adapting them to our specific problem,
which combine the power of deep neural networks (DNN) with the optimization of reinforcement
learning. Through the work of this thesis, we have laid a solid foundation and knowledge about
the integration of DRL with complex simulation environments for real-time decision-making. We
have demonstrated that the use of statistics on received signal strength, angle of arrival, and user
data rate is sufficient information to learn movement patterns of user groups and optimize in realtime the movement of mobile base stations that aim to maximize the experience of these users.
The results of the thesis indicate that DRL can be used in the next generations of communication
networks that can be controlled by autonomous agents in real-time.
4RESUMEN EJECUTIVO
La memoria del TFG del GTIST debe desarrollar en el texto los siguientes conceptos, debidamente justificados y discutidos, centrados en el ámbito
de la IT
CONCEPT (ABET) CONCEPTO (traducción) ¿Cumple?
(S/N)
¿Dónde?
(páginas)
1. IDENTIFY: 1. IDENTIFICAR:
1.1. Problem statement and opportunity 1.1. Planteamiento del problema y oportunidad S 1.
1.2. Constraints (standards, codes, needs,
requirements & specifications)
1.2. Toma en consideración de los condicionantes
(normas técnicas y regulación, necesidades,
requisitos y especificaciones)
S
2-5,
9-36,
37-45.
1.3. Setting of goals 1.3. Establecimiento de objetivos S 6-7.
2. FORMULATE: 2. FORMULAR:
2.1. Creative solution generation (analysis) 2.1. Generación de soluciones creativas (análisis) S 45-53,
57-66.
2.2. Evaluation of multiple solutions and
decision-making (synthesis)
2.2. Evaluación de múltiples soluciones y toma de
decisiones (síntesis) S
45-53,
57-66,
70-74.
3. SOLVE: 3. RESOLVER:
3.1. Fulfilment of goals 3.1. Evaluación del cumplimiento de objetivos S 53,
69-74.
3.2. Overall impact and significance
(contributions and practical recommendations)
3.2. Evaluación del impacto global y alcance
(contribuciones y recomendaciones prácticas) S 77-78.
Escuela Técnica Superior de Ingeniería de Telecomunicación
Universitat Politècnica de València
Edificio 4D. Camino de Vera, s/n, 46022 Valencia
Tel. +34 96 387 71 90, ext. 77190
www.etsit.upv.esA mis padres y mi abuela,
por mantener siempre en mi esas ganas de mejorar, apoyo incondicional y un cariño único.
A Alejandro, Sergio y Martina,
los mejores hermanos que he podido tener, gracias por hacerme feliz.
A mis amigos,
por ser un pilar fundamental y una gran fuente de diversión.
A mis tutores, Valery y David,
por confiar en mi y apoyarme en todo lo que pueden.
Todo esto es gracias a vosotros.Índice general
I Memoria
1. Introducción 1
1.1. Motivación y Descripción del Trabajo . . . . . . . . . . . . . . . . . . . . . . . 1
1.2. Introducción Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1. Estado del Arte de La Inteligencia Artificial Aplicada a Drones . . . . . 3
1.3. Establecimiento del Problema . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3.1. Modelo del Sistema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3.2. Objetivo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4. Modelado del Entorno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5. Estructura del Trabajo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2. Aprendizaje Reforzado Profundo 9
2.1. Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1. Proceso de Decisión de Markov . . . . . . . . . . . . . . . . . . . . . . 9
2.1.2. Periodicidad y Estados Terminales . . . . . . . . . . . . . . . . . . . . . 10
2.1.3. Procesos de Decisión de Markov Parcialmente Observables . . . . . . . 12
2.1.4. Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.5. Probabilidad de Trayectoria . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.6. Funciones de Valor y Q . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.6.1. Funciones Q y de Valor con Factor de Descuento . . . . . . . . 14
2.1.6.2. Funciones Óptimas . . . . . . . . . . . . . . . . . . . . . . . 15
2.1.7. Ecuaciones de Bellman . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2. Métodos de Aprendizaje Reforzado . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.1. Clasificación según El Objetivo . . . . . . . . . . . . . . . . . . . . . . 16
2.2.2. Clasificación según El Tipo de Política . . . . . . . . . . . . . . . . . . 16
2.2.3. Clasificación según El Uso de Modelo . . . . . . . . . . . . . . . . . . . 16
2.3. Aprendizaje por Valor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.1. Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.1.1. Exploración versus Explotación . . . . . . . . . . . . . . . . . 19
2.3.2. SARSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4. Aprendizaje por Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4.1. Iteración de Política . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4.1.1. Evaluación de la Política . . . . . . . . . . . . . . . . . . . . 21
2.4.1.2. Mejora de la Política . . . . . . . . . . . . . . . . . . . . . . . 21
2.4.2. Política de Gradientes . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4.2.1. El Teorema de La Política de Gradiente . . . . . . . . . . . . . 222.4.2.2. Algoritmo REINFORCE . . . . . . . . . . . . . . . . . . . . 24
2.4.3. El Problema de La Expansión Exponencial del Espacio de Estados . . . . 25
2.5. Redes Neuronales Artificiales . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.1. Perceptrón . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.2. Funciones de Activación . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.3. Perceptrón Multicapa . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.5.4. Proceso de Entrenamiento . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.5.4.1. Feedforward . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.5.4.2. Cálculo de Pérdida . . . . . . . . . . . . . . . . . . . . . . . . 29
2.5.4.3. Cálculo de Gradiente y Backpropagation . . . . . . . . . . . . 30
2.6. Estado del Arte Aprendizaje Reforzado Profundo . . . . . . . . . . . . . . . . . 34
3. Giulia 37
3.1. Naturaleza del Simulador . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.2. Cálculo de La Tasa de Transmisión . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.1. Ganancia de Antena . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.2. Modelado de La Pérdida por Distancia . . . . . . . . . . . . . . . . . . . 39
3.2.3. Modelado de Shadowing . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.4. Ganancia de Penetración . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.2.5. Modelado del Desvanecimiento Multicamino . . . . . . . . . . . . . . . 42
3.2.6. Modelado de Potencia de Señal Recibida . . . . . . . . . . . . . . . . . 43
3.2.7. Modelado de La Calidad de Señal . . . . . . . . . . . . . . . . . . . . . 44
3.2.8. Calculo de La Tasa de Transmisión . . . . . . . . . . . . . . . . . . . . 45
3.3. Creación de Un Entorno . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.4. Optimizaciones sobre Giulia . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.4.1. Plotting Ineficiente . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.2. Funciones Vectorizadas . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.3. Implementación GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.4.4. Comparación de Eficiencia del Simulador . . . . . . . . . . . . . . . . . 53
4. Metodología 55
4.1. Diseño del Problema de RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.1. Espacio de Estados . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.2. Espacio de Acciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.1.3. Recompensa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2. Algoritmos de RL para Resolver El Problema . . . . . . . . . . . . . . . . . . . 57
4.2.1. Deep Q Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.1. Naturaleza . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.2. Función de Pérdida . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.3. Red Objetivo . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2.1.4. Gradiente . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.5. Deadly Triad . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.6. Replay Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1.7. Exploración versus Explotación . . . . . . . . . . . . . . . . . 58
4.2.2. Trust Region Policy Optimization y Proximal Policy Optimization . . . . 59
4.2.2.1. Naturaleza . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.2.2.2. Beneficios . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60ÍNDICE GENERAL
4.2.2.3. Trust Region Policy Optimization . . . . . . . . . . . . . . . . 60
4.2.2.4. Proximal Policy Optimiaztion . . . . . . . . . . . . . . . . . . 61
4.2.2.5. Generalized Advantage Estimation . . . . . . . . . . . . . . . 61
4.3. Implementación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.3.1. Detalles de Implementación . . . . . . . . . . . . . . . . . . . . . . . . 64
4.3.1.1. Selección de Acciones . . . . . . . . . . . . . . . . . . . . . . 64
4.3.1.2. Entropía para La Selección de Acciones . . . . . . . . . . . . 64
4.4. Casos de Uso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1. Movimiento UEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1.1. UEs Estáticos . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4.1.2. UEs Dinámicos . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2. Inicialización Agentes . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 65
4.5. Parámetros e Hiperparámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.1. Parámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2. Hiperparámetros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2.1. DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.5.2.2. PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5. Resultados 69
5.1. User Equipments Estáticos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2. UEs en Movimiento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2.1. Movimiento Lineal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.2.1.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 71
5.2.1.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 71
5.2.2. Movimiento Circular . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2.2.1. Inicialización Estática . . . . . . . . . . . . . . . . . . . . . . 72
5.2.2.2. Inicialización Aleatoria . . . . . . . . . . . . . . . . . . . . . 73
5.3. Experimentos de Ablación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.3.1. Número de Capas y Neuronas . . . . . . . . . . . . . . . . . . . . . . . 75
5.3.2. Tasa de Aprendizaje y Número de episodios . . . . . . . . . . . . . . . . 75
5.3.3. Entropía de Epsilon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6. Conclusiones 77
Bibliografía 79
9Índice de figuras
1.1. Ejemplo situación con UEs en movimiento y red terrestre sin servicio. . . . . . . 2
1.2. Esquema básico entorno RL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.1. Estado inicial. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2. Episodio óptimo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3. Episodio subóptimo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.4. Proceso de decisión de Markov (MDP) con observación completa. . . . . . . . . 12
2.5. Proceso de decisión de Markov parcialmente observable (POMDP). . . . . . . . 12
2.6. Ejemplo cálculo valor V. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.7. Clasificación de diferentes modelos. . . . . . . . . . . . . . . . . . . . . . . . . 17
2.8. Algoritmo de aprendizaje Q y tabla Q. . . . . . . . . . . . . . . . . . . . . . . . 18
2.9. Esquema de un perceptrón. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.10. Esquema de un perceptrón multicapa. . . . . . . . . . . . . . . . . . . . . . . . 28
2.11. Grafo función g. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.12. Grafo computación con gradientes. . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1. Inclinación de una estación base [38]. . . . . . . . . . . . . . . . . . . . . . . . 39
3.2. Diagrama de radiación de la antena [38]. . . . . . . . . . . . . . . . . . . . . . . 40
3.3. Comparación entre el modelo ideal y el modelo 3GPP TR 36.814 de pérdida por
distancia. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4. Mapa desvanecimiento multicamino [38]. . . . . . . . . . . . . . . . . . . . . . 43
3.5. SINR ciudad de Dublín [38]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.6. Código ejemplo bucles for en list comprehension. . . . . . . . . . . . . . . . . . 48
3.7. Mejora por indexamiento directo de vectores. . . . . . . . . . . . . . . . . . . . 48
3.8. Código transformación de posición LCS a GCS en bucle for. . . . . . . . . . . . 50
3.9. Código transformación de posición LCS a GCS sin bucle for. . . . . . . . . . . . 51
3.10. Código fast fading de Numpy. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.11. Código asignación automática dispositivo. . . . . . . . . . . . . . . . . . . . . . 51
3.12. Código fast fading de torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.13. Código mW a dB de torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.14. Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR. . . . . 52
3.15. Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR usando
torch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.1. Ejemplo cálculo ángulos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2. Visualización tabla Q DQN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3. Ejemplo PPO. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.4. Sistema de entorno y agente. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634.5. Diagrama de flujo del entrenamiento PPO con torchrl. . . . . . . . . . . . . . . . 63
5.1. Comparación de trayectorias PPO y DQN. . . . . . . . . . . . . . . . . . . . . . 69
5.2. Comparación de recompensas entre DQN (fila de arriba) y PPO (fila de abajo). . 70
5.3. Visualización de episodios con posición de inicialización (0, -100). . . . . . . . . 72
5.4. Comparación de métricas en diferentes inicializaciones durante entrenamiento aleatorio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.5. Comparación de métricas en diferentes inicializaciones . . . . . . . . . . . . . . 73
5.6. Visualización de episodios con movimiento circular con inicialización en (0,0). . 74
5.7. Comparación de métricas en diferentes inicializaciones, incluyendo inicialización
en (0,0) no random. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75Índice de tablas
3.1. Parámetros típicos de una antena sectorial. . . . . . . . . . . . . . . . . . . . . . 39
3.2. PDPs para UEs peatones (Pedestrian, ≤3 km/h). . . . . . . . . . . . . . . . . . . 43
3.3. Resultados después de plotting ineficiente. . . . . . . . . . . . . . . . . . . . . . 47
3.4. Resultados tras vectorización y uso de GPU. . . . . . . . . . . . . . . . . . . . . 53
3.5. Porcentaje de tiempo ahorrado tras optimización. . . . . . . . . . . . . . . . . . 54
4.1. Parámetros del simulador. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.2. Hiperparámetros del algoritmo DQN. . . . . . . . . . . . . . . . . . . . . . . . 67
4.3. Hiperparámetros del algoritmo PPO. . . . . . . . . . . . . . . . . . . . . . . . . 67Listado de siglas empleadas
3GPP 3rd Generation Partnership Project..
6G Sixth-generation Technology for Wireless Communication.
A3C Asynchronous Advantage Actor Critic..
BS Base Station..
CNN Convolutional Neural Network..
DDPG Deep Deterministic Policy Gradient..
DQN Deep-Q-Network..
DRL Deep Reinforcement Learning..
GAE Generalized Advantage Estimation..
GAN Generative Adversarial Network..
ITU International Telecommunication Union..
MAC Media Access Control..
MDP Markov Decision Process..
ML Machine Learning..
MPC Multipath Component..
PDP Power Delay Profile..
POMDP Partially Observable Markov Decision Process..
PPO Proximal Policy Optimization..
ReLU Rectified Linear Unit..
RL Reinforcement Learning..RNN Recurrent Neural Network..
SARSA State-Action-Reward-State-Action..
SINR Signal-to-Interference-plus-Noise Ratio..
TDMA Time Division Multiple Access ..
TRPO Trust Region Policy Optimization..
UAV Unmanned Aerial Vehicle..
UES User Equipment..Parte I
MemoriaCapítulo 1
Introducción
1.1. Motivación y Descripción del Trabajo
Las situaciones de emergencia y accidentes, tales como los desastres naturales, plantean desafíos
significativos para las infraestructuras de comunicaciones. Durante estos eventos, las redes de comunicación se vuelven vitales para una variedad de tareas críticas, incluyendo las operaciones de
rescate, la coordinación de recursos y la comunicación entre equipos de emergencia y afectados.
Sin embargo, las redes de comunicación actuales presentan limitaciones importantes. A menudo,
no pueden desplegarse fácilmente en cualquier localización o cualquier momento, lo que dificulta
su uso efectivo en situaciones de emergencia.
En este contexto, las tecnologías emergentes de la sexta generación (6G) de comunicaciones móviles ofrecen una solución prometedora. Una de las innovaciones más destacadas es el uso de drones,
los cuales permiten el despliegue de redes de comunicación casi en cualquier lugar y en cualquier
momento, superando así las limitaciones de las infraestructuras fijas tradicionales. Estos drones
pueden portar estaciones base móviles, proporcionando cobertura de red en áreas afectadas por
desastres donde la infraestructura tradicional ha sido destruida o es inaccesible.
Sin embargo, la mera disponibilidad de drones no es suficiente para resolver todos los problemas
asociados con la comunicación en situaciones de emergencia. Es esencial que estos drones sean
capaces de adaptarse de manera dinámica y eficiente a las necesidades cambiantes de los equipamiento de usuario (UE, del inglés user equipment) y las condiciones del entorno. Estos drones
pueden ser realmente necesarios en operativos móviles, donde el seguimiento de estos equipos de
operación es crucial para un correcto funcionamiento de la red. Para lograr esto, se requiere una
inteligencia avanzada que permita a los drones tomar decisiones en tiempo real, como se observa
en la Figura 1.1.
En este trabajo, proponemos el desarrollo de un agente de aprendizaje reforzado (RL, del inglés
reinforcement learning) capaz de adaptarse dinámicamente a los movimientos de los UEs y a las
condiciones del entorno en tiempo real. Este agente empleará técnicas avanzadas de aprendizaje
prrofundo por refuerzo (DRL, del inglés deep reinforcement learning) para aprender el movimiento
de los UEs y optimizar los patrones de movimiento de los drones. Con esta capacidad de adaptación,
los drones podrán no solo posicionarse de manera óptima, sino también ajustarse continuamente a
los cambios en el entorno y las necesidades de los UEs, maximizando así la eficiencia de la red y
mejorando significativamente la experiencia del UE.
def mW_to_dBm_torch(value_mW):
result = 10 * torch.log10(value_mW)
return result
Figura 3.13: Código mW a dB de torch.
if isinstance(beam_activity_per_ue, np.ndarray):
mask = np.logical_and(beams_in_same_carrier_than_server,
PRB_ue_beam_interference_activity)↪→
sum_rss_mW = np.sum(tools.dBm_to_mW(np.where(mask, rsrp_prb_ue_to_cell_dBm,
np.NINF)),axis=2)↪→
else:
sum_rss_mW = np.sum(tools.dBm_to_mW(np.where(beams_in_same_carrier_than_server,
rsrp_prb_ue_to_cell_dBm, np.NINF)),axis=2)↪→
Figura 3.14: Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR.
if isinstance(beam_activity_per_ue, np.ndarray):
mask = torch.logical_and(torch.tensor(beams_in_same_carrier_than_server,
device=device), torch.tensor(PRB_ue_beam_interference_activity, device=device))↪→
selected_values_dBm_2 = torch.where(mask, torch.tensor(rsrp_prb_ue_to_cell_dBm,
device=device, dtype=torch.float64), torch.tensor(np.NINF, device=device))↪→
rsrp_prb_ue_to_cell_mW_2 = tools.dBm_to_mW_torch(selected_values_dBm_2)
sum_rss_mW = torch.sum(rsrp_prb_ue_to_cell_mW_2, axis=2)
sum_rss_mW= sum_rss_mW.cpu().numpy()
else:
selected_values_dBm = torch.where(torch.tensor(beams_in_same_carrier_than_server,
device=device), torch.tensor(rsrp_prb_ue_to_cell_dBm, device=device), np.NINF)↪→
rsrp_prb_ue_to_cell_mW = tools.dBm_to_mW_torch(selected_values_dBm)
sum_rss_mW = torch.sum(rsrp_prb_ue_to_cell_mW, axis=2)
sum_rss_mW = sum_rss_mW.cpu().numpy()
Figura 3.15: Código cálculo Receive Signal Strength (RSS) utilizado en cálculo SINR usando
torch.
523.4. OPTIMIZACIONES SOBRE GIULIA
Para llegar a esta versión final que se observa en la Figura 3.15, se evaluó si cada línea de código
era más eficiente al realizarla en Python o en Torch. La versión final, que incluye la conversión de
Torch a Numpy al final, resultó ser la más eficiente.
3.4.4. Comparación de Eficiencia del Simulador
Una vez implementadas las optimizaciones mediante la vectorización de funciones y el uso de
GPU, se procedió a comparar la eficiencia del simulador. Para una comparación realista de lo
que suponen estos cambios de GPU y vectorización, se utilizó como referencia el tiempo total
consumido por modelo medido después de la eliminación del plotting innecesario, presentada en
la Tabla 3.3. Tras aplicar todas las modificaciones, el tiempo total consumido por modelo se observa
en la Tabla 3.4.
Modelo Tiempo Total
3GPP TR 38.901 UMa lsc 63.226
ITU-R M.2135 UMa Umi colocated multilayer 39.023
ITU-R M.2135 UMa multilayer 32.528
3GPP TR 36.777 UMi AV 54.778
3GPP TR 36.777 UMa AV 51.053
3GPP TR 38.901 UMi lsc 47.532
ITU-R M.2135 UMa 24.732
ITU-R M.2135 UMi 27.092
3GPP TR 36.814 Case-1 16.666
3GPP TR 36.814 Case-1.omni 14.396
3GPP TR 38.901 UMa lsc single bs 7.456
3GPP TR 36.814 Case-1 single bs 4.689
Tabla 3.4: Resultados tras vectorización y uso de GPU.
Para evaluar cuantitativamente las mejoras, se calculó el porcentaje de tiempo ahorrado para cada
modelo, reflejando la eficiencia ganada en comparación con el tiempo de ejecución original. La
Tabla 3.5 muestra estos porcentajes de ahorro.
Los resultados indican una notable mejora en los tiempos de ejecución para la mayoría de los
modelos. En particular, los modelos 3GPP TR 38.901 UMa lsc e ITU-R M.2135 UMa multilayer
muestran ahorros superiores al 90 % y 84 % respectivamente.
Es importante destacar que las mejoras no son tan evidentes en los modelos con tiempos de ejecución bajos. Estos modelos no aprovechan tanto la GPU debido a la menor complejidad de las
multiplicaciones matriciales o a que los bucles for no son lo suficientemente grandes como para que la implementación en C de Numpy demuestre su eficiencia. Estos resultados subrayan la
importancia de realizar un análisis detallado para identificar las áreas donde las optimizaciones
tendrán el mayor impacto.
En conclusión, las optimizaciones aplicadas han resultado en una mejora significativa en la eficiencia del simulador para la mayoría de los modelos, demostrando la efectividad de la vectorización
y el uso de GPU en la reducción del tiempo de ejecución.
53CAPÍTULO 3. GIULIA
Modelo Porcentaje de Tiempo Ahorrado
3GPP TR 38.901 lsc UMa fr1 Umi C band plus fr3 58.17 %
3GPP TR 38.901 UMa lsc 91.21 %
ITU-R M.2135 UMa Umi colocated multilayer 84.20 %
ITU-R M.2135 UMa multilayer 84.82 %
3GPP TR 36.777 UMi AV 57.53 %
3GPP TR 36.777 UMa AV 58.36 %
3GPP TR 38.901 UMi lsc 59.90 %
ITU-R M.2135 UMa 77.11 %
ITU-R M.2135 UMi 72.02 %
3GPP TR 36.814 Case-1 56.82 %
3GPP TR 36.814 Case-1.omni 36.05 %
3GPP TR 38.901 UMa lsc single bs 24.95 %
3GPP TR 36.814 Case-1 single bs 16.06 %
Tabla 3.5: Porcentaje de tiempo ahorrado tras optimización.
54Capítulo 4
Metodología
4.1. Diseño del Problema de RL
Para hacer una primera aproximación al problema, se decidió utilizar un entorno con baja complejidad, con el objetivo de poder observar si utilizar DRL es una solución viable.
Por tanto, definimos el número de UEs, U , igual a 10, y los inicializamos en un área cuadrada de
10m2. Estos representan por ejemplo un equipo de rescate moviéndose todos juntos en un escenario
de emergencia. Por otra parte, el número de drones, D, utilizados es 1.
4.1.1. Espacio de Estados
Como se explicó en la sección 2.1.3, el espacio de estados no es lo mismo que el espacio de observaciones. En nuestro problema, y al disponer y entender completamente el entorno, podríamos
definir los estados como la posición de los UEs, la distancia que tiene el agente a ellos y donde
se encuentra este. Pero esto no es lo que estamos buscando, ya que nuestro objetivo es que este
agente aprenda de la forma más parecida a lo necesario en un entorno real. Por tanto, este tipo de
información no nos conviene. El espacio final de observaciones viene definido por:
Pos = [ρD
1,t, ρD
1,t−m, ρD
1,t−M ] (4.1)
 ̄γu = [ ̄γu,t,  ̄γu,t−m,  ̄γu,t−M ] (4.2)
μθ = [μθ,t, μθ,t−m, μθ,t−M ] (4.3)
σθ = [σθ,t, σθ,t−m, σθ,t−M ] (4.4)
OBS = [Pos,  ̄γu, μθ, σθ] (4.5)
donde Pos es el vector de posiciones de los drones,  ̄γu representa el vector de la relación señal a
interferencia más ruido, μθ y σθ son la media y la desviación estándar del ángulo con el que se
recibe la señal, medidas tomadas con un punto de referencia desde el este, como se observa en
la Figura 4.1, y OBS es el vector de observaciones que está conformado por Pos,  ̄γu, μθ y σθ.
Además, disponemos de una memoria de tamaño, M , que almacena las últimas M observaciones.
El objetivo de utilizar estas observaciones es que el dron use la información que puede inferir a
través de su estación base, para tener una idea de la posición de los UEs sin directamente dársela
como estado.
55CAPÍTULO 4. METODOLOGÍAN
S
EO
Figura 4.1: Ejemplo cálculo ángulos.
4.1.2. Espacio de Acciones
Dependiendo del modelo utilizado, que se explican en la sección 4.2, el espacio de acciones será
discreto o continuo.
En el caso de un espacio de acciones discreto, el agente dispone de un conjunto finito de estas,
definido por el conjunto A = {↑, ↓, →, ←, ·}, donde cada acción corresponde a un movimiento en
una de las siguientes direcciones: norte (↑), sur (↓), este (→), oeste (←), o permanecer en la misma
posición (·). Además, la magnitud del movimiento es constante, denotada por d, es decir, el agente
se desplaza una distancia fija de d metros en una sola dirección.
En el caso de un espacio de acciones continuo el agente tiene dos variables a elegir, la dirección y
la magnitud del movimiento. La dirección se define por el ángulo θd y la magnitud por la distancia
d. Por lo tanto, el espacio de acciones es A = {(θ, d)}, donde θ ∈ [−180, 180) y d ∈ [0, dmáx].
Siendo dmáx, la distancia máxima que puede recorrer el dron en una acción. El ángulo, θd, está
definido con respecto al este.
4.1.3. Recompensa
La elección de que recompensa elegir es de vital importancia dentro del DRL, ya que es lo que
va a determinar el comportamiento del agente. En este caso, hemos utilizado como recompensa la
ecuación 1.12. Sin embargo, con esto no era suficiente, ya que al estar los UEs tan concentrados,
hacía que las posiciones cercanas a este clúster fueran ya de por si rentables para el agente, por
tanto, no le importaba estar en la mejor posición, sino que iba saltando entre diferentes posiciones
cercanas. Para solucionar esto, primero se realizó una normalización de la recompensa, y después
se ajustó la recompensa para los valores más altos utilizando la siguiente ecuación:
reward = 2
( reward − 4
8,104 − 4
)
− 1 (4.6)
Si la recompensa resultante es mayor a 1.47, entonces se asigna un valor fijo de 10 a la recompensa
para incentivar fuertemente estas posiciones óptimas. Los valores de la normalización, la recompensa, y la posterior recompensa fija se han obtenido mediante prueba y error, y se ha comprobado
que son los que mejor resultado dan.
564.2. ALGORITMOS DE RL PARA RESOLVER EL PROBLEMA
4.2. Algoritmos de RL para Resolver El Problema
En esta sección vamos a explicar los fundamentos teóricos de los algoritmos implementados a lo
largo de nuestro proyecto.
4.2.1. Deep Q Networks
Estas redes trajeron consigo una revolución al campo del aprendizaje reforzado, ya que fue el
primer trabajo de investigación que consiguió demostrar la capacidad de las redes neuronales en
entornos complejos como los videojuegos de la Atari 2600 [29].
4.2.1.1. Naturaleza
En este trabajo, la red implementada se trataba de una red neuronal convolucional, que se encargaba
de aproximar la función de valor de las tuplas estado-acción, Q(s, a), que es la función que nos
indica cuánto de buena es una acción en un estado concreto. Un concepto muy similar a lo que
sucede en el algoritmo de Q-learning 1 , introducido en la sección 2.3.1, en este caso implementado
con redes neuronales.
Esta red neuronal convolucional se puede intercambiar por cualquier otro aproximador de funciones, como una red neuronal densa, lo que permite adaptarlo a cualquier tipo de tarea.
El algoritmo en el que se basa el DQN es en el de Q-learning, pero con unas ciertas diferencias a
la hora de implementarlo.
4.2.1.2. Función de Pérdida
Lo primero es la manera de implementar la pérdida, ya que ahora no será una tabla en la que se
almacenen los valores Q(s, a), sino que será una red neuronal la que se encargue de aproximar
estos valores. De manera que la pérdida se calcula como:
Li (θi) = Es,a∼ρ(·)
[
(yi − Q (s, a; θi))2]
, (4.7)
donde yi = Es′∼ε [r + γ máxa′ Q(s′, a′; θi−1)|s, a] es el objetivo para la iteración i, y ρ(s, a) es
una distribución de probabilidad sobre las secuencias, s y las acciones, a, a la que nos referimos
como la distribución de comportamiento.
Los parámetros de la iteración anterior, θi−1, se mantienen fijos al optimizar la función de pérdida,
Li(θi), la cual no deja de ser una pérdida cuadrática como la presentada en la ecuación 2.36 entre
el valor predicho por la red y el valor que debería tener.
4.2.1.3. Red Objetivo
En la función de pérdida presentada en la ecuación 4.7, no solo se consigue optimizar la red de forma que en cada iteración consigue predecir mejor los valores de la recompensa, sino que además
se introduce el concepto de una red objetivo o copia. Esta red sirve principalmente para mantener
57CAPÍTULO 4. METODOLOGÍA
la estabilidad en el entrenamiento. De manera que θi es la red donde se van haciendo las actualizaciones en cada iteración y θi−1 es la red objetivo que se va actualizando cada más iteraciones.
4.2.1.4. Gradiente
El gradiente de dicha pérdida se quedaría como:
∇θi Li (θi) = Es,a∼ρ(·);s′∼E
[(
r + γ máx
a′ Q (s′, a′; θi−1
) − Q (s, a; θi)
)
∇θi Q (s, a; θi)
]
.
(4.8)
4.2.1.5. Deadly Triad
Teóricamente este algoritmo no está asegurado a converger, ya que cumple la deadly triad [49],
al mezclar aproximadores de funciones no lineales, con un algoritmo de optimización basado en
diferencias temporales y ser off-policy. En la práctica se ha demostrado que con ciertas técnicas,
como el uso de un replay buffer y target networks, el algoritmo converge y es capaz de aprender a
resolver tareas complejas.
4.2.1.6. Replay Buffer
Se trata de una técnica que se utiliza para mejorar la eficiencia del entrenamiento. Consiste en
almacenar las transiciones que se van produciendo en el entorno en un buffer. De este buffer se
muestrean las transiciones de manera uniforme para poder entrenar al agente. Este buffer tiene una
capacidad finita, por lo que cuando se llena, se van eliminando las transiciones más antiguas.
4.2.1.7. Exploración versus Explotación
El algoritmo DQN utiliza una estrategia, ε-greedy expresado en la ecuación 2.22, para balancear
la exploración y la explotación, donde un mayor ε hace que el agente tienda a elegir una acción
aleatoria antes que la acción que le produce más recompensa. Inicialmente, ε es alto para fomentar
la exploración, y disminuye gradualmente para favorecer la explotación de las políticas aprendidas.
El algoritmo de la DQN se puede observar en el bloque 6.
De manera intuitiva, ahora mismo la representación de los valores, Q, para cada tupla de estadoacción, pasaría de tener una visualización como en la Figura 2.8, a como la vista en la Figura 4.2.
En esta figura se quiere dar a entender que, gracias al uso de las redes neuronales, ya no es necesario
construir una tabla explícita, sino que se puede estimar de manera precisa cuál es la recompensa
esperada de cada tupla estado-acción. La capacidad que tienen las redes neuronales de extraer
características permite comprender las tuplas de manera conjunta y realizar agrupaciones de datos
complejos, como se observa en la imagen.
584.2. ALGORITMOS DE RL PARA RESOLVER EL PROBLEMA
Algorithm 6 DQN
1: Inicializar la memoria de reproducción D con capacidad N
2: Inicializar la función de valor de acción Q con pesos aleatorios
3: for episodio = 1, M do
4: Inicializar la secuencia s1 = {x1}.
5: for t = 1, T do
6: Con probabilidad ε seleccionar una acción aleatoria at
7: de lo contrario seleccionar at = máxa Q∗(st, a; θ)
8: Ejecutar la acción at en el emulador y observar la recompensa rt e imagen xt+1
9: Establecer st+1 = st, at, xt+1
10: Almacenar la transición (st, at, rt, st+1) en D
11: Muestrear un minibatch aleatorio de transiciones (sj , aj , rj , sj+1) de D
12: Establecer yj =
{
rj si sj+1 es terminal
rj + γ máxa′ Q(sj+1, a′; θ) si sj+1 no es terminal
13: Realizar un paso de descenso de gradiente en (yj − Q(sj , aj ; θ))2 según la Ecuación 4.8
14: end for
15: end for
Figura 4.2: Visualización tabla Q DQN.
4.2.2. Trust Region Policy Optimization y Proximal Policy Optimization
Estos dos algoritmos difieren mucho de los que había en DRL, ya que en lugar de aproximar
funciones de valor, y a raíz de esto, elegir la acción más probable, se aproxima directamente la
política.
4.2.2.1. Naturaleza
Estos algoritmos están basados en los Policy Gradients y la ecuación base 2.26 que estos presentan, por tanto son on-policy, y model-free. En la literatura no encontramos un consenso sobre si
estos métodos son actor-crítico o basados en política. Ambos heredan la estructura del algoritmo
REINFORCE presentado en la sección 2.4.2.2, sin embargo añaden una red neuronal de la función
59CAPÍTULO 4. METODOLOGÍA
de valor para hacer la optimización. Por tanto, los consideraremos métodos actor-crítico.
En lugar de usar directamente la función, Rt, se emplea la ventaja, ˆAt, definida en la siguiente
ecuación: ˆAt = Rt − V (st), (4.9)
donde V (st) es la función de valor del estado st. Esta ventaja indica cuánto mejor es una acción
en comparación con la media de las acciones posibles en un estado concreto. La función, V (st),
puede estimarse directamente con una red neuronal o indirectamente mediante métodos iterativos
de la función de valor. Es esto lo que le da a TRPO y PPO un punto de vista de actor-crítico: estimar
la política, π, y la función de valor, V (s).
4.2.2.2. Beneficios
Buscan mejorar varios problemas asociados con los Policy Gradients. Uno de los principales desafíos es la difícil elección de parámetros, como la tasa de aprendizaje. Además, los cambios significativos en la política pueden causar divergencias en el entrenamiento, lo que impide que el agente
aprenda adecuadamente. PPO y TRPO mitigan este problema limitando la magnitud del cambio
en la política, asegurando actualizaciones más graduales y estables.
Otro problema es la eficiencia en términos de muestras. Al ser métodos on-policy, TRPO y PPO no
pueden reutilizar las muestras de entrenamiento, lo que incurre una mayor complejidad comparado
con métodos off-policy. Necesitan generar nuevas muestras para cada actualización de la política,
haciendo el proceso de entrenamiento más costoso en términos de uso de datos.
4.2.2.3. Trust Region Policy Optimization
El TRPO, basa la función a optimizar en [35], definiendo el objetivo de la siguiente forma:
max
θ
ˆEt
[ πθ (at | st)
πθold (at | st) ˆAt
]
subject to ˆEt [KL [πθold (· | st) , πθ (· | st)]] ≤ δ.
(4.10)
La intuición detrás de esta ecuación es que, la nueva distribución de pesos, θ, no se aleje demasiado
de la distribución anterior, θold, de manera que se garantiza que las actualizaciones de la política
sean suaves y no provoquen divergencias. Con la función objetivo 4.10, se busca que la política
de los nuevos pesos maximice las acciones que tienen ventaja positiva, y que la divergencia de
Kullback-Leibler entre las dos políticas no sea mayor que un valor, δ. La divergencia de KullbackLeibler es una medida de la diferencia entre dos distribuciones de probabilidad. Además, dentro
se implementa el importance sampling, representado como la fracción entre la política actual, πθ,
y la anterior, πθold , técnica utilizada para poder estimar la ventaja de la nueva política utilizando
las muestras de una política anterior. El problema que tiene TRPO es que es necesario calcular el
operador de Hessiano, lo cual es computacionalmente costoso. Por tanto, se propone una versión
simplificada de este algoritmo, que es el PPO.
604.2. AGORITMOS DE RL PARA RESOLVER EL PROBLEMA
4.2.2.4. Proximal Policy Optimiaztion
En lugar de darle importancia a la diferencia entre distribuciones con la divergencia KullbackLeibler, partimos de la siguiente:
LCP I (θ) = ˆEt
[ πθ (at | st)
πθold (at | st) ˆAt
]
= ˆEt
[
rt(θ) ˆAt
]
, (4.11)
donde definimos rt(θ) como el ratio de las políticas, que es la fracción de probabilidad de la política
actual y la anterior. Por tanto, se nos queda la siguiente función de pérdida:
LCLIP (θ) = ˆEt
[
mín
(
rt(θ) ˆAt, clip (rt(θ), 1 − ε, 1 + ε) ˆAt
)]
, (4.12)
donde ε es un hiperparámetro que controla la magnitud del cambio en la política. Si el ratio de las
políticas es mayor que 1 + ε, se recorta a 1 + ε, y si es menor que 1 − ε, se recorta a 1 − ε. De esta
manera, se garantiza que la política no cambie demasiado en cada iteración, evitando divergencias
en el entrenamiento.
4.2.2.5. Generalized Advantage Estimation
La Estimación de la Ventaja Generalizada (GAE) [50] es una técnica utilizada en PPO para evaluar
mejor qué tan buena es una acción en un estado. En lugar de considerar solo las recompensas
inmediatas, GAE utiliza un factor de descuento, λ, para combinar recompensas futuras y actuales
de manera más sofisticada. A diferencia del descuento exponencial tradicional utilizado por el
factor, γ, GAE aplica un promedio ponderado exponencialmente. Esto suaviza las fluctuaciones, y
proporciona una estimación más precisa y estable de la ventaja, ayudando a PPO a aprender mejores
políticas de manera más eficiente. Al ajustar el factor, λ, podemos controlar cuánto queremos
confiar en las recompensas a largo plazo frente a las inmediatas, logrando un equilibrio que mejora
la calidad del aprendizaje del agente.
El algoritmo de PPO se puede observar en el bloque 7.
Algorithm 7 PPO, Estilo Actor-Crítico
1: for episodio = 1, 2, ... do
2: for actor = 1, 2, ..., N do
3: Ejecutar la política πθold en el entorno durante T pasos de tiempo
4: Calcular las estimaciones de ventaja ˆA1, . . . , ˆAT
5: end for
6: Optimizar el objetivo LCP I con respecto a θ, con K épocas y tamaño de minibatch M ≤
N T
7: θold ← θ
8: end for
Un ejemplo para una intuición de como funciona este algoritmo se observa en la Figura 4.3, donde
podemos ver un ejemplo en el que un agente debe subir una montaña, y se observa como sigue tres
principales caminos en diferentes episodios.
Se puede observar como la política, π, va mejorando a lo largo de los episodios, pero sin realizar
cambios bruscos. La flecha roja indica una primera inicialización del agente donde la solución no
61CAPÍTULO 4. METODOLOGÍA
es la óptima. La flecha amarilla es un siguiente episodio donde el agente ha aprendido a subir un
poco la montaña, y así obtener mejores recompensas. La flecha verde, y última, es a la que converge
el algoritmo, siendo así la política óptima.
De esta figura podemos extraer como el PPO, en lugar de poder aprender directamente aquella
política verde, es más conservador y va escalando poco a poco la montaña. Esto, que a priori no
parece eficiente, es necesario ya que cambios drásticos en la distribución de pesos de la política
pueden llevar al agente a no converger.
Figura 4.3: Ejemplo PPO.
4.3. Implementación
Para la implementación de los algoritmos, hemos utilizado la librería de aprendizaje por refuerzo
pytorchrl [51], una herramienta de código abierto que incluye los algoritmos de aprendizaje por refuerzo más comunes. Sin embargo, debido a su simplicidad, el algoritmo DQN se ha implementado
desde cero. En contraposición, hemos decidido implementar PPO utilizando una librería externa
debido a su mayor complejidad y necesidad de precisión. Además, esta elección facilita la futura
incorporación de otros algoritmos.
En la Figura 4.4 se muestra cómo hemos integrado Giulia en la librería torchrl. Los elementos en
verde representan las partes relacionadas con Giulia, mientras que los elementos en rojo corresponden a la librería torchrl. Esta librería permite diseñar un entorno con el cual el agente interactúa, y
hemos integrado Giulia en esta parte del código.
Para la comunicación entre todos los módulos, utilizamos tensordict, una librería desarrollada por el
mismo autor de pytorchrl. Tensordict permite la comunicación mediante diccionarios de tensores,
ofreciendo una gran versatilidad para aplicar funciones a los tensores y trabajar con ellos en la
GPU.
Además, hemos diseñado un diagrama de flujo que se aplica a ambos casos de uso. En la Figura 4.5,
se presenta el flujo de trabajo que seguimos para entrenar con el algoritmo PPO utilizando la librería
624.3. IMPLEMENTACIÓNTORCHRL
ENTORNO ALGORITMO
INICIALIZACIÓN
RESET
PASO
AGENTE
ACCIÓN
PÉRDIDA
TENSORDICT
Figura 4.4: Sistema de entorno y agente.
pytorchrl.Observaciones
Despliegue UEs
Despliegue UAV
Acciones
Agente
NO SI
CÁLCULO VENTAJA
ENTRENAMIENTO
DE RED NEURONAL
EVALUACIÓN SOBRE
ENTORNO
RESETEO DE ENTORNO
Movimiento UAV
Movimiento UEs
¿Paso final?
SI NO
¿Episodio
Final?
Guardar Modelos
Figura 4.5: Diagrama de flujo del entrenamiento PPO con torchrl.
63CAPÍTULO 4. METODOLOGÍA
4.3.1. Detalles de Implementación
En esta sección, explicaremos algunos detalles avanzados sobre la implementación del algoritmo
PPO.
4.3.1.1. Selección de Acciones
Debido a la naturaleza del algoritmo PPO, es necesario definir un método más complejo para seleccionar acciones en un espacio continuo. La red neuronal genera una distribución de probabilidad
como salida, y se selecciona un valor de esta distribución.
La distribución de probabilidad generada es una distribución normal, caracterizada por una media
y una desviación estándar. Por lo tanto, la red neuronal debe devolver la media y la desviación
estándar para cada dimensión de la acción. En este caso, dado que las acciones son el ángulo y la
velocidad, la red neuronal devolverá cuatro valores.
Durante el entrenamiento, para fomentar la exploración, se seleccionan acciones muestreadas de la
distribución normal basada en la media y la desviación estándar. En cambio, durante la inferencia,
se selecciona la acción con la mayor probabilidad, que corresponde a la media de la distribución.
4.3.1.2. Entropía para La Selección de Acciones
Para promover la exploración de estados y acciones en PPO, se introduce un nuevo hiperparámetro llamado entropy coefficient. Este coeficiente incrementa la desviación estándar al decidir las
acciones, haciendo que el agente, durante el entrenamiento, tienda a tomar acciones más diversas
y alejadas de la media de la distribución.
4.4. Casos de Uso
En esta sección presentamos los diferentes casos de uso en los que los agentes van a ser entrenados.
Se distingue entre movimiento de UEs e inicialización de los agentes.
4.4.1. Movimiento UEs
En esta subsección explicamos los diferentes tipos de movimientos que van a seguir los UEs en
nuestro entorno. El entorno en el que se pueden desplazar está delimitado en un rectángulo de 300m
x 300m.
4.4.1.1. UEs Estáticos
Como mencionamos anteriormente, inicializamos 10 UEs distribuidos uniformemente en un área
de 10m x 10m. Esta región de inicialización se extiende desde x = 0 hasta x = 10 y desde y = 0
hasta y = 10. En este escenario, los UEs permanecen en sus posiciones iniciales y no se mueven.
644.4. CASOS DE USO
4.4.1.2. UEs Dinámicos
Una vez realizado el estudio sobre los UEs estáticos, es interesante avanzar un poco más en el
objetivo del trabajo, estos patrones se acercan más a los que se pueden observar en la realidad. Se
plantean dos nuevos movimientos para los UEs:
Movimiento lineal: En este caso el clúster de UES comienza centrado en (0,0), avanza hacia
el punto (300,0). Cambia de sentido hacia la izquierda hasta llegar a (-300,0) y, por último,
vuelve a la posición (0,0).
Movimiento circular: En este caso los UEs siguen un movimiento circular con el centro en
(0,0) y un radio de 200m. En este caso, el clúster de UEs está inicializado en (0,-200),
El agente debe ser capaz de seguir el movimiento lineal y el circular.
4.4.2. Inicialización Agentes
Dentro de los escenarios mostrados previamente, tenemos que seleccionar tambien donde inicializamos el agente. Esta decisión va a ser de vital importancia a la hora de determinar si el agente
aprende o no.
4.4.2.1. Inicialización Estática
Durante el entrenamiento y la evaluación el agente se mantiene en la misma posición. Este tipo de
inicialización se utiliza como una primera aproximación al problema, para comprender si es capaz
de resolver una tarea más sencilla.
4.4.2.2. Inicialización Aleatoria
Con el objetivo de saber si estos algoritmos son capaces de generalizar. Durante el entrenamiento
se inicializa el agente en posiciones aleatorias de dentro del rectángulo sobre el que trabajamos.
Se entrena durante los episodios que sea necesario y, después, se realiza una evaluación. Esta
evaluación por norma general será fija, es decir, a pesar de estar entrenando en posiciones aleatorias
se comprueba el rendimiento del agente siempre desde la misma posición o conjunto de posiciones.
Por ejemplo, realizamos 100 episodios con esta inicialización aleatoria. Posteriormente ponemos al
modelo en modo evaluación, con el objetivo de no contaminar los datos de entrenamiento, y vemos
si sabe llegar al clúster de UEs si lo ponemos en la posición (0,100). Si este episodio es uno de los
primeros que necesita el modelo para aprender, lo común es que el agente no sepa llegar a los UEs.
Una vez se vayan avanzando en los episodios deberíamos observar como en estas evaluaciones
el agente si es capaz de llegar. De esta manera se comprueban si el agente aprende a medida que
avanzan los episodios.
65CAPÍTULO 4. METODOLOGÍA
4.5. Parámetros e Hiperparámetros
En esta sección vamos a introducir los parámetros utilizados para el simulador y los hiperparámetros con los que hemos implementado los algoritmos. Los parámetros son esenciales para definir
las condiciones del entorno de simulación, mientras que los hiperparámetros son cruciales para el
ajuste y el rendimiento de los algoritmos de aprendizaje reforzado.
4.5.1. Parámetros
Los parámetros del simulador se definen en la tabla 4.1. Estos incluyen el número de UEs y drones,
el modelo de referencia, el tipo de entorno, y la duración de la simulación.
Parámetro Valor
Número de UEs 10
Número de drones 1
Modelo 3GPP TR 36.814
Tipo de Entorno Rectangular
Longitud Temporal Simulación 128
Tabla 4.1: Parámetros del simulador.
4.5.2. Hiperparámetros
Debido a las diferencias de los algoritmos, para un correcto funcionamiento, es necesario implementar diferentes hiperparámetros en cada uno de ellos. A continuación, se detallan los hiperparámetros utilizados para los algoritmos DQN y PPO en las tablas 4.2 y 4.3, respectivamente. Los
hiperparametros óptimos han sido seleccionados a partir de [29] y [36] para DQN y PPO respectivamente.Además de una validación empírica basada en un grid-search de múltiples valore.
4.5.2.1. DQN
La tabla 4.2 muestra los hiperparámetros utilizados para el algoritmo DQN. Estos incluyen detalles
sobre la memoria, las capas ocultas, las neuronas por capa, la función de activación, la tasa de
aprendizaje, y otros parámetros específicos de la técnica ε-greedy.
4.5.2.2. PPO
La tabla 4.3 muestra los hiperparámetros utilizados para el algoritmo PPO. incluyen detalles sobre
la memoria, las capas ocultas, las neuronas por capa, la función de activación, la tasa de aprendizaje,
el tamaño de batch y minibatch, las épocas de entrenamiento, y otros parámetros específicos del
método PPO como ε de recorte y entropía.
664.5. PARÁMETROS E HIPERPARÁMETROS
Hiperparámetro Valor
Tamaño de Memoria 4
Capas Ocultas 2
Número de neuronas por capa 256
Función de Activación ReLU
Tasa de Aprendizaje 1e-4
Tamaño de Batch 128
Tasa de ε-greedy inicial 0.9
Tasa de ε-greedy final 0.05
Decaimiento por episodio de tasa de ε-greedy 8,5e−5
Factor de descuento, γ 0.99
Tabla 4.2: Hiperparámetros del algoritmo DQN.
Hiperparámetro Valor
Tamaño de Memoria 4
Capas Ocultas 3
Número de neuronas por capa 128
Función de Activación ReLU
Tasa de Aprendizaje 1e-4
Tamaño de Batch 128
Tamaño de minibatch 64
Épocas de entrenamiento 15
Factor de descuento, γ 0.99
ε de recorte 0.2
Entropía de ε 0.02
Factor GAE, λ 0.95
Tabla 4.3: Hiperparámetros del algoritmo PPO.
67CAPÍTULO 4. METODOLOGÍA
68Capítulo 5
Resultados
5.1. User Equipments Estáticos
En esta sección vamos a comparar los resultados obtenidos con DQN y PPO, en el caso en el que
los UEs no se desplazan. Esto se hace con el objetivo de hacer un cribado para decidir que modelo
nos interesa para los siguientes casos de uso.
5.1.1. Inicialización Estática
En la primera fila de imágenes de la Figura 5.1 se muestran los resultados de entrenamiento en
el entorno explicado previamente, pero con el agente siempre inicializado en (-100,0), tanto en el
entrenamiento como en la evaluación. Como hemos introducido en la sección 4.4.2, esto se hace
con el objetivo de saber si nuestro agente puede o no resolver este problema.100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 0
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 1000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 2000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 3000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 0
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 1000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 2000
Posición Inicial del Dron
Trayectoria del Dron
UEs100
7550 250 25 50 75100
Posición en x 10075 50 250 25 5075
100
Posición en y
0
10
20
30
40
50
60
Posición en z
Evaluación de posición en episodio 3000
Posición Inicial del Dron
Trayectoria del Dron
UEs
Figura 5.1: Comparación de trayectorias PPO y DQN.
A simple vista se observa que ambos modelos, DQN y PPO, convergen de una manera rápida, a los
69CAPÍTULO 5. RESULTADOS
1000 episodios ya tienen un conocimiento de donde se encuentran los UEs. El modelo PPO muestra
una mayor precisión a la hora de mantenerse estático encima de los usuarios, principalmente por
su naturaleza basada en un espacio de acciones continuo. En DQN, si el agente toma alguna acción
incorrecta, se estará desplazando 10 metros. Sin embargo, si en PPO se toma una decisión incorrecta
es más probable que la acción tomada sea menor a esos 10 metros. Las gráficas de recompensa para
ambos algoritmos se presentan en la Figura 5.2.
5.1.2. Inicialización Aleatoria
Además, también llevamos a cabo esta comparación en el caso de inicialización aleatoria, donde el
aprendizaje es más complejo. Como se observa en la Figura 5.2, no hay una gran diferencia en los
resultados entre entrenar con aleatoriedad o no, lo que indica que el agente aprende y sabe llegar
al centro desde cualquier posición. Por tanto, es capaz con los parámetros de entrada del modelo,
que no incluyen las posiciones de los usuarios, de tener una representación de donde se encuentran
los usuarios.0 500 1000 1500 2000 2500 3000
Episodios
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Recompensas
Recompensas media por episodio de DQN
Recompensas DQN Random
Recompensas DQN No Random
(a) Recompensa media por episodio DQN.0 500 1000 1500 2000 2500 3000
Episodios
2
0
2
4
6
8
10
Recompensas
Recompensas medias por episodio de PPO
Recompensas PPO No Random
Recompensas PPO Random (b) Recompensa media por episodio PPO.
Figura 5.2: Comparación de recompensas entre DQN (fila de arriba) y PPO (fila de abajo).
Un aspecto de particular importancia son los valores de recompensas que se aprecian para los
modelos DQN y PPO. En el caso de DQN la recompensa media máxima tiene un valor de 1.4,
mientras que en PPO el máximo de recompensa tiene un valor de aproximadamente 9. Volvemos
a lo mencionado previamente, esto se debe a la naturaleza del propio DQN, al estar incializándose
en (-100,0), solo tiene una resolución de movimiento, d. Por tanto no puede llegar a converger al
punto donde se consigue la mayor recompensa, en este caso esa posición es la cercana a (5,5).
5.2. UEs en Movimiento
Como se observa en la sección anterior, el algoritmo DQN tiene un carácter más rígido, principalmente heredado por la discretización a la hora de seleccionar las acciones, por lo que para una
mayor adaptación a los patrones de movimiento del dron a los de los UEs he decidido utilizar PPO
en casuísticas más complejas.
705.2. UES EN MOVIMIENTO
5.2.1. Movimiento Lineal
En la Figura 5.3, se puede observar el ejemplo actual. En este caso, los UEs se desplazan de derecha a izquierda inicializados en la posición (0,0), llegando al extremo derecho (300,0) luego al
izquierdo (-300,0) y volviendo al centro (0,0), como se explicó en la subsección 4.4.1.1. Además,
los UEs están inicializados en un clúster de 10x10.
5.2.1.1. Inicialización Estática
Como se ha realizado en el ejemplo anterior, primero se realizan experimentos con el agente inicializado en la misma posición en entrenamiento y en evaluación. En la Figura 5.3, se puede observar
el caso en el que el agente se inicializa en (-100,0). La flecha azul indica el movimiento que tiene
el conjunto de los usuarios.
El agente muestra un claro aprendizaje conforme avanzan los episodios, siendo capaz de posicionarse exactamente encima de los UEs. Con 2000 episodios, el dron todavía no es capaz de entender
que los UEs se mueven de derecha a izquierda. La situación cambia con 4000 episodios donde el
dron sigue la misma trayectoria que los UEs.
En la Figura 5.4, se muestran las gráficas correspondientes con la distancia media a los UEs (medida
únicamente con respecto a los ejes x e y, ya que el z no tiene movimiento), además de la recompensa
media por episodio que tiene el agente. Este resultado nos ayuda a entender que el dron llega a
converger a una solución independientemente de la posición de inicialización. De esta forma el
dron es capaz de encontrar a los UEs y seguirles desde cualquier posición inicial.
Se observa como la distancia media tiene una variación muy grande hasta que llega al episodio
4000, en el cual ya converge a una política buena. Si miramos la gráfica de la recompensa media
obtenida en el episodio 4000, todavía tenemos una política que puede mejorar. Esto se debe a
que la posición óptima está extremadamente cerca de los UEs, por tanto el agente puede seguir
optimizando las decisiones a tomar para que así se encuentre en todo momento exactamente encima
de ellos.
5.2.1.2. Inicialización Aleatoria
Al igual que se realizó en la experimentación de los UEs estáticos para comprobar el aprendizaje
del agente. En este caso también se han implementado simulaciones para determinar la capacidad
del dron para encontrar soluciones óptimas iniciándolo desde posiciones aleatorias. Estos resultados se observan en la Figura 5.5. Analizado dichos resultados, se puede observar que gracias a la
inicialización aleatoria durante el entrenamiento, el agente aprende a resolver todas las posiciones
de la misma manera. Esto es un avance, ya que no necesitamos un entrenamiento especifico para cada posición de salida del dron. En consecuencia, independientemente de la inicialización, el
dron aprende el movimiento de los usuarios alcanzando en todo momento la máxima recompensa. Este hallazgo difiere respecto a los resultados obtenidos con inicialización estática, donde la
dependencia de dicha inicialización es excesivamente elevada en lo concerniente a la recompensa
media obtenida. Por tanto, concluimos que el agente sabe aproximarse y seguir a los usuarios desde
cualquier punto del escenario.
71CAPÍTULO 5. RESULTADOS
(a) Episodio 0. (b) Episodio 2000.
(c) Episodio 4000. (d) Episodio 6000.
Figura 5.3: Visualización de episodios con posición de inicialización (0, -100).
5.2.2. Movimiento Circular
Después de obtener los resultados del movimiento lineal, se decide realizar el circular. Al suponer
este un mayor reto para los UEs, debido a que necesitan realizar acciones más dispares que en el
ejemplo anterior. Es decir, tienen que ir cambiando la trayectoria poco a poco para poder seguir el
movmiento circular. Sin embargo, cuando es lineal puede mantener la acción de seguir recto hasta
que llega al extremo.
5.2.2.1. Inicialización Estática
En la Figura 5.6, se muestra como el agente es capaz de seguir a los UEs cuando estos tienen un
movimiento circular. Indicar que el agente siempre parte de la posición inicial (0,0). Se observa
725.2. UES EN MOVIMIENTO0 1000 2000 3000 4000 5000 6000
Episodio
0
50
100
150
200
250
300
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (0, 100)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (-100, 0)
Inicialización en (100, 0)
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
0
2
4
6
8
Recompensa Media en Evaluación
Comparación de Recompensa Media en Evaluación
Inicialización en (0, 100)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (-100, 0)
Inicialización en (100, 0)
(b) Comparación de Recompensa Media en
Evaluación.
Figura 5.4: Comparación de métricas en diferentes inicializaciones durante entrenamiento
aleatorio.0 1000 2000 3000 4000 5000 6000
Episodio
0
50
100
150
200
250
300
350
400
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
2
0
2
4
6
8
Recompensa Media en Evaluación
Comparación de Recompensa Media en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
(b) Comparación de Recompensa Media en
Evaluación.
Figura 5.5: Comparación de métricas en diferentes inicializaciones
como al episodio 2000 ya es capaz de seguir a los UEs, el resto de episodios hasta el final, como
pasaba anteriormente, los utiliza para ir refinando las acciones que tiene que decidir.
Una representación más detallada de la distancia y la recompensa media se encuentra en la Figura 5.7, donde se observa lo comentado previamente. En el episodio 2000 el agente ya ha aprendido
a realizar un seguimiento de los usuarios, pero como sucedía en el caso de movimento lineal, esta
política todavía se puede mejorar para acercarse a la óptima.
5.2.2.2. Inicialización Aleatoria
A continuación pasamos a hacer el experimento en el que el agente se inicializa de manera aleatoria. Sin embargo, para evaluar, comparamos que pasaría si posicionamos al agente en diferentes
posiciones. En la Figura 5.7, se observan dichos valores además de compararse con el de la inicialización estática.
73CAPÍTULO 5. RESULTADOS
(a) Episodio 0. (b) Episodio 2000.
(c) Episodio 4000. (d) Episodio 6000.
Figura 5.6: Visualización de episodios con movimiento circular con inicialización en (0,0).
Tras observar los resultados, se concluye que para poder realizar un buen bucle de entrenamiento,
y que el agente pueda aprender a llegar desde todas las posiciones, con la misma rapidez con la
que lo hace en inicialización estática, se debería entrenar durante más episodios.
5.3. Experimentos de Ablación
El ajuste de los parámetros es una es una ardua tarea que requiere una gran cantidad de experimentos. Los parámetros implementados en las tablas presentadas anteriormente, Tablas 4.2 y 4.3, son
los definitivos que obtuvimos después de realizar múltiples entrenamientos. Esta sección explica
cómo afectan los diferentes parámetros a la convergencia del algoritmo PPO.
745.3. EXPERIMENTOS DE ABLACIÓN0 1000 2000 3000 4000 5000 6000
Episodio
100
200
300
400
500
Distancia Media a los UEs en Evaluación
Comparación de Distancia Media a los UEs en Evaluación
Inicialización en (-100, 0)
Inicialización en (0, -100)
Inicialización en (0, 0)
Inicialización en (0, 100)
Inicialización en (100, 0)
Inicialización en (0,0) no random
(a) Comparación de Distancia Media a los
UEs en Evaluación.0 1000 2000 3000 4000 5000 6000
Episodio
2
0
scholar link https://scholar.google.com/citations?user=U-WrIfQAAAAJ&hl=es&oi=ao



USE THE SCHOLAR TO CREATE THE PUBLICATIONS PAGE



DO ALL THE CHANGES IN @/al-folio needed, also access to internet if you need to do changes too 